WEEK 3: CLASSIFICATION

* CLASSIFICATION WITH LOGISTIC REGRESSION
  Motivations
  **** *Examples = Whether an email is spam (categories are Y/N), whether financial transactions are fraudulent (Y/N), is a cancer tumor
        malignant (Y/N). 
       *These are examples of BINARY CLASSIFCATION (y can only be one of two classes)...usually Yes/No, True/False, 0/1 
        (with 0 denoting false and 1 denoting True).
       *0 class = NEGATIVE CLASS (represents the absence of something); 1 class = POSITIVE CLASS (represents the presence of something). Can
        be arbitrary depending on the problem
       *Try to fit a straight line through this problem and it won't work...outlier examples can cause the best fit line to shift, and the
        threshold (DECISION BOUNDARY) also shifts even if it didn't need to
       *A way to try is by using a THRESHOLD (if below a value, predict 0; above a value, predict 1)
       *Linear Regression may get luckyand work well, but often does not in classification problems
       *Interesting = Logistic Regression is mainly for problems with two categories only (binary classification).
  Logistic Regression
  **** *Logistic regression fits an s-shaped curve to the data
       *Based on threshold, likeliness is decided
       *SIGMOID FUNCTION is the important mathematical function to know as it relates to the logistic regression algorithm...it has that 
        s-shaped curve. Also called the LOGISTIC FUNCTION, it outputs between 0 and 1.
       *Sigmoid function formula: g(z) = 1/(1+e^(-z)). When z is large, g(z) as a sigmoid function will be very close to 1; vice versa 
        when it is small/negative, it will be closer to 0.
       *Incorporating sigmoid into the logistic regression algorithm: 1) store w[vec]*x[vec] + b in variable z; 2) next step is to pass
        z through the g(z) = 1/(1+e^(-z)) sigmoid function. You then have the model f_w[vec],b_(x[vec]) = g(w[vec]*x[vec] + b), or
        1/(1+e^-(w[vec]*x[vec] + b)).
       *The way to interpret logistic regression output is to think about it as the probability that the class/label y is 1, given an 
        input x. The probability of it being 0 and the probability of it being 1 have to add up to 100%.
       *Notation typically used in research: f_w[vec],b_(x[vec]) = P(y=1|x[vec]; w[vec], b); states tht w and b are parameters that affect
        the computation of what is the probability that y is equal to 1 given the input feature x
  Decision Boundary
  **** *Common threshold is 0.5 (higher than 0.5 = y-hat is 1; lower than 0.5 = y-hat is 0)
       *Model predicts 1 whenever w[vec]*x[vec] + b >= 0. Model predicts 0 whenever w[vec]*x[vec] + b < 0.
       *DECISION BOUNDARY is where w[vec]*x[vec] + b is equal to 0. Example = when z is w1x1 +w2x2 + b and w1 is 1, w2 is 1 and b is -3;
        this equals z = x1 + x2 - 3 = 0 (at the decision boundary), which equals x1 + x2 = 3...this is the decision boundary.
       *In a non-linear example, when z is w1(x^2)_1 +w2(x^2)_2 + b and w1 is 1, w2 is 1 and b is -1; this equals 
        z = (x^2)_1 + (x^2)_2 - 1 = 0 (at the decision boundary), which equals (x^2)_1 + (x^2)_2 = 1
       *Can get complex depending on the polynomial terms, etc
      
* COST FUNCTION FOR LOGISTIC REGRESSION
  Cost Function for Logistic Regression
  **** *Recall: Cost function gives us a way to measure how well a set of parameters fit training data and gives you a way to try and
        choose better parameters.
       *This video shows that squared error cost function is not ideal for logistic regression; shows which cost function is
       *The example is given of a cancer dataset; what is malignant and what isn't; n is for number of features, i is for number of
        training examples, and it's binary classification, so the target variable y is 0 or 1; and the logistic regression model is 
        f_w[vec],b_(x[vec]) = 1/(1+e^-(w[vec]*x[vec] + b)). The question to ask is, given this dataset, how can you choose w[vec] and 
        b?
       *Squared error cost function works for linear regression because there is typically one minimum (bowl shaped); for 
        logistic regression, it's hilly so there's many local minima that the cost function can get trapped at
       *Example is squared error cost function which can be rewritten as J(w[vec], b) = (1/(m))*((1/2)*sum of (f_w,b(x(i)) minus y(i)) 
        squared over all of the examples)...won't work, but there is a different cost function that can make the cost function of a 
        nonlinear function convex again so convergence to the global minimum can look good
       *The formula is L(f_w[vec],b_(x[vec](i), y(i)) = {-log(f_w[vec],b_(x[vec](i))) if y(i) = 1
                                                         -log(1 - f_w[vec],b_(x[vec](i))) if y(i) = 0}
       *The above formula is the logistic loss function
       *Recall that cost function is the test for how well you are doing on all training examples
       *A natural log curve is almost shaped like half of a downward shaped parabola...the negative of that which is employed by the 
        logistic loss function is like half of an upward facing parabola (soup bowl)
       *F in -log(F) is always between 0 and 1, and 1 is where a logarithmic curve intersects the horizontal axis (10**0 = 1)...so the 
        curve (because logistic functions always have outputs between 0 and 1) essentially only matters between where the curve hits the
        y axis and where it hits the x axis
`      *Loss is lowest when f_w[vec],b_(x[vec](i)) predicts close to the true label
       *Actually, the plot of -log(1 - f_w[vec],b_(x[vec](i))) is like the right half of a soup bowl (which makes sense because loss is
        lowest when f equals 0; when f is 1 it's the left half)
       *Losses approaching the wrong thing approach infinity
       *The logistic loss function makes it a convex and can create a global minimum
       *Proving convexity is beyond the scope of the course
  Simplified Cost Function for Logistic Regression
  **** *Recall = We defined the logistic LOSS function as L(f_w[vec],b_(x[vec](i), y(i)) = {-log(f_w[vec],b_(x[vec](i))) if y(i) = 1
                                                         -log(1 - f_w[vec],b_(x[vec](i))) if y(i) = 0}
       *A simpler way to define it is (in a binary classification model, where the output has to be 0 or 1)
        L(f_w[vec],b_(x[vec](i), y(i)) = -y(i)*log(f_w[vec],b_(x[vec](i)))-(1 - y(i))*log(1 - (f_w[vec],b_(x[vec](i)))); this is because
        the half of the equation that doesn't correlate would be zeroed out
       *The simplified COST function is 1/m * sum of (L(f_w[vec],b_(x[vec](i), y(i))) over all of the training examples; this could also 
        be written as -1/m * sum of [y(i)*log(f_w[vec],b_(x[vec](i)))+(1 - y(i))*log(1 - (f_w[vec],b_(x[vec](i))))] over all of the 
        training examples
       *This cost function is derived from the statistical principle MAXIMUM LIKELIHOOD ESTIMATION, which helps to efficiently find
        parameters for different models

* GRADIENT DESCENT FOR LOGISTIC REGRESSION
  Gradient Descent Implementation
  **** *Recall = The cost function for logistic regression is 
        J(w[vec], b) = -1/m * sum of [y(i)*log(f_w[vec],b_(x[vec](i)))+(1 - y(i))*log(1 - (f_w[vec],b_(x[vec](i))))] over all of the 
        training examples
       *Based on the above cost function, the GRADIENT DESCENT ALGORITHM is still the same
        repeat until convergence {w1 = w1 - alpha*(1/(m)) *(sum of (f_w[vec],b(x[vec](i)) minus y(i))*x1(i))
         .
         .
         .
         wn = wn - alpha*(1/(m)) *(sum of (f_w[vec],b(x[vec](i)) minus y(i))*xn(i)) over all training examples;
         b = b - alpha*(1/(m)) *(sum of (f_w[vec],b(x[vec](i)) minus y(i))) over all of the training examples}.
       *Should also be simulataneous update
       *Looks same as linear regression but is different because function definition for f of x has changed...
        in linear regression it's f_w[vec],b(x[vec]) = (w[vec] * x[vec]) + b.
        in logistic regression it's f_w[vec],b(x[vec]) = 1/(1+e^-(w[vec]*x[vec] + b)).
       *Same as linear, you can monitor convergence, employ vectorization, feature scaling, 
