WEEK 1: WELCOME TO MACHINE LEARNING!

* OVERVIEW OF MACHINE LEARNING
  ** Machine learning is used in standard web searches, image recognition, recommendation, spam
  ** Also making its way into big companies and industrial applications (climate change solutions, diagnosis in healthcare, factory work)
  ** Machine learning = The science of getting computers to learn without being explicitly programmed
  ** The earlier version of this course actually led to the founding of Coursera, interestingly!

* APPLICATIONS OF MACHINE LEARNING
  ** We will learn about the important algorithms and the practical tips on making them work well
  ** Machine learning started as a subfield of artificial intelligence (AI)
  ** Examples of simple programming = Finding the shortest path from A to B (GPS)
  ** There was not a way to do interesting things like web search, recognize human speech, diagnose with x-rays, build self-driving cars...
     the only way it was understood that a machine could do things like this was to have it learn it by itself.
  ** Applications by A.N. = Speech recognition (Siri), Computer Vision (Google Maps), AI for augmented reality, payment fraud, 
     self-driving teams, healthcare, agriculture, ecommerce, and other problems.
  ** A.N. believes most industries will be touched by it in the near future
  ** Artificial General Intelligence is also expected down the line, but not soon
  ** AI is expected to add $13 trillion in value annually by 2030

* SUPERVISED VS UNSUPERVISED MACHINE LEARNING
  What's machine learning?
  **** *Field of study that gives computers the ability to learn without being explicitly programmed - Arthur Samule (1959)
       *Arthur Samuel wrote a checkers program (computer played tens of thousands of games against itself)
       *Two types of machine learning = supervised and unsupervised
       *Supervised learning used most in real-world applications; also unsupervised learning and recommender systems
  Supervised Learning Part 1
  **** *Supervised learning learns x to y (input to output mapping); you give your learning algorithm examples (right answers) to learn from
       *Examples of supervised learning = The input (x) of an email and the output of spam/not spam (y)...the application is spam filtering.
       *Another is x being audio and y being text transripts...the application is speech recognition.
       *Or x being English and y being Spanish...the application is machine translation.
       *Most lucrative application is in online advertising...x is info about ad/user info, and will you click on the ad or not (y).
       *Application of self-driving car...x is image, radar info, and y is finding the position of other cars.
       *Application of visual inspection...x is image of manufactured product (like cell phone), and y is determining if it's defected?
       *When models learn from these input/outputs, they can be given pairs they've not seen before and try to prodce the right output.
       *Different approaches to modelling (creating a straight-line model, a curve, or other).
       *Housing price prediction is an example of regression (predict a number out of an infinite possibility of numbers based on inputs),
        with house price being the desired output.
  Supervised Learning Part 2 
  **** *Recap = Supervised learning algorithms learn from input to output, or x to y mapping. Regression learns to predict numbers
        out of an infinite possibility of numbers.
       *Second type of supervised learning algorithm is the classification algorithm.
       *Example: Breast Cancer Detection. Classifcation models can predict if a lump in the breast is malignant (cancerous) or benign
        (nothing there). Dataset may have tumors of various sizes labelled as either benign or malignant; in that case there would only be
        two possible outputs (likely encoded as 0 or 1), as opposed to an infinite amount with regression.
       *You can have more than two outputs for classification. 
       *Output classes/categories = targets for classification problems.
       *Classification predicts categories/classes; could be no-numeric (image classifcation; cat or dog). It predicts from finite number of 
        possible values, as opposed to an infinte number of possible values with regression.
       *Can use more than one input to predict an output (ex: insead of just tumor size for breast cancer, tumor size and age).
       *In classification, model/algorithm, instead of a line of best fit, may find a boundary that separates different classes (like
        benign and malignant tumors).
       *Often many more than two input values are used/required...in breast cancer, other inputs typically used are thickness of the lump,
        uniformity of cell size, uniformity of cell shape, etc.
       *Recap = Supervised learning algorithms learn from input to output, or x to y mapping...learning alorithms learn from the right 
        answers. Two major types of supervised learning: regression (predict number) and classification (predict categories); 
        regression has an infinite possible amount of output values, and classification has a finite amount of  output values.
  Unsupervised Learning Part 1
  **** *Recap = Supervised learning classification; each sample has an output label (breast cancer = benign or malignant).
       *In unsupervised learning, we're given data that doesn't have output labels; job is simply to find something interesting (pattern,
        etc)
       *Called unsupervised b/c we're not trying to supervise the algorithm to give a right answer...just find interesting patterns;
        this type of unsupervised learning is called CLUSTERING
       *Clustering can be found in things like Google news...looks at news items and groups related stories together; valuable in the sense
        it's not feasible to manually do this every moment of every day
       *Clustering can also be found in things like DNA microarrays; based on DNA sequencing, being able to group individuals
       *Clustering can also be in grouping ustomers to understand preferences, etc
  Unsupervised Learning Part 2
  **** *Recap = Whereas in supervised learning, data comes with inputs x and outputs y...in unsupervised learning data comes with inputs
        x but not outputs y. Algorithm's goal is finding structure in the data. CLUSTERING is an example of unsupervised learning which
        groups similar data points together.
       *Another type of unsupervised learning is ANOMALY DETECTION (finding unusual data points). Important for fraud detection in the 
        financial system.
       *Another type of machine learning is DIMENSIONALITY REDUCTION (compressing a big dataset into a much smaller one while losing as 
        little information as possible).
  Jupyter Notebooks
  **** *Recap = Seen supervised learning and unsupervised learning and examples of both.
       *Most widely used tool in mahine learning is Jupyter Notebook. The tool that highest level practitioners use.
       *Markdown cells = text. Code cell = code.

* REGRESSION MODEL
  Linear Regression Model Part 1
  **** *Will go through the supervised learning process with a basic dataset
       *Example dataset will be housing dataset (input = size, output = price)...skills transferable to other future algorithms
       *Model fits a line of best fit through the data
       *Make estimates based on line of best fit created by model
       *Supervised learning is "supervised" because you are training a model by giving it all of the right answers (ex: size of house, and
        price that the model should predict)
       *Regression models predict numbers (infinite possibilities); linear regression is one example
       *Recall = The other type of supervised learning predicts categories (finite possibilities)
       *Also helpful to look at a data table
       *Standard notations = Training set (dataset used to train the model), x (input variable/feature), y (output/target variable),
        m (training examples), (x,y) is one specific training example, (x(i), y(i)) is the ith training example with i referring to a
        specific row in the data table
  Linear Regression Model Part 2
  **** *Recall = A training set includes input variables/features (size of house) and output targets (price of house)
       *To train the model, you 1) feed the training set (both input features and output targets) into a learning algorithm; then 2) the 
        supervised learning algorithm produces some function f (can be called hypothesis).
       *The job of function f becomes to take a new input x and output an estimate/prediction for y (y-hat)
       *The function f is called the model
       *Math representation of f (linear regression with one variable) : f_w,b(x) = wx + b; meas f is a function that takes x as input,
        and depending on the values of w and b, f will output some value of a prediction y-hat. Can also just write f(x).
       *Linear vs non-linear (discussed in the future)? But linear is a foundation to get to more complex.
       *The model in the video is linear regression with one input variable, or univariate linear regression.
       *To make it work, you ust construct a cost function, which will be discussed in the next video.
  Cost Function Formulas
  **** *Recall = A training set includes input variables/features (size of house) and output targets (price of house). Model used to fit
        the dataset is f_w,b(x) (or f(x)) = wx + b.
       *Standard notation: w and b are parameters of the model...also referred to as coefficients or weights.
       *The b parameter is the y-intercept; the w parameter is the slope
       *With linear regression, choose values for w and b so that the straight line you get from function f fits the data well, as
        compared to other lines that aren't as close.
       *Recall = A training example is (x(i), y(i)) with y as the target; for a given input x(i), the function also makes a prediction
        for y and the value is labeled y-hat(i).
       *Standard notation: Y-hat(i) = f_w,b(x(i)) (or f(x(i))). Also f_w,b(x(i)) (or f(x(i))) = w(x(i)) + b.
       *Question = How do you find values for w and b so the prediction y-hat(i) is close to the true target y(i) for all (x(i), y(i))?
       *How to measure how well a line fits the training data = a COST FUNCTION.
       *Cost functions take y-hat predictions and compares it to the target y by taking y-hat(i) and subtracting it by y(i)...that 
        difference is called the ERROR, or how far off the prediction is from the target. Then the square of that error is taken. We sum
        these squared errors for all of the training examples.
       *The more training examples a dataset has would mean the higher the squared error calculated, by virtue of more examples...to 
        combat this, we will take the average squared error instead of the total...and further the convention is to divide by 2m.
       *Cost function: (1/(2*m)) * (sum of ((y-hat(i) minus y(i)) squared) over all of the training examples). Also called the squared
        error cost function.  Can also be (1/(2*m)) * (sum of (f_w,b(x(i)) minus y(i)) squared) over all of the training 
        examples).
       *Cost function notation = J(w,b).
  Cost Function Intuition
  **** *Recap: We have model f_w,b(x) (or f(x)) = wx + b, with parameters x and b. Depending on the different values chosen for w and b,
        you get different straight lines for the model...you want to find values so that the straight line fits the training data well.
        To measure how well w and b fit the data, you have a cost function J(w,b) = 1/(2*m)) * (sum of (f_w,b(x(i)) minus y(i)) squared) 
        over all of the training examples) which measures the difference between the model's predictions and the actual true values for
        y. Linear regression then tries to make J(w,b) as small as possible, with a goal to minimize J(w,b).
       *Video works with a simplified version to better visualize the cost function...uses f_w(x) = wx.
       *J(w) = 1/(2*m)) * (sum of (f_w(x(i)) minus y(i)) squared) over all of the training examples). Minimize J(w).
       *F_w(x) (when parameter w is fixed, this is a function of x, and y depends on the value of input x) vs J(w) (cost function is a
        function of w, and cost depends on parameter)...
       *Example is if a dataset has 1 point (1,1); if in f_w(x) = wx, w = 1, then f(1) would predict 1...then the cost function would be 
        1/(2*(1 training example)) * ((1-1))**2, which would equal 0, and J(1) would equal zero. If parameter w were 2, then f(1) would
        predict 2...then the cost function would be (1/2) * ((2-1)**2), and J(2) would equal 1/2. Etc, etc.
       *You could try positive and negative numbers for the parameters.
       *Usually a cost function ends up a parabola shape. In the example above, the parabola's low point would be at 1 where the model f
        exactly fits the data and the corresponding cost (average prediction "error") is 0.
       *Recap is in J(w) or J(w,b), find the values of w or w/b that minimize J (like 1 did in our example, vs 2).
  Visualizing the Cost Function
  **** *Recap: There's the model f_w,b(x) (or f(x)) = wx + b, with parameters x and b. There's the cost function
        J(w,b) = 1/(2*m)) * (sum of (f_w,b(x(i)) minus y(i)) squared) over all of the training examples)...the goal of linear regression
        is to minimize the cost function J(w,b) over parameters w and b.
       *This video adds b back to the model parameters.
       *In this case, the model f_w,b(x) is a function of x and J(w,b) is a function of model parameters w and b.
       *With two parameters, it's now a three-dimensional parabola/soup bowl.
       *When w is something and b is something, then the height of the surface above that point is J when w and b equal that.
       *Instead of 3d surface plots, graph contour plots...it's just a soupbowl stretched out. You are basically slicing the 
        3d surface plot...imagine sling a bowl.
       *When on the same line of a contour plot, even if they have different values for w and b, that means they have the same cost.
