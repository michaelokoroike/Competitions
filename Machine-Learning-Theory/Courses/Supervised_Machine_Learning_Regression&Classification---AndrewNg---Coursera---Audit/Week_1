WEEK 1: WELCOME TO MACHINE LEARNING!

* OVERVIEW OF MACHINE LEARNING
  ** Machine learning is used in standard web searches, image recognition, recommendation, spam
  ** Also making its way into big companies and industrial applications (climate change solutions, diagnosis in healthcare, factory work)
  ** Machine learning = The science of getting computers to learn without being explicitly programmed
  ** The earlier version of this course actually led to the founding of Coursera, interestingly!

* APPLICATIONS OF MACHINE LEARNING
  ** We will learn about the important algorithms and the practical tips on making them work well
  ** Machine learning started as a subfield of artificial intelligence (AI)
  ** Examples of simple programming = Finding the shortest path from A to B (GPS)
  ** There was not a way to do interesting things like web search, recognize human speech, diagnose with x-rays, build self-driving cars...
     the only way it was understood that a machine could do things like this was to have it learn it by itself.
  ** Applications by A.N. = Speech recognition (Siri), Computer Vision (Google Maps), AI for augmented reality, payment fraud, 
     self-driving teams, healthcare, agriculture, ecommerce, and other problems.
  ** A.N. believes most industries will be touched by it in the near future
  ** Artificial General Intelligence is also expected down the line, but not soon
  ** AI is expected to add $13 trillion in value annually by 2030

* SUPERVISED VS UNSUPERVISED MACHINE LEARNING
  What's machine learning?
  **** *Field of study that gives computers the ability to learn without being explicitly programmed - Arthur Samule (1959)
       *Arthur Samuel wrote a checkers program (computer played tens of thousands of games against itself)
       *Two types of machine learning = supervised and unsupervised
       *Supervised learning used most in real-world applications; also unsupervised learning and recommender systems
  Supervised Learning Part 1
  **** *Supervised learning learns x to y (input to output mapping); you give your learning algorithm examples (right answers) to learn from
       *Examples of supervised learning = The input (x) of an email and the output of spam/not spam (y)...the application is spam filtering.
       *Another is x being audio and y being text transripts...the application is speech recognition.
       *Or x being English and y being Spanish...the application is machine translation.
       *Most lucrative application is in online advertising...x is info about ad/user info, and will you click on the ad or not (y).
       *Application of self-driving car...x is image, radar info, and y is finding the position of other cars.
       *Application of visual inspection...x is image of manufactured product (like cell phone), and y is determining if it's defected?
       *When models learn from these input/outputs, they can be given pairs they've not seen before and try to prodce the right output.
       *Different approaches to modelling (creating a straight-line model, a curve, or other).
       *Housing price prediction is an example of regression (predict a number out of an infinite possibility of numbers based on inputs),
        with house price being the desired output.
  Supervised Learning Part 2 
  **** *Recap = Supervised learning algorithms learn from input to output, or x to y mapping. Regression learns to predict numbers
        out of an infinite possibility of numbers.
       *Second type of supervised learning algorithm is the classification algorithm.
       *Example: Breast Cancer Detection. Classifcation models can predict if a lump in the breast is malignant (cancerous) or benign
        (nothing there). Dataset may have tumors of various sizes labelled as either benign or malignant; in that case there would only be
        two possible outputs (likely encoded as 0 or 1), as opposed to an infinite amount with regression.
       *You can have more than two outputs for classification. 
       *Output classes/categories = targets for classification problems.
       *Classification predicts categories/classes; could be no-numeric (image classifcation; cat or dog). It predicts from finite number of 
        possible values, as opposed to an infinte number of possible values with regression.
       *Can use more than one input to predict an output (ex: insead of just tumor size for breast cancer, tumor size and age).
       *In classification, model/algorithm, instead of a line of best fit, may find a boundary that separates different classes (like
        benign and malignant tumors).
       *Often many more than two input values are used/required...in breast cancer, other inputs typically used are thickness of the lump,
        uniformity of cell size, uniformity of cell shape, etc.
       *Recap = Supervised learning algorithms learn from input to output, or x to y mapping...learning alorithms learn from the right 
        answers. Two major types of supervised learning: regression (predict number) and classification (predict categories); 
        regression has an infinite possible amount of output values, and classification has a finite amount of  output values.
  Unsupervised Learning Part 1
  **** *Recap = Supervised learning classification; each sample has an output label (breast cancer = benign or malignant).
       *In unsupervised learning, we're given data that doesn't have output labels; job is simply to find something interesting (pattern,
        etc)
       *Called unsupervised b/c we're not trying to supervise the algorithm to give a right answer...just find interesting patterns;
        this type of unsupervised learning is called CLUSTERING
       *Clustering can be found in things like Google news...looks at news items and groups related stories together; valuable in the sense
        it's not feasible to manually do this every moment of every day
       *Clustering can also be found in things like DNA microarrays; based on DNA sequencing, being able to group individuals
       *Clustering can also be in grouping ustomers to understand preferences, etc
  Unsupervised Learning Part 2
  **** *Recap = Whereas in supervised learning, data comes with inputs x and outputs y...in unsupervised learning data comes with inputs
        x but not outputs y. Algorithm's goal is finding structure in the data. CLUSTERING is an example of unsupervised learning which
        groups similar data points together.
       *Another type of unsupervised learning is ANOMALY DETECTION (finding unusual data points). Important for fraud detection in the 
        financial system.
       *Another type of machine learning is DIMENSIONALITY REDUCTION (compressing a big dataset into a much smaller one while losing as 
        little information as possible).
  Jupyter Notebooks
  **** *Recap = Seen supervised learning and unsupervised learning and examples of both.
       *Most widely used tool in mahine learning is Jupyter Notebook. The tool that highest level practitioners use.
       *Markdown cells = text. Code cell = code.

* REGRESSION MODEL
  Linear Regression Model Part 1
  **** *Will go through the supervised learning process with a basic dataset
       *Example dataset will be housing dataset (input = size, output = price)...skills transferable to other future algorithms
       *Model fits a line of best fit through the data
       *Make estimates based on line of best fit created by model
       *Supervised learning is "supervised" because you are training a model by giving it all of the right answers (ex: size of house, and
        price that the model should predict)
       *Regression models predict numbers (infinite possibilities); linear regression is one example
       *Recall = The other type of supervised learning predicts categories (finite possibilities)
       *Also helpful to look at a data table
       *Standard notations = Training set (dataset used to train the model), x (input variable/feature), y (output/target variable),
        m (training examples), (x,y) is one specific training example, (x(i), y(i)) is the ith training example with i referring to a
        specific row in the data table
  Linear Regression Model Part 2
  **** *Recall = A training set includes input variables/features (size of house) and output targets (price of house)
       *To train the model, you 1) feed the training set (both input features and output targets) into a learning algorithm; then 2) the 
        supervised learning algorithm produces some function f (can be called hypothesis).
       *The job of function f becomes to take a new input x and output an estimate/prediction for y (y-hat)
       *The function f is called the model
       *Math representation of f (linear regression with one variable) : f_w,b(x) = wx + b; meas f is a function that takes x as input,
        and depending on the values of w and b, f will output some value of a prediction y-hat. Can also just write f(x).
       *Linear vs non-linear (discussed in the future)? But linear is a foundation to get to more complex.
       *The model in the video is linear regression with one input variable, or univariate linear regression.
       *To make it work, you ust construct a cost function, which will be discussed in the next video.
  Cost Function Formulas
  **** *Recall = A training set includes input variables/features (size of house) and output targets (price of house). Model used to fit
        the dataset is f_w,b(x) (or f(x)) = wx + b.
       *Standard notation: w and b are parameters of the model...also referred to as coefficients or weights.
       *The b parameter is the y-intercept; the w parameter is the slope
       *With linear regression, choose values for w and b so that the straight line you get from function f fits the data well, as
        compared to other lines that aren't as close.
       *Recall = A training example is (x(i), y(i)) with y as the target; for a given input x(i), the function also makes a prediction
        for y and the value is labeled y-hat(i).
       *Standard notation: Y-hat(i) = f_w,b(x(i)) (or f(x(i))). Also f_w,b(x(i)) (or f(x(i))) = w(x(i)) + b.
       *Question = How do you find values for w and b so the prediction y-hat(i) is close to the true target y(i) for all (x(i), y(i))?
       *How to measure how well a line fits the training data = a COST FUNCTION.
       *Cost functions take y-hat predictions and compares it to the target y by taking y-hat(i) and subtracting it by y(i)...that 
        difference is called the ERROR, or how far off the prediction is from the target. Then the square of that error is taken. We sum
        these squared errors for all of the training examples.
       *The more training examples a dataset has would mean the higher the squared error calculated, by virtue of more examples...to 
        combat this, we will take the average squared error instead of the total...and further the convention is to divide by 2m.
       *Cost function: (1/(2*m)) * (sum of ((y-hat(i) minus y(i)) squared) over all of the training examples). Also called the squared
        error cost function.  Can also be (1/(2*m)) * (sum of (f_w,b(x(i)) minus y(i)) squared) over all of the training 
        examples).
       *Cost function notation = J(w,b).
  Cost Function Intuition
  **** *Recap: We have model f_w,b(x) (or f(x)) = wx + b, with parameters x and b. Depending on the different values chosen for w and b,
        you get different straight lines for the model...you want to find values so that the straight line fits the training data well.
        To measure how well w and b fit the data, you have a cost function J(w,b) = 1/(2*m)) * (sum of (f_w,b(x(i)) minus y(i)) squared) 
        over all of the training examples) which measures the difference between the model's predictions and the actual true values for
        y. Linear regression then tries to make J(w,b) as small as possible, with a goal to minimize J(w,b).
       *Video works with a simplified version to better visualize the cost function...uses f_w(x) = wx.
       *J(w) = 1/(2*m)) * (sum of (f_w(x(i)) minus y(i)) squared) over all of the training examples). Minimize J(w).
       *F_w(x) (when parameter w is fixed, this is a function of x, and y depends on the value of input x) vs J(w) (cost function is a
        function of w, and cost depends on parameter)...
       *Example is if a dataset has 1 point (1,1); if in f_w(x) = wx, w = 1, then f(1) would predict 1...then the cost function would be 
        1/(2*(1 training example)) * ((1-1))**2, which would equal 0, and J(1) would equal zero. If parameter w were 2, then f(1) would
        predict 2...then the cost function would be (1/2) * ((2-1)**2), and J(2) would equal 1/2. Etc, etc.
       *You could try positive and negative numbers for the parameters.
       *Usually a cost function ends up a parabola shape. In the example above, the parabola's low point would be at 1 where the model f
        exactly fits the data and the corresponding cost (average prediction "error") is 0.
       *Recap is in J(w) or J(w,b), find the values of w or w/b that minimize J (like 1 did in our example, vs 2).
  Visualizing the Cost Function
  **** *Recap: There's the model f_w,b(x) (or f(x)) = wx + b, with parameters x and b. There's the cost function
        J(w,b) = 1/(2*m)) * (sum of (f_w,b(x(i)) minus y(i)) squared) over all of the training examples)...the goal of linear regression
        is to minimize the cost function J(w,b) over parameters w and b.
       *This video adds b back to the model parameters.
       *In this case, the model f_w,b(x) is a function of x and J(w,b) is a function of model parameters w and b.
       *With two parameters, it's now a three-dimensional parabola/soup bowl.
       *When w is something and b is something, then the height of the surface above that point is J when w and b equal that.
       *Instead of 3d surface plots, graph contour plots...it's just a soupbowl stretched out. You are basically slicing the 
        3d surface plot...imagine slicing a bowl.
       *When on the same line of a contour plot, even if they have different values for w and b, that means they have the same cost.
  Visualization Examples
  **** *Recap: Paramters of f_w,b(x) are the coordinates of the cost function contour plot 
       *Shows interactive console 
       *GRADIENT DESCENT is the algorithm that automatially finds the values w and b that minimize the cost function J

* TRAIN THE MODEL WITH GRADIENT DESCENT
  Gradient Descent
  **** *Recap: We recently looked at different choices of the parameters w and b and see what cost value they get you
       *Gradient descent is the systematic way to find the values w and b that result in the smallest possible cost value
       *Used very widely in machine learning, including in deep learning
       *With gradient descent you have the cost function J(w,b) and you want to minimize it.
       *Gradient descent is an algorithm that can be used to minimize any function, not just a linear regression cost function...also
        can be used to minimize a function with more than two parameters.
       *Gradient descent: 1) starts with a random guess of w and b (usually set to 0); 2) keep changing w and b to try to reduce the cost
        of J(w,b); 3) J settles at a minimum
       *It's possible for there to be more than one minimum...not in squared error cost function (which always leads to the bowl-shaped 
        hammock)...but if you're training a neural network model for example
       *Example is standing on top of a hill on a hilly golf course...gradient descent looks around and asks where would be the best place
        to move down the hill as efficiently as possible (referred to mathematically as the DIRECTION OF STEEPEST DESCENT)...a baby step
        takes you down faster than it would in another direction. With each step you spin around and look for the direction of steepest 
        descent. And so on until you find yourself at the bottom of the valley, or the "LOCAL MINIMUM".
       *Because you can choose starting points at the surface by choosing starting values for parameters w and b, depending on where you
        start, the direction of steepest descent may be diffrent than when you chose other values, which may cause the gradient descent
        process to complete at a different local minimum than when starting at the other starting points for w and b...the low points
        discovered in both places are called LOCAL MINIMA.
  Implementing Gradient Descent
  **** *Gradient Descent algorithm: w (update) = w - alpha*([d/dw]J(w,b))
       *The above formula basically says update w by adjusting it by a small amount
       *Unpacking the Gradient Descent algorithm formula: = sign is the assignment operator (== is truth assertion), 
        alpha is the LEARNING RATE (usually between 0 and 1; controls how big of a step you take downhill),
        [d/dw]J(w,b) is the DERIVATIVE TERM for cost function J (controls which direction you want to take your step in; also
        determines with alpha the size of the step)
       *Derivatives come from calculus, but you don't need to go that deep to understand!
       *Gradient Descent algorithm: b (update) = b - alpha*([d/db]J(w,b))
       *For a gradient descent algorithm, you update w and b until they CONVERGE (reach a point at a local minimum where these parameters 
        no longer change much when taking a step)
       *In gradient descent, the updates for the parameters occur simultaneously; this is done by calculating the update formula in a temp 
        variable and then assigning the temp variable values to the parameters...this is so the pre-updated w can go into the update calc
        for the  b parameter, and/or vice versa. It is incorrect to update w or b before calculating the new value for the other parameter.
        With that said, it turns out that even if the non-simultaneous way is done, that will probably work anyway.
  Gradient Descent Intuition
  **** *Recap: Gradient descent algorithm is || repeat until convergence {w - alpha*([d/dw]J(w,b)); b - alpha*([d/db]J(w,b))}. Alpha 
        controls how big of a step you take. The d/dw is a (partial) derivative term.
       *Use a simpler example of cost function J(w). The gradient descent formula becomes w - alpha*([d/dw]J(w))...like the 
        two-dimesnional graph we had earlier (see Cost Function Inutiton video).
       *Say w is at a point on the parabolic cost function...the way to think about the derivative at the point is to draw a 
        TANGENT LINE, a straight line touching the curve at the point. The SLOPE of that line is the derivative of the function J 
        at that point.
       *Learning rate is always a positive number...if the slope at the point is positive, you will decrease w...if negative you will 
        increase w.
  Learning Rate
  **** *Recap: Gradient descent update rule for simplified function is w = w - alpha*([d/dw]J(w))
       *If alpha (the learning rate) is too small, you multiply the derivative term by some really small number; the gradient descent is
        a small baby step...you decrease the cost J but incredibly slowly
       *If alpha is too large, gradient descent may overshoot and never reachthe minimum (actually could get worse with each step taken);
        it may fail to converge, or even diverge
       *If you're already at a local minimum, gradient descent will leave w unchanged (the slope at local minima is 0...multiply alpha by 0
        and it will lead to gradient descent telling w to remain w (w = w - alpha*0, or w = w - 0, or w = w)
       *Gradient descent can reach a local minimum even with a fixed learning rate, because as we get near a local minimum, the derivative
        becomes smaller as the slope gets less steep, and the update steps get smaller...so the learning rate doesn't even need to be 
        changed
  Gradient Descent for Linear Regression (mostly calculus)
  **** *Recap: We've looked at the linear regression model and the cost function and the gradient descent algorithm. This video will
        use the squared error cost function for the linear regression model with gradient descent, which allows us to train the linear
        regression model to fit a straight line. The linear regression model is f_w,b(x) (or f(x)) = wx + b; 
        the sq. error cost function is J(w,b) = 1/(2*m)) * (sum of (f_w,b(x(i)) minus y(i)) squared) over all of the training examples);
        the gradient descent algorithm is [repeat until convergence {w - alpha*([d/dw]J(w,b)); b - alpha*([d/db]J(w,b))}].
       *If you calculate the derivative in the GD algorithm, you would get 
        1/(m)) * (sum of (f_w,b(x(i)) minus y(i))*x(i)) over all of the training examples) for the w derivative term and 
        1/(2*m)) * (sum of (f_w,b(x(i)) minus y(i)) over all of the training examples) for the b derivative term
       *[d/dw]J(w,b) = [d/dw]1/(2*m)) * (sum of (f_w,b(x(i)) minus y(i)) squared) over all of the training examples) =
        [d/dw]1/(2*m)) * (sum of (wx(i)+b-y(i)) squared) over all of the training examples)
       *1/2m makes the partial derivative neater
       *Partial derivative takes original cost function and finds the derivative while taking terms from it (in the model f(x) is wx+b;
        x is part of the derivative of w, and there's nothing for b
       *Carry those derivative terms out in the gradient descent algorithm until convergence
       *Squared error cost functions never have multiple local minima... it is a bowl-shaped CONVEX FUNCTION
  Running Gradient Descent
  **** *Runs a model and shows how steps take it along a journey
       *Batch gradient descent = Every step taken uses all of the training examples
       *There are other versions of gradient descent, but this is the main one for linear regression
  Full Cycle of a Machine Learning Project
  **** *1) Define scope of project (define project)
       *2) Collect data (define and collect data)
       *3) Train model (training, error analysis, iterative improvement)
       *4) Deploy in a production environment (deploy, monitor, maintain system)
       *Sometimes you may have to go back to steps 3 and/or 2 depending on how well the data is working
       *Deploying the model = Take model -> implement it in an inference server. If mobile app is used, the app will make an API call
        to pass audio/imagery to the inference serve, after which the inference server returns an inference (prediction) to the mobile
        app.
       *Software engineering needed to make reliable predictions, scaling, logging, system monitoring, model update. This is MLOps
        (machine learning operations).
