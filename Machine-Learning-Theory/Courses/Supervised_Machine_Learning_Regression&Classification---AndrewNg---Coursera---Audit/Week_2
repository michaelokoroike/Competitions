WEEK 2: REGRESSION WITH MULTIPLE INPUT VARIABLES

* MULTIPLE LINEAR REGRESSION
  Multiple Features
  **** *Recap: In the original version of linear regression, we had a single input variable x (size of house) which we used to predict y
        (price of house). The model therefore was f_w,b(x) = wx + b.
       *Now we will know the number of bedrooms, the number of floors, and the age of the home.
       *Notation = For multiple input variables: x1, x2...xj (j being the specific input feature (column)); 
        n will equal the total number of features. x^i represents the ith training example (row/array of all input variables); this is
        sometimes referred to as a VECTOR including all of the input features of the ith training example, more specifically a ROW VECTOR.
        (x^i)_j is the value of feature j in the ith training example. Sometimes an arrow is drawn over x^i to show it is a vector.
        *Now the model is not f_w,b(x) = wx + b, but f_w,b(x) = w1x1 + w2x2 + w3x3 + w4x4 + b.
        *Example = 0.1x1 (size of house) + 4x2 (bedrooms) + 10x3 (number of floors) - 2x4 (age of house) + 80  (base price). A way to
        interpret is, if assuming the model is trying to predict the price of a house in thousands of dollars, the base price of a house is 
        80,000 ($1000*80). For every square foot, price will increase by $1000*.01, or $100. For each bedroom, the price increases by $4,000;
        for each floor, the price increases by $10,000; for each year the house has existed the price decreases by $2,000.
        *With n features, the model will be f_w,b(x) = w1x1 + w2x2 + ... + wnxn + b.
        *Notation for the above is f_w[vec],b(x[vec]) = (w[vec] * x[vec]) + b. The dot product of w[vec] * x[vec] is the sum of 
         w1x1 + w2x2 + ... + wnxn.
        *This type of linear regression wh multiple input variables is MULTIPLE LINEAR REGRESSION.
   Vectorization Part 1
   **** *Vectorization makes your code shorter and makes it run more efficiently. Also allows you to take advantage of numerical linear 
         algebra libraries, as well as GPU hardware (speeds up computer graphics).
        *Example of vectorization: w is a vector of [w1, w2, w3] and b is a number ans x is a vector of [x1, x2, x3]. Here n = 3;
         algebra would index these vectors 1-3, but Python indexes them 0-2.
        *Without vectorization you go through [w1]*[x1] + [w2]*[x2] + [w3]*[x3] + b...with vectorization, in numpy you can do
         numpy.dot(w,x) + b.
        *It's faster than other ways (manual for each pair, for loop) because numpy uses parallel hardware in your computer.
   Vectorization Part 2
   **** *Example: Look at an example without vectorization (a for loop) and example with vectorization (np.dot(w,x))
        *Without vectorization at each timestamp it will calculate a wx+b function...one step at a time
        *With vectorization (np.dot(x)) computer multiplies each pair of w and x with each other in parallel in one step, then
         uses specialized hardware to add up the w*x products efficently rather than 1 by 1; matters with large data sets
        *Example of how this helps with multiple linear regression: problem with 16 features/parameters, in addition to parameter b. 
         You calculate 16 derivative terms for the 16 weights; and you store the value of the weights and the derivatives in two numpy 
         arrays. If you want to compute an update step without vectorization, you'd have to individually do w1 = w1 - 0.1(derivative of
         j at w1), w2 = w2 - 0.1(derivative of j at w2), and so on until the 16 different features are accounted for. With
         vectorization, you can do  w = w - 0.1*d and the computer will do parallel processing on the numpy arrays
        *Overall, vectorization is just quicker
   Gradient Descent for Multiple Linear Regression
   **** *This video will put multiple linear regression, gradient descent, and vectorization together.
        *Recap: Previous notation is parameters (w1...wn, b), model (f_w,b(x) = w1x1 + w2x2 + ... + wnxn + b), 
         cost function (J(w1...wn, b)). Vectorized notation is parameters w = vec[w1 ... n] (a vector of length n) and b still being
         a number; model is f_w[vec],b(x[vec]) = (w[vec] * x[vec]) + b, with w[vec] * x[vec] as the dot product; 
         cost function is J(w[vec], b). Gradient descent without vectorization is 
         repeat until convergence {w - alpha*([d/dw]J(w1..wn,b)); b - alpha*([d/db]J(w1....wn,b)) and with vectorization is
         repeat until convergence {w - alpha*([d/dw]J(w[vec],b)); b - alpha*([d/db]J(w[vec],b)).
        *Gradient descent for one feature is 
         repeat until convergence {w = w - alpha*(1/(m)) *(sum of (f_w,b(x(i)) minus y(i))*x(i)) over all of the training examples;
         b = b - alpha*(1/(m)) *(sum of (f_w,b(x(i)) minus y(i))) over all of the training examples}.
        *Gradient descent for multiple features is 
         repeat until convergence {w1 = w1 - alpha*(1/(m)) *(sum of (f_w[vec],b(x[vec](i)) minus y(i))*x1(i))
         .
         .
         .
         wn = wn - alpha*(1/(m)) *(sum of (f_w[vec],b(x[vec](i)) minus y(i))*xn(i)) over all training examples;
         b = b - alpha*(1/(m)) *(sum of (f_w[vec],b(x[vec](i)) minus y(i))) over all of the training examples}.
        *An alternative to gradient descent is the NORMAL equation. It works only for linear regression (and really nothing else)
         for solving for w and b without iteration. Disadvantages are this does not generalize to other learning algorithms and is slow
         on large datasets. Some machine learning libraries use normal equation on the backend...but gradient descent is recommended.

* GRADIENT DESCENT IN PRACTICE
  Feature Scaling Part 1
  **** *When the possible range of values of a feature is large, a good model typically learns to choose a typically small parameter value
        and vice versa
       *This is important and can be seen in plotting points and corresponding cost functions; the variable with the much larger range of 
        values typically has a narrow contour because a small change to the parameter of that feature can typically have a large impact on the 
        cost J/prediction of the model, and vice versa again (because that small contour is multiplied by a very large number)
       *What happens in this scenario is gradient descent may end up bouncing up and down/back and forth for a long time before finding
        its global minimum (video shows that it can bounce back and forth in predictions/errors...see 
        https://medium.com/@yennhi95zz/6-a-beginners-guide-to-gradient-descent-and-feature-scaling-in-machine-learning-bbbeecbb5d51).
        This is why we SCALE FEATURES.
       *When we scale, contours look more normal and gradient descent find =s a much more direct path to the global minimum
  Feature Scaling Part 2
  **** *Ways to scale include dividing each original x1, x2, etc by the maximum value of its range (min-max scaling); you can also do
        mean normalization (subtract each original x1, x2, etc by the average (Greek mu) of its range, and divide that by the difference 
        between the maximum and minimum value of the range); another is z-score normalization (subtract each original x1, x2, etc by the 
        average (Greek mu) of its range, then divide by standard deviation)
       *Rule of thumb = When scaling, ideal is between -1 and 1 for each feature x...but ranges like -3 to 3, or -0.3 to 0.3, are
        acceptable. 0 and 3 is also ok. But something like -100 to 100, or -0....1 to 0.00001 on the other side, needs to be rescaled.
  Checking Gradient Descent for Convergence
  **** *Gradient descent rule is w = w - alpha*([d/dw]J(w[vec],b)); b = b - alpha*([d/db]J(w[vec],b)...one of the important aspects
        of the equations are alpha. The way to check if it's working properly is to plot the value of cost function J at each iteration
        of gradient descent. Y would be J(w[vec], b) value, X would be number of iterations. Should look like a hockey stick. The curve
        is called a LEARNING CURVE.
        *J should not increase after 1 iteration (means alpha was chosen poorly, or is too large).
        *You can also tell when convergence occurs. The number of iterations necessary for convergence may differ for different applications.
        *You can also use an AUTOMATIC CONVERGENCE TEST (if J(w[vec], b) decreases by less than Greek epsilon value, then you declare
         convergence). But you have to choose epsilon, which is difficult.
  Choosing the Learning Rate
  **** *Recap: It it's too small, it may run slowly...but if it's too large, it may not even converge (may overshoot...cost could actually 
        go up...imagine a parabola with a ball plinko-ing upwards)
        *Could also be due to broken code (adding alpha times derivative instead of subtracting)
        *With a small enough learning rate, cost J(w[vec], b) should decrease on every iteration...if with really small numbers that 
         doesn't happen, that usually means there's a bug in the code
        *A really small alpha is a debugging step and is not the most appropriate choice a lot of the time (may take too long)
        *Try a range (0.001, 0.01, 0.1, 1) for a handful of iterations, find what's just right
  Feature Engineering
  **** *Example = In housing prices frontage and depth may be two different features, but can be made into one more predictive area 
        feature...becomes w1x1 + w2x2 + w3x3 + b, with x1 and x2 being frontage and depth, and x3 being area. Gives the model 
        another option as to what is the most important predictor of price.
        *It's using intuition and understanding of a problem, and designing new "solutions" (features) to help solve the problem
