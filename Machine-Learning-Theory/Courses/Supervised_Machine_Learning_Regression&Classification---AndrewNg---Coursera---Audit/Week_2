WEEK 2: REGRESSION WITH MULTIPLE INPUT VARIABLES

* MULTIPLE LINEAR REGRESSION
  Multiple Features
  **** *Recap: In the original version of linear regression, we had a single input variable x (size of house) which we used to predict y
        (price of house). The model therefore was f_w,b(x) = wx + b.
       *Now we will know the number of bedrooms, the number of floors, and the age of the home.
       *Notation = For multiple input variables: x1, x2...xj (j being the specific input feature (column)); 
        n will equal the total number of features. x^i represents the ith training example (row/array of all input variables); this is
        sometimes referred to as a VECTOR including all of the input features of the ith training example, more specifically a ROW VECTOR.
        (x^i)_j is the value of feature j in the ith training example. Sometimes an arrow is drawn over x^i to show it is a vector.
        *Now the model is not f_w,b(x) = wx + b, but f_w,b(x) = w1x1 + w2x2 + w3x3 + w4x4 + b.
        *Example = 0.1x1 (size of house) + 4x2 (bedrooms) + 10x3 (number of floors) - 2x4 (age of house) + 80  (base price). A way to
        interpret is, if assuming the model is trying to predict the price of a house in thousands of dollars, the base price of a house is 
        80,000 ($1000*80). For every square foot, price will increase by $1000*.01, or $100. For each bedroom, the price increases by $4,000;
        for each floor, the price increases by $10,000; for each year the house has existed the price decreases by $2,000.
        *With n features, the model will be f_w,b(x) = w1x1 + w2x2 + ... + wnxn + b.
        *Notation for the above is f_w[vec],b(x[vec]) = (w[vec] * x[vec]) + b. The dot product of w[vec] * x[vec] is the sum of 
         w1x1 + w2x2 + ... + wnxn.
        *This type of linear regression wh multiple input variables is MULTIPLE LINEAR REGRESSION.
   Vectorization Part 1
   **** *Vectorization makes your code shorter and makes it run more efficiently. Also allows you to take advantage of numerical linear 
         algebra libraries, as well as GPU hardware (speeds up computer graphics).
        *Example of vectorization: w is a vector of [w1, w2, w3] and b is a number ans x is a vector of [x1, x2, x3]. Here n = 3;
         algebra would index these vectors 1-3, but Python indexes them 0-2.
        *Without vectorization you go through [w1]*[x1] + [w2]*[x2] + [w3]*[x3] + b...with vectorization, in numpy you can do
         numpy.dot(w,x) + b.
        *It's faster than other ways (manual for each pair, for loop) because numpy uses parallel hardware in your computer.
   Vectorization Part 2
   **** *Example: Look at an example without vectorization (a for loop) and example with vectorization (np.dot(w,x))
        *Without vectorization at each timestamp it will calculate a wx+b function...one step at a time
        *With vectorization (np.dot(x)) computer multiplies each pair of w and x with each other in parallel in one step, then
         uses specialized hardware to add up the w*x products efficently rather than 1 by 1; matters with large data sets
        *Example of how this helps with multiple linear regression: problem with 16 features/parameters, in addition to parameter b. 
         You calculate 16 derivative terms for the 16 weights; and you store the value of the weights and the derivatives in two numpy 
         arrays. If you want to compute an update step without vectorization, you'd have to individually do w1 = w1 - 0.1(derivative of
         j at w1), w2 = w2 - 0.1(derivative of j at w2), and so on until the 16 different features are accounted for. With
         vectorization, you can do  w = w - 0.1*d and the computer will do parallel processing on the numpy arrays
        *Overall, vectorization is just quicker
   Gradient Descent for Multiple Linear Regression
   **** *This video will put multiple linear regression, gradient descent, and vectorization together.
        *Recap: Previous notation is parameters (w1...wn, b), model (f_w,b(x) = w1x1 + w2x2 + ... + wnxn + b), 
         cost function (J(w1...wn, b)). Vectorized notation is parameters w = vec[w1 ... n] (a vector of length n) and b still being
         a number; model is f_w[vec],b(x[vec]) = (w[vec] * x[vec]) + b, with w[vec] * x[vec] as the dot product; 
         cost function is J(w[vec], b). Gradient descent without vectorization is 
         repeat until convergence {w - alpha*([d/dw]J(w1..wn,b)); b - alpha*([d/db]J(w1....wn,b)) and with vectorization is
         repeat until convergence {w - alpha*([d/dw]J(w[vec],b)); b - alpha*([d/db]J(w[vec],b)).
        *Gradient descent for one feature is 
         repeat until convergence {w = w - alpha*(1/(m)) *(sum of (f_w,b(x(i)) minus y(i))*x(i)) over all of the training examples;
         b = b - alpha*(1/(m)) *(sum of (f_w,b(x(i)) minus y(i))) over all of the training examples}.
        *Gradient descent for multiple features is 
         repeat until convergence {w1 = w1 - alpha*(1/(m)) *(sum of (f_w[vec],b(x[vec](i)) minus y(i))*x1(i))
         .
         .
         .
         wn = wn - alpha*(1/(m)) *(sum of (f_w[vec],b(x[vec](i)) minus y(i))*xn(i)) over all training examples;
         b = b - alpha*(1/(m)) *(sum of (f_w[vec],b(x[vec](i)) minus y(i))) over all of the training examples}.
        *An alternative to gradient descent is the NORMAL equation. It works only for linear regression (and really nothing else)
         for solving for w and b without iteration. Disadvantages are this does not generalize to other learning algorithms and is slow
         on large datasets. Some machine learning libraries use normal equation on the backend...but gradient descent is recommended.

* GRADIENT DESCENT IN PRACTICE
  Feature Scaling Part 1
  **** *When the possible range of values of a feature is large, a good model typically learns to choose a typically small parameter value
        and vice versa
       *This is important and can be seen in plotting points and corresponding cost functions; the variable with the much larger range of 
        values typically hasa narrow contour because a small change to the parameter of that feature can typically have a large impact on the 
        cost J/prediction of the model, and vice versa again (because that small contour is multiplied by a very large number)
       *What happens in this scenario is gradient descent may end up bouncing up and down/back and forth for a long time before finding
        its global minimum (video shows that it can bounce back and forth in predictions/errors...see 
        https://medium.com/@yennhi95zz/6-a-beginners-guide-to-gradient-descent-and-feature-scaling-in-machine-learning-bbbeecbb5d51).
        This is why we SCALE FEATURES.
       *When we scale, contous look more normal and gradient descent find =s a much more direct path to the global minimum
