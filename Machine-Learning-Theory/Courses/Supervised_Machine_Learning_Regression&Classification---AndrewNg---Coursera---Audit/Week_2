WEEK 2: REGRESSION WITH MULTIPLE INPUT VARIABLES

* MULTIPLE LINEAR REGRESSION
  Multiple Features
  **** *Recap: In the original version of linear regression, we had a single input variable x (size of house) which we used to predict y
        (price of house). The model therefore was f_w,b(x) = wx + b.
       *Now we will know the number of bedrooms, the number of floors, and the age of the home.
       *Notation = For multiple input variables: x1, x2...xj (j being the specific input feature (column)); 
        n will equal the total number of features. x^i represents the ith training example (row/array of all input variables); this is
        sometimes referred to as a VECTOR including all of the input features of the ith training example, more specifically a ROW VECTOR.
        (x^i)_j is the value of feature j in the ith training example. Sometimes an arrow is drawn over x^i to show it is a vector.
        *Now the model is not f_w,b(x) = wx + b, but f_w,b(x) = w1x1 + w2x2 + w3x3 + w4x4 + b.
        *Example = 0.1x1 (size of house) + 4x2 (bedrooms) + 10x3 (number of floors) - 2x4 (age of house) + 80  (base price). A way to
        interpret is, if assuming the model is trying to predict the price of a house in thousands of dollars, the base price of a house is 
        80,000 ($1000*80). For every square foot, price will increase by $1000*.01, or $100. For each bedroom, the price increases by $4,000;
        for each floor, the price increases by $10,000; for each year the house has existed the price decreases by $2,000.
        *With n features, the model will be f_w,b(x) = w1x1 + w2x2 + ... + wnxn + b.
        *Notation for the above is f_w[vec],b(x[vec]) = (w[vec] * x[vec]) + b. The dot product of w[vec] * x[vec] is the sum of 
         w1x1 + w2x2 + ... + wnxn.
        *This type of linear regression wh multiple input variables is MULTIPLE LINEAR REGRESSION.
   Vectorization Part 1
   **** *Vectorization makes your code shorter and makes it run more efficiently. Also allows you to take advantage of numerical linear 
         algebra libraries, as well as GPU hardware (speeds up computer graphics).
        *Example of vectorization: w is a vector of [w1, w2, w3] and b is a number ans x is a vector of [x1, x2, x3]. Here n = 3;
         algebra would index these vectors 1-3, but Python indexes them 0-2.
        *Without vectorization you go through [w1]*[x1] + [w2]*[x2] + [w3]*[x3] + b...with vectorization, in numpy you can do
         numpy.dot(w,x) + b.
        *It's faster than other ways (manual for each pair, for loop) because numpy uses parallel hardware in your computer.
   Vectorization Part 2
   **** *Example: Look at an example without vectorization (a for loop) and example with vectorization (np.dot(w,x))
        *Without vectorization at each timestamp it will calculate a wx+b function...one step at a time
        *With vectorization (np.dot(x)) computer multiplies each pair of w and x with each other in parallel in one step, then
         uses specialized hardware to add up the w*x products efficently rather than 1 by 1; matters with large data sets
        *Example of how this helps with multiple linear regression: problem with 16 features/parameters, in addition to parameter b. 
         You calculate 16 derivative terms for the 16 weights; and you store the value of the weights and the derivatives in two numpy 
         arrays. If you want to compute an update step without vectorization, you'd have to individually do w1 = w1 - 0.1(derivative of
         j at w1), w2 = w2 - 0.1(derivative of j at w2), and so on until the 16 different features are accounted for. With
         vectorization, you can do  w = w - 0.1*d and the computer will do parallel processing on the numpy arrays
        *Overall, vectorization is just quicker
         
