WEEK 3: ADVICE FOR APPLYING MACHINE LEARNING

* ADVICE FOR APPLYING MACHINE LEARNING
  Deciding What to Try Next
  **** *If you get large errors, you can...
        - Get more training examples
        - Try smaller sets of features
        - Try getting additional features
        - Try adding polynomial features
        - Try decreasing lambda
        - Try increasing lambda
       *This week we will learn how to carry out a diagnostic (a way to test what is or isn't working with an algorithm, to see what is or
        isn't working with a learning algorithm, to gain guidance to improving its performance). They can take time to implement, but are 
        very useful.
  Evaluating a Model
  **** *When you have a model with a single feature predicting a single target, easy to graph; when more, not really feasible (How do you
        plot a more than 3 dimensional graph?); this is why we do fractions of dataset for training and testing (gives a way to 
        systematically evaluate how well the algorithm is doing, as opposed to needing to plot or graph)
       *Ex: Fourth order polynomial for a five datapoint training set fits the data really well, but does not generalize well
       *Training/test error for regression = Squared error cost function over training/test examples...good to compare the two to see how 
        well the function generalizes
       *Training/test error for classification = Logistic cost function over training/test examples (regularize during training)...best way
        to do it is figure out what the fraction of the training set and the fraction of the test set which the function misclassified
  Model Selection and Training/Cross Validation/Test Sets
  **** *Training error typically performs lower than the actual testing/generalization error, as the parameters are fit to the training
        data, generalization error being the average error on new examples that were not in the training set
       *Choose a model based on order of polynomial...can be flawed to use the test error of this because extra degrees of polynomials
        are chosen using the test set
       *This is why we split the data into training, cross validation, and testing...split into the three subsets, use squared error for
        cost function,...if you have dev set, you now can test your nth-order polynomial function, rather than it being decided by your
        test set
       *Cross validation means to cross check the validity/accuracy of different models...also called the validation set, or the
        development/dev set
       *Choose based on lowest cross validation error when you have to make decisions such as fitting the model or choosing the model
        architecture...parameters, order not affected by test dataset
  
* BIAS AND VARIANCE
  Diagnosing Bias and Variance
  **** *High bias is training cost is high and cross validation cost is high (the costs approximate each other)
       *High variance is training cost is low and cross validation cost is high (cross validation cost is higher than training cost)
       *Just right = training cost is low and cross validation cost is low
       *As the degree of polynomial increases, training cost goes down, but cross validation cost goes down but back up 
        (starts by underfitting and then overfits)
       *High bias and high variance = training cost is high (and cross validation cost is greater than training cost)
  Regularization and Bias/Variance
  **** *Recap = We saw how degree of polynomial affects bias and variance
       *This video discusses how regularization (and its parameter lambda) affects bias and variance
       *Example = Set lambda to 10,000...when the regularization parameter is large that motivates the algorithm to keep the w parameters
        very small and you end up with f_w[vec],b(x[vec]) = b, or a constant value. This is a high bias model that underfits/doesn't even
        do well on the training set...training cost is large.
       *Example = Set lambda to 0...when the regularization is small (and there is really no regularization) you end up with an overfit.
        This is a high variance model that overfits, where training cost is small, but cross validation cost is large.
       *The goal as stated before is a small training cost and a small cross validation cost; cross validation is a good way to find a 
        good lambda value.
       *Choose based on cross validation trial and error of different lambda values; minimizing the training cost and calculating a 
        cross validation cost
       *Larger lambda is, the larger the training cost (more weight given to regularization term...parameters are tiny, underfits even 
        training); if lambda is too large or too small, the larger the cross validation cost (overfits if lambda is small, underfits if
        lambda is large, as stated in above example bulletpoints). It's actually a mirror image of the effect degree of polynomial has on
        training cost and cross validation cost; cross validation cost is high if degree of polynomial is too small or too large because
        underfits if a small degree of polynomial but overfits if a large degree of polynomial, training cost decreases as degree of 
        polynomial increases/gets larger because the model is more complex. Essentially, high bias/underfitting was on the left of the 
        degree of polynomial graph and on the right of the lambda graph, while high variance/overfitting was on the right of the degree
        of polynomial graph and the left of the lambda graph.
  Establishing a Baseline Level of Performance
  **** *This video discusses concrete numbers (hypothetically) of what training cost and cross validation cost may be, and how to judge
        high variance or high bias. Uses speech recognition.
       *Example: What if training cost is 10% (meaning the model transcribes 90% of the audio clips that it's trained on perfectly), but
        cross validation cost is 15%?
       *For the example, it would seem like even the training cost is high which means high bias (underfit), but also need to consider
        human level performance (how well humans can transcribe speech from the audio clips)...if you hypothetically find human level
        cost is 10%, and you find there are just some audio clips where really no one can accurately transcribe what was said, then you 
        cannot expect the learning algorithm to do much better. Essentially, first and foremost see if the training error is much higher
        than a human level of performance. In this example, when benchmarking to human level performance, training error can actually
        be said to be low, but cross validation error is high, meaning it would actually more so be high variance (overfit).
       *Other ways to establish a baseline are competing algorithms performance, or guesses based on performance.
       *If difference between baseline and training cost is high, then problem is high bias; if difference between training cost and
        cross validation cost is high, then you have high variance
