WEEK 3: ADVICE FOR APPLYING MACHINE LEARNING

* ADVICE FOR APPLYING MACHINE LEARNING
  Deciding What to Try Next
  **** *If you get large errors, you can...
        - Get more training examples
        - Try smaller sets of features
        - Try getting additional features
        - Try adding polynomial features
        - Try decreasing lambda
        - Try increasing lambda
       *This week we will learn how to carry out a diagnostic (a way to test what is or isn't working with an algorithm, to see what is or
        isn't working with a learning algorithm, to gain guidance to improving its performance). They can take time to implement, but are 
        very useful.
  Evaluating a Model
  **** *When you have a model with a single feature predicting a single target, easy to graph; when more, not really feasible (How do you
        plot a more than 3 dimensional graph?); this is why we do fractions of dataset for training and testing (gives a way to 
        systematically evaluate how well the algorithm is doing, as opposed to needing to plot or graph)
       *Ex: Fourth order polynomial for a five datapoint training set fits the data really well, but does not generalize well
       *Training/test error for regression = Squared error cost function over training/test examples...good to compare the two to see how 
        well the function generalizes
       *Training/test error for classification = Logistic cost function over training/test examples (regularize during training)...best way
        to do it is figure out what the fraction of the training set and the fraction of the test set which the function misclassified
  Model Selection and Training/Cross Validation/Test Sets
  **** *Training error typically performs lower than the actual testing/generalization error, as the parameters are fit to the training
        data, generalization error being the average error on new examples that were not in the training set
       *Choose a model based on order of polynomial...can be flawed to use the test error of this because extra degrees of polynomials
        are chosen using the test set
       *This is why we split the data into training, cross validation, and testing...split into the three subsets, use squared error for
        cost function,...if you have dev set, you now can test your nth-order polynomial function, rather than it being decided by your
        test set
       *Cross validation means to cross check the validity/accuracy of different models...also called the validation set, or the
        development/dev set
       *Choose based on lowest cross validation error when you have to make decisions such as fitting the model or choosing the model
        architecture...parameters, order not affected by test dataset
  
