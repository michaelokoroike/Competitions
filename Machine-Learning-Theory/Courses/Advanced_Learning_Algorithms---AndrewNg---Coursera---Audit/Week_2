WEEK 2: NEURAL NETWORK TRAINING

* NEURAL NETWORK TRAINING
  TensorFlow Implementation
  **** *We are given the code that is used to code a neural network in TensorFlow
       *Step 1: Import tensorflow (and Sequential and Dense from tensorflow)
       *Step 2: Create the simple model (Sequential([Dense(units=25, activation='sigmoid')...])
       *Step 3: Ask tensorflow to compile and choose a loss function (ex: model.compile(loss=BinaryCrossentropy())
       *Step 4: Call the fit function (ex: model.fit(x, y, epochs=100)); epoch is determining how many steps (in gradient descent terms) you
        may want to run
       *Understand how algorithms and things work so that if the training doesn't work correctly initially, you can debug based on your 
        conceptual understanding of what you are doing
  Training Details
  **** *Recap = Loss function is error of one training example; cost function is average error over all training examples
       *The same model training steps done for logistic regression can be done for neural networks (specify how to compute model given input
        x and parameters w and b by defining the model; then specify loss and cost; then train on data to minimize cost)
       *Step 1: Create the simple model (model = Sequential([Dense(...), Dense(...), Dense(...)])
       *Step 2: Use model.compile to define the loss function (model.compile(loss='BinaryCrossentropy'))
       *Step 3: Call function to minimize the cost as a function of the parameters of the neural network (model.fit(X,y, epochs=100))
       *Go over Step 1 in detail; this defines the architecture of the neural network.
       *Go over Step 2 in detail; this specifies the loss function which defines the cost function used for the neural network. Typically
        same as for logistic regression, which is -y(i)*log(f_w[vec],b_(x[vec](i)))-(1 - y(i))*log(1 - (f_w[vec],b_(x[vec](i)))), where y(i)
        is the ground truth (target label) for a training example i, and f_w[vec],b_(x[vec](i)) is the neural network output prediction. In
        TensorFlow, this is referred to as binary crossentropy because the function itself is referred to as crossentropy and binary just
        reemphasizes that it is a binary classification problem. Example for a regression problem would be if you wanted to minimize a 
        squared error loss, and choosing model.compile(loss='MeanSquaredError()'). Cost is with respect to all of the parameters in a 
        neural network.
       *Go over Step 3 in detail; this runs gradient descent (epochs is number of steps to take). The key part of the computation is the
        partial derivative term; TensorFlow ses BACKPROPAGATION to compute the derivative terms.
       *Interesting note; Keras started out as its own project but was merged into TensorFlow. Libraries are mature so good to work
        with them, but also good to implement from scratch and know how things work under the hood.

* ACTIVATION FUNCTIONS
  Alternatives to the Sigmoid Function
  **** *Recap = We've essentially to this point looked at neural networks as algorithms with a bunch of logistic regression equations
        (sigmoid functions) bunched together; but this does not have to be the case.
       *Example = In the demand prediction example, we viewed the parameter of awareness as a binary parameter (they are either aware as
        a customer or they are not), in which case the sigmoid function was used; but there is the possibility it could be the range of
        nonnegative numbers (an infinite scale)...some are aware and more aware than others, while others may be aware but less so
       *Common non-sigmoid function is g(z) = max(0, z) which can take on 0 or any nonnegative value; this is called ReLU (stands for
        Rectified Linear Unit)
       *Another common one is LINEAR ACTIVATION FUNCTION where g(z) = z
       *Just to clarify, when using SIGMOID, g(z) = 1/(1 + e^-z)
  Choosing Activation Functions
  **** *Depending on the ground truth/target label y is, there will be a natural choice for the activation function for the output layer
       *You can choose different activation functions for the hidden layers/different neurons in the neural network
       *Example: When doing a binary classification problem, use sigmoid as the output layer
       *Example: If trying to predict whether the stock price will go up or down, the solution can be positive or negative, in which case
        ReLU wouldn't be great because the prediction would be 0 or higher...but it would be great to use linear activation function which
        can predict positive and negative numbers
       *Example: If trying to predict the price of a house, which can never be negative, using ReLU may be the most efficient
       *ReLU is the most common choice hidden layers; efficient (just requires computing the max of 0 and a computed number), and it only
        goes flat in one part of the graph (as opposed to sigmoid where it goes flat in multiple) which makes gradient descent faster
        (when graphs have places where there are a lot of flat areas, gradient descent is slow)
       *Summary = For output layer (use sigmoid for binary classification; linear for regression that can be positive or negative; ReLU
        if y can only take on positive/nonnegative values); for the hidden layers use ReLU as the default
       *Example code = (model = Sequential([Dense(units=25, activation='relu'), Dense(units=15, activation='relu'), 
                                            Dense(units=1, activation='sigmoid')])
       *Many other activation functions to choose from
  Why Do We Need Activation Functions?
  **** *Video goes over both why we need activation functions and why neural networks don't work if you just use linear activation functions
        in every neuron
       *Using linear activation functions will render the neural network as nothing more than a linear regression function; this would defeat
        the purpose of creating a neural network because it would not be able to fit anything more complex (see video); a linear function
        of a linear function is itself a linear function; if all hidden layers and the output layers are linear then it would be basically
        a linear regression, but if it were linears for all of the hidden layers and a sigmoid for the activation, then it becomes a logistic
        regression (and the neural network doesn't do anything that you can't just do with logistic regression)...this is why a common rule
        of thumb is to not use linear if you don't have to, and to default to ReLU

* MULTICLASS CLASSIFICATION
  Multiclass
  **** *Multiclass classification = Classification problem where you can have more than two outputs possible (not just 0 or 1)
       *Example: If you're trying to determine a zip code, there's 10 possible digits to choose from/recognize (but still cannot be 
        any value)
       *We will look at softmax for this, which is a generalization of the logistic regression algorithm
  Softmax
  **** *Recall = Logistic regression is used when y can take on two possible output values (z = w[vec]*x[vec] + b; a = g(z) = 1/(1+e^(-z))
        = P(y = 1|x), or probability that y is 1 given input x
       *You can essentially with logistic regression have an activation where P(y = 1|x), and also one where P(y = 0|x)...probability that
        y = 0 is essentially 1 - the probability that y is 1 (based on the fact those are the only two outputs)...softmax just
        generalizes this
       *Number of examples of one class/number of examples in total = activation function for a neuron
       *Equation: a_j = e^z_j/(sum of all e^z's) with j as one of the classes, or P(y = j|x[vec])
       *Softmax becomes the same as logistic regression when only two outputs possible
       *Loss function is -log(a_j) if y = j for every instance/class (left half of a bowl)
