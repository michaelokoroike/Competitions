WEEK 1: NEURAL NETWORKS

* NEURAL NETWORKS INTUITION
  Welcome!
  **** *This course will teach us about neural networks (deep learning algorithms) and decision trees
       *Also we will learn practical advice on how to build machine learning systems (collecting more data; using a bigger GPU; etc)
       *This week we will learn inference from a neural network; next week is training; third week is the practical advice; final week is 
        decision trees
  Neurons and the Brain
  **** *Original motivation of creating neural networks was to create an algorithm that mimics how the brain works; we will look at how the
        brain works
       *Work on neural networks started in the 1950s and then fell out of favor for a while; but regained in popularity in the 1980s and 
        1990s; fell out of favor in the late 1990s again, but enjoyed another resurgence in 2005 and on (rebranded with deep learning)
       *First area neural networks/deep learning had a significant influence on was speech recognition (Geoff Hinton, others)
       *After speech recognition was computer vision
       *After computer vision was text/natural language processing
       *Now used in everything from climate change to online advertising to medical imaging to product recommendation and more
       *The neural networks of today have nothing to do with how the brain learns; shown structure of neuron (cell body, inputs/dendrites,
        output/electrical impulses through axon); the neural networks of deep learning (in simplified terms) have a neuron that takes some
        inputs (which are just numbers) does some computation and outputs another number, which then is likely an input to a second neuron
       *An aside; Andrew thinks we don't really know how well the brain works (fundamental breakthroughs that occur every few years in
        neuroscience)...don't take the biological motivation too seriously
       *Neural networks have grown in popularity because with phones/overall digitization of our society, there's more data; we saw as you
        fed traditional algorithms like linear regression and logistic regression more data, the performance wasn't continuing to increase
        and weren't scaling; but neural networks performed better in general and the larger the neural network, the better the performance;
        also why GPUs have taken off   
  Demand Prediction
  **** *Uses an example of if a tshirt will be a bestseller or not (yes or no)
       *Classification problem: x (price) is input, f(x) is 1/(1+e^-(w[vec]*x[vec] + b)) because we use sigmoid function; in neural
        networks, f(x) becomes termed a for activation (neuroscience term for how much a neuron is sending another neuron)
       *The logistic regression unit can be thought of as a very simplified neuron in the brain (takes as input the price x, computes the
        sigmoid function, and outputs number a (probability of being a top seller in the example)
       *Complex version of the example; x is (price, shipping cost, marketing, material); price and shipping cost are both input to one
        neuron (affordability), marketing the sole input of another neuron (awareness), price and material the inputs of another neuron
        (perceived quality); those three numbers are then inputted to a single neuron (through a logistic regression unit) and the 
        predicted probability is outputted. The three neurons (and the single neuron) are grouped into a LAYER. The one neuron layer is
        called the OUTPUT LAYER. Affordability, awareness, and perceived quality are the ACTIVATIONS (these activations are essentially
        layer outputs). Price, shipping cost, marketing, material is called the INPUT LAYER.
       *In large neural networks, each neuron has access to every feature/value from the previous layer; on us to learn to ignore what is
        irrelevant.
       *Notation = x[vec] (input layer) + a[vec] (activation layer)
       *The activation layer is called the HIDDEN LAYER.
       *Remove the input layer; it's just logistic regression.
       *4 numbers (input layer) to 3 numbers (hidden layer) to 1 number (output layer).
       *Shown multiple hidden layer examples (called MULTILAYER PERCEPTRON)
  Example: Recognizing Images
  **** *You want a neural network, in the example, to take an image of a person in the picture and output what the identity is of the 
        person in the picture.
       *Representation of a 1000x1000 pixel picture in a computer is a 1000x1000 grid (MATRIX) of 1000 rows and 1000 columns
       *If you unload the values of this list and unload them into a vector, you get a vector of a million pixel intensity values.
       *You would train a neural network to take that vector of values, input them/extract new features into a 1st hidden layer, then a
        2nd hidden layer, then a 3rd hidden layer, and finally to the outputnlayer which outputs the probability it is a particular
        person. How it would work is in the 1st layer, the neurons may be looking for short lines or edges in the image; the 2nd layer
        may learn to group together little short lines or edges in order to look for parts of faces; the 3rd layer would then
        aggregate the parts of faces into faces/face shapes, and try to learn and predict faces...the farther down the line of layer,
        the larger size the region which the neurons look at (activations are higher level features).

* NEURAL NETWORK MODEL
  Neural Network Layer
  **** *Layers of neurons are the fundamental building blocks of neural networks
       *Essentially, each neuron in a layer is a little logistic regression unit, where the activation (output of the neuron to another
        neuron) can be thought of as, in a logistic regression sense, g(w[vec]*x[vec] + b) where g is a sigmoid function...this is for 
        each neuron (an aside; number the layers...the notation, for example, of the activation vector of the first layer is a[vec]^[1], and the 
        parameters for the first and second neuron in the first layer is w[vec]^[1]_1 and w[vec]^[1]_2 (square brackets are layer notation).
       *In the example used, in the second and final layer (with only 1 neuron), the activation can be thought of as 
        g(w[vec]^[2]_1*a[vec]^[1] + b^[2]_1), with parameters from neuron 1 in the second layer, multiplied by the activation vector output
        from the first layer. This is set to a^[2]_1, or is the value of the activation for the first neuron in the second layer.
       *In the example used, in the final aspect (no more layers), you predict..in this example, is a^[2]_1 greater than or equal to 0.5?
        If yes, then y-hat (the prediction) is 1, and if no, y-hat is 0. Threshold is changeable.
  More Complex Neural Networks
  **** *Example of the notation of a neural network with four layers = 
        x[vec] (input) ---> a[vec]^1 (activation/output of hidden neuron layer 1) ---> 
        a[vec]^2 (activation/output of hidden neuron layer 2) ---> a[vec]^3 (activation/output of hidden neuron layer 3) --->
        a[vec]^4 (activation/output of output neuron layer 4)
       *Recap = Subscripts of w and b in neural network notation denote the neuron within the layer which the figures correspond with, while
        within the superscript brackets are the current overall layer which the neuron is in (relative to the neural network); say the 
        superscript value first
       *General notation : a^[l]_j = g(w[vec]^[l]_j*a[vec]^[l-1] + b^[l]_j). This gives you the activation of layer l, unit (neuron) j.
        Parameters w and b of layer l, unit j. W is multiplied by output of l - 1 (which is the previous layer). G is the activation
        function; typically the sigmoid function but can be otherwise. We can also say x[vec] = a[vec]^[0] (input layer).
  Inference: Making Predictions (Forward Propagation)
  **** *Example of handwritten digit recognition
       *Binary classification problem: is the image 0 or is it 1?
       *Each image is an 8x8 grid, or 64 pixel intensity values ranging from 0 to 255 (255 denotes bright white and 0 denotes a black pixel)
       *Create a neural network with two hidden layers (first hidden layer has 25 neurons, second hidden layer has 15 neurons, output layer
        has 1 unit); in the example (see below)...
       *To get from x[vec] (input layer) to a[vec]^1 (first hidden layer) = {g(w[vec]^[1]_1*x[vec] + b^[1]_1) 
                                                                             ... 
                                                                             g(w[vec]^[1]_25*x[vec] + b^[1]_25)}
       *To get from a[vec]^1 (first hidden layer) to a[vec]^2 (second hidden layer) = {g(w[vec]^[2]_1*a[vec]^1 + b^[2]_1) 
                                                                                       ... 
                                                                                       g(w[vec]^[2]_15*a[vec]^1 + b^[2]_15)}
       *To get from a[vec]^2 (second hidden layer) to a[vec]^3 (output layer) = {g(w[vec]^[3]_1*a[vec]^2 + b^[2]_1)}; only 1 unit
       *Then, is a^[3]_1 greater than or equal to 0.5? If yes, guess image as digit 1, but if not, guess image as digit 0.
       *Because the process goes from left to right, the process is called FORWARD PROPAGATION because we are propagating the activations
        of neurons.

* TENSORFLOW IMPLEMENTATION
  Inference in Code
  **** *Uses coffee beans as an example
       *The way to view it is if undercooked (too low of a temperature), not good; if too short of a duration, not good; overcooked 
        (too long duration or too high a temperature)
       *The question is with a feature vector x with both temperature and duration, how do we do inference in a neural network to tell us
        whether or not a temperature and duration setting will result in good coffee or not
       *#1 - Set x to be an array of two numbers (representing the two variables temperature and duration)
       *#2 - Create hidden layer 1 as a Dense variable (units=how many neurons in the layer, activation='sigmoid'/etc)...Dense is a layer
       *#3 - Apply the values of x to the function created in step #2 (the Dense layer is a function); this is a1
       *#4 - Create hidden layer 2 as a Dense variable (units=how many neurons in the layer, activation='sigmoid'/etc)
       *#5 - Apply the values of a1 to the function created in step #2 (the Dense layer is a function); this is a2
       *#6 - Make an inference based on threshold
       *Brings back the handwritten digit classification function

  Data in TensorFlow
  **** *Consistent framework for creating neural networks...inconsistencies with how data is represented in NumPy and in TensorFlow due
        to discrepancy in the times when both platforms were created...good to be aware of conventions of each (we're starting with
        TensorFlow)
       *We are shown the notation of a matrix in numpy (each row is a bracket within, columns are the amount of items in each bracket);
        example is np.array([[2,3,4],[5,6,7]])...2,3,4 and 5,6,7 are two rows with three columns and a 2x3 matrix...outer square brackets
        groups them together...matrix just is a 2d array of numbers...notation is number of rows by columns...matrices can have
        different dimensions
       *Examples of np.arrays: [[200, 17]] is a 1x2 matrix (row vector); [[200], [17]] is a 2x1 matrix (column vector); [200, 17] is
        a 1d vector with no rows or columns and is just a list of numbers
       *With TensorFlow, instead of the 1d vectors we used with NumPy and algorithms in the first course, we use the matrices; this allows
        TensorFlow to allow very large datasets...handling data in matrices instead of 1d arrays allows TensorFlow to be a little more 
        computationally efficient
       *Going back to the activation vector example in "Inference: Making Predictions (Forward Propagation)" section above, 
        a[vec]^1 is a 1x25 matrix...will print out tf.Tensor (float32 is decimal point using 32 bits of memory; Tensor is a data type
        used to store and carry out computations on matrices efficiently...more than anything, a way of representing matrices)
       *The tf.Tensor and np.array are the two different ways to represent a matrix...tensors can be easily converted to numpy arrays by
        for example, if a1 was a tensor, doing a1.numpy()
       *This is important because you may be used to uploading and working with data in Numpy, but then when you try to work with 
        TensorFlow, it converts it to its own internal format...you have the option to onvert back to numpy
  Building a Neural Network
  **** *This video shows how to build a neural network in TensorFlow; learn about a different way of building a neural network that will
        be simpler than we have seen so far
       *Recall = How to do forward propagation
       *TensorFlow has a different way of carrying out forward prop as well as learning; instead of the examples #1 through #6 in the 
        Inference in Code section, you can instead create the Dense layers and string them together with the Sequential function in 
        TensorFlow...it's the equivalent of manually passing data from layer to layer
       *ex: model = Sequential([layer_1, layer_2])
       *To train on a model, you call model.compile() and model.fit(x,y)
       *To infer, you'd then do model.predict(x_new), with x_new being examples that weren't trained on
       *To recap example of a neural network in code (1 is create the Dense layers; 2 is create the model and link the layers using
        Sequential; 3 is initialize the x and the y; 4 is model.compile; 5 is model.fit(x,y); 6 is model.predict(x_new))
       *Usually the layers are initialized within calling the Sequential (example: model: Sequential([Dense(units=3, activation='Sigmoid'),
        ...]))
       *Good to know what code is actually doing; rarely are neural networks done in Python but we will so that we can implement it from 
        scratch and really understand how it works.

* NEURAL NETWORK IMPLEMENTATION IN PYTHON
