WEEK 1: NEURAL NETWORKS

* NEURAL NETWORKS INTUITION
  Welcome!
  **** *This course will teach us about neural networks (deep learning algorithms) and decision trees
       *Also we will learn practical advice on how to build machine learning systems (collecting more data; using a bigger GPU; etc)
       *This week we will learn inference from a neural network; next week is training; third week is the practical advice; final week is 
        decision trees

  Neurons and the Brain
  **** *Original motivation of creating neural networks was to create an algorithm that mimics how the brain works; we will look at how the
        brain works
       *Work on neural networks started in the 1950s and then fell out of favor for a while; but regained in popularity in the 1980s and 
        1990s; fell out of favor in the late 1990s again, but enjoyed another resurgence in 2005 and on (rebranded with deep learning)
       *First area neural networks/deep learning had a significant influence on was speech recognition (Geoff Hinton, others)
       *After speech recognition was computer vision
       *After computer vision was text/natural language processing
       *Now used in everything from climate change to online advertising to medical imaging to product recommendation and more
       *The neural networks of today have nothing to do with how the brain learns; shown structure of neuron (cell body, inputs/dendrites,
        output/electrical impulses through axon); the neural networks of deep learning (in simplified terms) have a neuron that takes some
        inputs (which are just numbers) does some computation and outputs another number, which then is likely an input to a second neuron
       *An aside; Andrew thinks we don't really know how well the brain works (fundamental breakthroughs that occur every few years in
        neuroscience)...don't take the biological motivation too seriously
       *Neural networks have grown in popularity because with phones/overall digitization of our society, there's more data; we saw as you
        fed traditional algorithms like linear regression and logistic regression more data, the performance wasn't continuing to increase
        and weren't scaling; but neural networks performed better in general and the larger the neural network, the better the performance;
        also why GPUs have taken off
        
  Demand Prediction
  **** *Uses an example of if a tshirt will be a bestseller or not (yes or no)
       *Classification problem: x (price) is input, f(x) is 1/(1+e^-(w[vec]*x[vec] + b)) because we use sigmoid function; in neural
        networks, f(x) becomes termed a for activation (neuroscience term for how much a neuron is sending another neuron)
       *The logistic regression unit can be thought of as a very simplified neuron in the brain (takes as input the price x, computes the
        sigmoid function, and outputs number a (probability of being a top seller in the example)
       *Complex version of the example; x is (price, shipping cost, marketing, material); price and shipping cost are both input to one
        neuron (affordability), marketing the sole input of another neuron (awareness), price and material the inputs of another neuron
        (perceived quality); those three numbers are then inputted to a single neuron (through a logistic regression unit) and the 
        predicted probability is outputted. The three neurons (and the single neuron) are grouped into a LAYER. The one neuron layer is
        called the OUTPUT LAYER. Affordability, awareness, and perceived quality are the ACTIVATIONS (these activations are essentially
        layer outputs). Price, shipping cost, marketing, material is called the INPUT LAYER.
       *In large neural networks, each neuron has access to every feature/value from the previous layer; on us to learn to ignore what is
        irrelevant.
       *Notation = x[vec] (input layer) + a[vec] (activation layer)
       *The activation layer is called the HIDDEN LAYER.
       *Remove the input layer; it's just logistic regression.
       *4 numbers (input layer) to 3 numbers (hidden layer) to 1 number (output layer).
       *Shown multiple hidden layer examples (called MULTILAYER PERCEPTRON)

  Example: Recognizing Images
  **** *You want a neural network, in the example, to take an image of a person in the picture and output what the identity is of the 
        person in the picture.
       *Representation of a 1000x1000 pixel picture in a computer is a 1000x1000 grid (MATRIX) of 1000 rows and 1000 columns
       *If you unload the values of this list and unload them into a vector, you get a vector of a million pixel intensity values.
       *You would train a neural network to take that vector of values, input them/extract new features into a 1st hidden layer, then a
        2nd hidden layer, then a 3rd hidden layer, and finally to the outputnlayer which outputs the probability it is a particular
        person. How it would work is in the 1st layer, the neurons may be looking for short lines or edges in the image; the 2nd layer
        may learn to group together little short lines or edges in order to look for parts of faces; the 3rd layer would then
        aggregate the parts of faces into faces/face shapes, and try to learn and predict faces...the farther down the line of layer,
        the larger size the region which the neurons look at (activations are higher level features).

* NEURAL NETWORK MODEL
  Neural Network Layer
  **** *Layers of neurons are the fundamental building blocks of neural networks
       *Essentially, each neuron in a layer is a little logistic regression unit, where the activation (output of the neuron to another
        neuron) can be thought of as, in a logistic regression sense, g(w[vec]*x[vec] + b) where g is a sigmoid function...this is for 
        each neuron (an aside; number the layers...the notation, for example, of the activation vector of the first layer is a[vec]^[1], and the 
        parameters for the first and second neuron in the first layer is w[vec]^[1]_1 and w[vec]^[1]_2 (square brackets are layer notation).
       *In the example used, in the second and final layer (with only 1 neuron), the activation can be thought of as 
        g(w[vec]^[2]_1*a[vec]^[1] + b^[2]_1), with parameters from neuron 1 in the second layer, multiplied by the activation vector output
        from the first layer. This is set to a^[2]_1, or is the value of the activation for the first neuron in the second layer.
       *In the example used, in the final aspect (no more layers), you predict..in this example, is a^[2]_1 greater than or equal to 0.5?
        If yes, then y-hat (the prediction) is 1, and if no, y-hat is 0. Threshold is changeable.
  
  More Complex Neural Networks
  **** *Example of the notation of a neural network with four layers = 
        x[vec] (input) ---> a[vec]^1 (activation/output of hidden neuron layer 1) ---> 
        a[vec]^2 (activation/output of hidden neuron layer 2) ---> a[vec]^3 (activation/output of hidden neuron layer 3) --->
        a[vec]^4 (activation/output of output neuron layer 4)
       *Recap = Subscripts of w and b in neural network notation denote the neuron within the layer which the figures correspond with, while
        within the superscript brackets are the current overall layer which the neuron is in (relative to the neural network); say the 
        superscript value first
       *General notation : a^[l]_j = g(w[vec]^[l]_j*a[vec]^[l-1] + b^[l]_j). This gives you the activation of layer l, unit (neuron) j.
        Parameters w and b of layer l, unit j. W is multiplied by output of l - 1 (which is the previous layer). G is the activation
        function; typically the sigmoid function but can be otherwise. We can also say x[vec] = a[vec]^[0] (input layer).

  Inference: Making Predictions (Forward Propagation)
  **** *Example of handwritten digit recognition
       *Binary classification problem: is the image 0 or is it 1?
       *Each image is an 8x8 grid, or 64 pixel intensity values ranging from 0 to 255 (255 denotes bright white and 0 denotes a black pixel)
       *Create a neural network with two hidden layers (first hidden layer has 25 neurons, second hidden layer has 15 neurons, output layer
        has 1 unit); in the example (see below)...
       *To get from x[vec] (input layer) to a[vec]^1 (first hidden layer) = {g(w[vec]^[1]_1*x[vec] + b^[1]_1) 
                                                                             ... 
                                                                             g(w[vec]^[1]_25*x[vec] + b^[1]_25)}
       *To get from a[vec]^1 (first hidden layer) to a[vec]^2 (second hidden layer) = {g(w[vec]^[2]_1*a[vec]^1 + b^[2]_1) 
                                                                                       ... 
                                                                                       g(w[vec]^[2]_15*a[vec]^1 + b^[2]_15)}
       *To get from a[vec]^2 (second hidden layer) to a[vec]^3 (output layer) = {g(w[vec]^[3]_1*a[vec]^2 + b^[2]_1)}; only 1 unit
       *Then, is a^[3]_1 greater than or equal to 0.5? If yes, guess image as digit 1, but if not, guess image as digit 0.
       *Because the process goes from left to right, the process is called FORWARD PROPAGATION because we are propagating the activations
        of neurons.
