WEEK 1: Derivatives and Optimization

* LESSON 1: DERIVATIVES
  Course Introduction
  **** *We will learn about derivatives, what they mea, how it works, how is it used in machine learning. Learn about advanced
        techniques in Newton's method
       *Machine learning is a lot of minimizing and maximizing functions...this is the main point of calculus (this minimizing and
        maximizing is done with derivatives).
       *Large fraction of machine learning is creating a cost function and minimizing it (this is calculus)...when you want to train 
        a machine learning model, you see the error and try to minimize it using derivatives, gradients, and more.
       *The optimizers are based on calculus.
       *Gradient descent is taking small steps to get lower and lower values.
       *Data can be very high dimensional and move in all different directions.
       *Newton's method is a way to minimize functions as an alternative to gradient descent.
  Machine Learning Motivation
  **** *We'll learn the intuition behind concept of derivative; example is velocity (
       *We'll also learn about linear functions, constant functions, quadratic polynomials, exponential functions, and logarithmic
        functions...we'll learn the derivatives of these
       *We'll learn sum rule, product rule, chain rule, and multiplication by scalars...to find derivatives of more complex functions
       *Derivatives are used to optimize functions (maximize or minimize them)
       *We'll learn about square loss and log loss (two most important loss functions in machine learning)
       *Video shows with line of best fit, you are given the method of predcting a value in a regression problem...shown the difference
        for regression and classification...going to learn the math underlying concepts4
  Motivation to Derivatives - Part 1
  **** *First thing when he thinks abut derivatives is velocity; a derivative is a instantaneous change of rate of a function...in driving
        the function is distance and the derivative is velocity
       *Note: Slope is change in distance/change in time
  Derivatives and Tangents
  **** *The derivative of a function at a point is the slope of the tangent at that point
  Slopes, Maxima and Minima
  **** *Horizontal lines have a slope of zero
       *Max or min of a function occurs where the derivative is zero
  Derivatives and their Notation
  **** *This video shows two ways to express a derivative; Leibniz's notation and LaGrande's notation
       *Recap that slope is defined by change in distance over change in time...can also be known as delta_x / delta_t...slope at a 
        point is dx/dt
       *Works by recalculating slpe at a point until the interval between delta_x and delta_t is so tiny that we officially have the
        tangent line
       *If y = f(x)...the notation y = f'(x) is LaGrange's Notation; dy/dx, or d/dx*f(x), is Leibniz's selection
  Some Common Derivatives - Lines
  **** *Goal is to be able to calulate derivatives of most functions
       *Constant/linear/quadratic
  Some Common Derivatives - Quadratics
  **** *The simplest quadratic is the parabola y = x**2, with the slope being the change of f (delta_f) over the change of x (delta_x), 
        as the change of x (delta_x) goes to 0
       *The slope can also be written as ((x + delta_x)**2 - (x)**2)/delta_x; in the example of y = x**2, if you decrease delta_x, delta_f
        decreases, and at x is 1, f approaches 2; so the slope at the tangent is 2
  Some Common Derivatives - Higher Degree Polynomials
  **** *A cubic function is y = x**3, in the shape of a long sguiggly vertical line, with the slope being the change of f (delta_f) over 
        the change of x (delta_x), as the change of x (delta_x) goes to 0
       *The slope can be written as ((x + delta_x)**3 - (x)**3)/delta_x; in the example of y = x**3, if you decrease delta_x, delta_f
        decreases, and at x is 1, f approaches 2; so the slope at the tangent is 3
  Some Common Derivatives - Other Power Functions
  **** *We are shown the 1/x function, which has the derivative of a hyperbola graph. The slope can be written as 
        ((x + delta_x)**-1 - (x)**-1)/delta_x
       *We are shown f'(x) = nx***n-1, if f(x) = x**n
