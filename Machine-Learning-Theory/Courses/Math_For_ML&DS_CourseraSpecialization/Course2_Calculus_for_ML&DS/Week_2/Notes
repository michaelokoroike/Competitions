WEEK 2: Gradients and Gradient Descent

* LESSON 1 - GRADIENTS
  Introduction to Tangent Planes
  **** *Will deal in functions with two or more variables; the idea of a tangent line in two dimensions generalizes in the idea of
        a tangent plane in three dimensions
       *Optimizing functions with two or more variables can be very complicated; gradient descent is a method that speeds it up
       *Already learned about derivatives for functions with one variable, with derivatie being the slope of the tangent line at each
        point on the curve; now with functions with two inputs and one output (3-d) the derivative is a tangent plane
       *To find tangent planes, cut the 3d graph at x = ? and y = ?, and you have it...see the video
  Partial Derivatives - Part 1
  **** *The sliced space is the partial derivative; you treat one variable as a constant to do so (ex: for f(x, y) = x2 + y2...if you
        make y a constant, you'll get the function of one variable x)...with y as a constant, partial derivative is 2x, and vice versa
        for x as a constant
  Partial Derivatives - Part 2
  **** *If f(x) = 3x2y3, partial derivative with respect to x is 6xy3...with respect to y is 9x2y2
  Gradients
  **** *Gradients essentially are vectors carrying partial derivatives (for f(x, y) = x2 + y2, gradient is [2x | 2y]
  Gradients and Maxima/Minima
  **** *Plane is parallel to the floor when at a local max or min (equals 0)
       *In 3d plane, local minimum is when the slopes of the partial derivatives of both tangent line given by partial derivatives are zero
       *All partial derivatives need to be zero to find a local minimum
  Optimization with Gradients: An Example
  **** *Multiple dimentions (3d) with same sauna example
  Optimization using Gradients: An Analytical Method
  **** *Reiteration; minimize the sum of squares cost
       *Tough to solve when there are multiple variables in a partial derivative

* LESSON 2 - GRADIENT DESCENT
  Optimization using Gradient Descent in one variable - Part 1
  **** *Example is f(x) = e^x - log(x); finding the minimum requires us finding f'(x) and setting it equal to 0, then solve for x...
        derivative of f(x) : e^x - 1/x = 0...hard to solve for but this equates to e^x = 1/x, with x equalling 0.5671 (omega constant)
       *Shown a guesstimation method that may not lead to the minimum but is close enough
  Optimization using Gradient Descent in one variable - Part 2
  **** *Logic says on a curve where the slove is positive, you want to subtract to be closer to a minimum, and when the slope is negative,
        you want to add to be closer to the minimum...so basically, the new point should be the old point - slope
       *The caveat is at certain parts of the slope, the derivative may be steep and the slope is large, causing a big and chaotic
        jump; these jumps may be so large that you actually overstep the minimum...so we want to ensure small steps are taken by adding
        a LEARNING RATE, making the equation for a new point the old point - (learning rate*slope)...this is good because then there
        are no outlandish steps and the steps are relative to position; think about golf and driving when far away from the hole and
        putting when close...same dynamic.
       *What's described above is GRADIENT DESCENT (do a gradient descent alg?)
  Optimization using Gradient Descent in one variable - Part 3
  **** *A learning rate can't be too large (you miss the minum) or too small (take forever to converge to the minimum)
       *Research problem to find a good learning rate
       *Another drawback of the difficulty of finding a good learning rate is in a function with a lot of curves, it may converge to 
        local minima and not global minima...you need to run it many times with many starting points (batch?)
