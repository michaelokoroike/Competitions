WEEK 3: Confidence Intervals and Hypthesis Testing

* LESSON 1 - Confidence Intervals
  Confidence Interals - Overview
  **** *Example: In the world population, you'll never have a way to measure the ccurate height of every individual, you have to get samples and rely on those sample means...but
        YOU'LL ALWAYS HAVE A DEGREE OF UNCERTAINTY HOW ACCURATE YOUR RESULTS IN ARE ACCORDANCE TO THE REALITY OF THE POPULATION. This is where confidence intervals come in.
       *Confidence intervals allow you to use sample means with A DEGREE OF CERTAINTY.
       *Confidenc intervals have a lower and upper limit that contain the population parameters with degree of certainty.
       *Example is you lose your key along the side of the road, and you and your friend go to look for it. You give a best guess as to where it may be, and then decide on a search 
        distance east and west to rtry and find it in case the guess may not be correct (which it likely isn't on the dot correct); what actually matters is the search distance, and
        how large it should be, based on how confident you'll be with that search distance to actually find the key, which is expressed as a percentage. The lower and upper limits are 
        the bounds of the search. The key, importantly, is unknown and fixed, while the interval is random (the guess) and generated with desired (un)certainty.
       *Going off the example, a 50% confidence level means not that 50% of keys are the confidence interval, but that 50% of guesses are within a search distance of the key.
       *Going off the example, the true mean value is where the key is...it has an unknown and fixed value...we randomly generate a confidence interval to estimate where it is 
        located based off of a random sample from the population. Example would be taking the height of 1 person and going from there; measure their height and use it to generate pop.
        mean (the one person's height is the sample mean, or x-bar)...(x and x bar have same mean, though we don't know true meaning of x?)...can't knowif the person is taller or shorter
        than average, so question to ask is how far are most sample means from the true population means? if typical, our sample mean will be in that range; to answer this we need a 
        MARGIN OF ERROR (distance on either side of mean) and a CONFIDENCE LEVEL (probability that sample mean is within that margin of error); you get these values by starting with a 
        SIGNIFICANCE LEVEL (probability that sample mean falls outside margin of error)...confidence level is 1 - significance level
       *CONFIDENCE INTERVAL IS SAMPLE MEAN (X BAR) PLUS/MINUS MARGIN OF ERROR
       *WHEN CONFIDENCE LEVEL IS 95%, 95% OF CONFIDENCE INTERVALS PERTAINING TO EACH INDIVIDUAL EXAMPLE (LIKE WITH A PERSON AND THEIR HEIGHT, VS POP. MEAN) WILL CONTAIN THE POP. MEAN,
        WHILE 5% WON'T
  Confidence Intervals - Changing the Interval
  **** *Mean of sample means = population mean (in theory)...doesn't depend on sample size
       *Standard deviation of sample means = population standard deviation divided by square root of number of individuals in population...does depend on sample size (makes sense...the
        larger the sample size, the more acurate representation of the population, hence sample means are going to be closer to the population mean, hence less variation); graphically,
        this looks like the distribution getting taller and having more outcomes concentrated nearer to the mean...in this case, with confidence levels remaining the same, the difference
        is with higher sample size, the confidence intervals become smaller while still generating the same levels of accuracy, because you have a more precise estimate of the true value
        of the population mean
       *When confidence level is lowered, what happens is smaller margins of error are used, and because of this, there end p being more instances where overall cofidence interval(s) does
        not contain the true population mean...but of note is the actual distribution (which is of sample means generated) is not changed by confidence level
       *Summary of key points: 1) confidence intervals are a sample mean with a margin of error added to each side; 2) confidence level is the probability that a confidence interval
        contains the population mean (like 95%); 3) ideally we have high confidence and a narrow interval; 4) larger samples give narrower interval; 5) decreasing confidence level
        will decrease confidence interval
  Confidence Intervals - Margin of Error
  **** *Given a general case of a population with standard mean and variance; with goal of trying to find average height of population (standard mean), you take a sample of size n and 
        calculate the sample mean x bar; the sample mean is probably a little taller or a little shorter than the true poplation mean (it's a random variable with standard mean, but
        variance divided by population), but close enough; as sample size increases, that sample man is going to theoretically converge towards population mean (note, you still don't 
        actually know the population mean); to make a confidence interval (knowing outliers/etc are possible in sample), use knowledge 68% of normal distribution is within 1 sd of mean,
        and 95% of the distribution is within 2 sd, and this is where Z-SCORE comes in (example: point 1 sd above mean has a Z SCORE of 1, and point 1 sd below mean has Z SCORE of -1)...
        Z SCORE of a point is found by subtracting mean and dividing by standard deviation (note; the standard nomal distribution is actually also called the Z DISTRIBUTION)
       *Actual Z scores for 2 sd are -1.96 (z_.025) and 1.96(z_.975), and are called CRITICAL VALUES
       *Knowing this, margin of error is sample mean +/- 1.96*sample standard deviation (populaion sd divided by sqrt of n number of samples; STANDARD ERROR)
       *All possible by Central limit Theorem (we can assume normality wity samples and population)
       *Note: What is standard deviation of a normal distribution?
  Interactive Tool: Confidence Intervals
  **** *What was shown in the video; multiple different intervals and 80% of them including the population mean
  Confidence Intervals - Calculation Steps
  **** *1) Find the sample mean
       *2) Define a confidence level
       *3) Get the critical value for this confidence level
       *4) Find the standard error (sample standard deviation, divided by sqrt of n in sample pop)...larger the population, sample statistics are all that can be drawn; this standard error
           is actually based off sample standard deviation, which is why it is divided by sqrt of n, to mathematically suggest as a SAMPLE gets larger, standard deviation shrinks because
           variation/uncertainty shrinks
       *5) Find the margin of error (criticsl value * standard error)
       *6) Add/subtractmargin of error to the sample mean
       *Assumptions - simple random sample, sample size > 30 or population is normal, 
  Confidence Intervals - Example
  **** *Example of a place with a population of 6000, but a sample size of 49; mean height of the samples is 1 meter (170 cm) and the standard deviation is 25 cm; we know margin of error 
        is standard deviation/sqrt of number of samples, so margin of error is the 1.96 (confidence level threshold) * sd (25) / sqrt of individuals in the sample (sqrt, 14, etc)
       *Question: Why again is the sample standard deviation one which we can be confident about? (it is part of the standard error, which is part of the margin of error, which both take
        into consideration the size of the sample; as the sample size gets bigger, the standard error and margin of error gets smaller, because we have a better estimate of true pop. mean,
        and essentially more data is concentrated closer around the mean, hence sd's get smaller...the thing is if something falls outside of a 95% confidence level, then it's smaller
        than or larger than 97.5% of the mean (or two sds), so by adding and subtracting the margin of error (z score of 2 sds, for instance, and the standard error), you are creating
        a 2 sd distribution for each sample mean generated, which will only not contained the population mean if outside of two sds of the true population distribution, which would be 
        outside of 95% of the data of the true population distribution...therefore, we can have 95% confidence about a true population value falling within a confidence interval based on 
        sample mean and standard deviation (sample statistics, which lead to confidence interval)
  Calculatng Sample Size
  **** *Example; we get a margin of error, but we decide we want a smaller one; we need a bigger sample size, but what is the smallest sample size that will allow us to obtain a smaller
        desired margin of error
       *Remember, margin of error is critical value * (sd/sqrt of sample size)...use values you have
  Difference Between Confidence and Probability
  **** *Population mean = fixed but unknown, no probability distributions, in the interval or not, does not fall within an interval 95% of the time
       *Sample mean = they have a probability distribution (sampling distributions of sample = values change; concept of confidence interval is tied to sample mean and changes based on
        the value of sample mean...95% confidence has to do with REPEATING A SAMPLING EXPERIMENT MANY TIMES AND CALCULATING THE INTERVLS FOR EACH SAMPLE ESTIMATE...95% OF THE TIME, THE
        CONFIDENCE INTERVALS WILL CONTAIN THE MEAN; IT IS NOT THE PROBABILITY THAT ONE SPECIFIC INTERVAL CONTAINS THE POPULATION MEAN (IT EITHER CONTAINS IT OR NOT), BUT RATHER THE 
        SUCCESS RATE OF CONSTRUCTING A CONFIDENCE INTERVAL
  Unknown Standard Deviation
  **** *Student t-distribution when we don't know standard deviation; it is simply using the sample standard deviation when the population one is unknown...no longer a normal distribution
        and has fatter tails (more variability)
       *Note: z-score is ok to use when it's a normal distribution...when it's not normal, use a T-SCORE; all else is the same in terms
       *note: T-distributions are defined by DEGREES OF FREEDOM (number of samples used, minus 1)...higher the degrees of freedom, the closer it is to a normal distribution
  Confidence Intervals for Proportion
  **** *For means, confidence intervals = sample means +/- margin of error (margin of error is z/t value * (sd/sqrt of n))
       *For proportions, confidence interval = sample proportion +/- margin of error (margin of error is z/t value * (sqrt of (p-hat*(1-p-hat))/sqrt of n))
       *Difference in calculation is in standard error and how it is calculated

* LESSON 2 - Hypothesis Testing
  Defining Hypotheses
  **** *Hypothesis Testing = A way to tell if some belief you have about the population is likely to be true or false
       *Good example = email spam detector to determine if a email is ham (good email) or spam (bad email). By default, we will assume that all the emails are ham/good because it's worse
        to delete a good email than to put a spam email in our inbox...this assumption is the NULL HYPOTHESIS (baseline assumption when we assume nothing is happening; notation is H_0...
        baseline)...the special assumption that we are trying to identify is the ALTERNATIVE HYPOTHESIS (notation is H_1...what you want to prove). The null and alternative hypotheses are 
        MUTUALLY EXCLUSIVE (example is email can't be ham and spam at the same time) and have a True/False answer. In examplw, if there's evidence that an email is spam, then the null 
        hypothesis is rejected and thealternative hypotesis (the email is spam)...if not enough evidence to show that the emaik is spam, you can reject the null hypothesis
       *Clarifying = If plenty of evidence against H_0, reject H_0/null (and accept H_1/alternative)...this action would lead tosending email to spam
  Type 1 and Type 2 Errora
  **** *With email example, either we send email to spam or to regular inbox...either could be wrong
       *Type 1 error = FALSE POSITIVE/FALSE 1 (Sending a good/ham email to spam/1)
       *Type 2 error = FALSE NEGATIVE/FALSE 0 (Thinking a spam email was good/ham/0)
       *Again, Type 1 error is worse than Type 2 error because, in example of spam, sending a good email to spam is worse than not sending a true spam email to regular inbox...so the
        question is, what is the greatest level of probability that you are willing to tolerate (how many times are you willing to incorrectly send a good/ham/0/negative email to 
        spam/1 box in order to have a good spam classifier that sends most emails to correct place? This maximum probability is called the SIGNIFICANCE LEVEL (denoted by letter a).
       *Significance level is between 0 and 1 (example is if significance level is 0, it will always lean to H_0 (good email), and if the significance level is 1, it will always lean to
        H_1 (spam email)); WE WANT A SMALL SIGNIFICANCE LEVEL, BUT NEVER 0...0.05 IS TYPICAL (in example this means a good email is sent to spam 5% of the time)
       *Note: A thing to watch out for is IF YOU REDUCE A TYPE 1 ERROR TOO MUCH, YOU ARE INCREASING THE TYPE 2 ERROR, AND VICE VERSA
       *Notation for significance level is a = max P(Type 1 error), or max P(reject H_0|H_0)
  Right-Tailed, Left-Tailed, and Two-Tailed Tests
  **** *Example: Average heightss of 18 year olds in the US; we find the sample mean (68 inches), while historically it was 66 inches; null hypothesis is it's still 66 inches, but
        there are three different alternatives
       *Right-tailed test in above example is null of 66 and alternative of greater than 66 (alternative is to right of null)...example of errors that can come up is a Type 1 error
        (saying mean is greater than 66, even though it did not actually change) or a Type 2 error (not rejecting that mean is 66, even though it actually got bigger)
       *Left-tailed test in above example is null of 66 and alternative of less than 66 (alternative is to left of null)...example of errors that can come up is a Type 1 error
        (saying mean is less than 66, even though it did not actually change) or a Type 2 error (not rejecting that mean is 66, even though it actually got smaller)
       *Two-tailed test in above example is null of 66 and alternative of not equal to 66 (alternative is either side of null)...example of errors that can come up is a Type 1 error
        (saying mean is not equal to 66, even though it did not actually change) or a Type 2 error (not rejecting that mean is 66, even though it actually changed/got bigger or smaller)
       *Note: Data needs to be reliable (representative of population, randomized/randomly chosen, sample size large enough)
       *Note: Sample means are TEST STATISTICS, as is the S-SQUARE STATISTIC (later video)
  P-Value
  **** *If sample mean is too far away from null, reject H_0/null...but wat is "too far away"?
       *Example with height problem; if sd is 3 and sample size is 10, distribution is N(66, (3^2)/10). Question for too far away is "how likely was the sample if H_0/null is true?"...
        and if the answer is very unlikely, reject H_0/null. Remember alternative is greater than 66, and Type 1 error probability needs to be less than significance level of 0.05 
        (error is the false positive of saying it's the bigger height we looked to find, when in fact it didn't actually change)
       *Type 1 error probability = probability mean is greater than sample mean 68 given that the population mean is 66...ths was .03, less than .05...because that's true, reject H_0
        with a 5% significance level
       *P-value is the probability that, assuming the H_0/null is true, that the test statistic takes on a value as extreme or more extreme than the value observed (moves in the direction
        of H_1)
       *Decision rule depends on significance value you choose...if p-value is less than significance value, reject H_0 and accept H_1 as true
       *Note from Quiz, #1:  A smaller p-value indicates stronger evidence against the null hypothesis. When the p-value is small, it suggests that the observed data are unlikely to have 
        occurred if the null hypothesis were true, leading to rejection of the null hypothesis.
       *Note from Quiz, #2: The p-value represents the probability of observing the data or more extreme results under the assumption that the null hypothesis is true. It helps assess 
        the strength of evidence against the null hypothesis based on the observed data.
       *Note from Quiz, #3:You reject the null hypothesis if the p-value is less than alpha (significance level). This decision rule is commonly used in hypothesis testing to determine 
        whether the observed data provides enough evidence to reject the null hypothesis.
       *Z-statistic = sample mean minus pop. mean/(sd/sqrt of samples)
  Critical Values
  **** *What is the least extreme value you can get and still reject H_0/null? This is a sample that has a p-value of exactly significance level value...this is called a CRITICAL VALUE.
       *Uusually denoted as k_alpha (critical value) to denote dependency on alpha (significance value)...if sample value, greater than critical value, reject H_0/null
       *Reject if score greater/less/unequal to critical in right/left/two tailed test
  Power of a Test
  **** *Type 2 errors can be for any value above population values
       *Type 2 error = beta
       *Power of the Test = function that tells you for each possible value of the population (alternative hypothesis) the probability of rejecting H_0...
        notation is P(Reject H_0|value=H_1)...complementary to P(Do not reject H_0|value=H_1)
  Interpreting Results
  **** *Steps in Hypothesis Testing = 
       *1) State your hypotheses (find baseline null (66 in height example), alternative hypothesis to prove (above 66 in height example)); 
       *2) Design your Test (decide test statistic to work with (x bar), defining the significance level (0.05 for example...max probability of making a Type 1 error, and should be small)
       *3) Compute observed statistic based on sample (sample mean/x-bar of 68 for example)
       *4) Reach a conclusion (if the p-value is smaller than the significance level, reject H_0/null; because sample mean is far enough away from perceived null hypothesis/H_0 mean based
           on a significance level, so, considering this is a randomized, representative, and lare enough sample, it can be believable that there is a difference from historical belief?)
       *Note: Ask why reject null based on p-value/significance level?
       *Type 1 error = rejecting the null hypothesis when it is true (reject H_0 when H_0 is true; false positive; choose H_1/positive when H_0/negative)
       *Type 2 error = do not reject the null hypothesis when it is false (do not reject H_0 when H_0 is false; false negative; choose H_0/negative when H_1/positive)
       *Note: Positive is when you can reject the null/prove a changed H_1 state; negative is when you cannot reject null/stay at unchanged H_0 state.
       *Significance level is max probability of a Type 1 error
       *For a fixed sample size, Type 1 and Type 2 errors are entangled
       *For P-values, if P(reject H_0|H_0) < significance level, reject H_0 and accept H_1...the p value represents the probability of seeing the observed data by chance (a small p value
        means chances of observing sample test statistics are small based on prior beliefs (which would mean we should reject prior beliefs)
       *For test conclusions, if you reject the null hypothesis, you accept the alternative hypothesis as true...but if you cannot reject the null hypothesis, you instead conclude with 
        there was not enough evidence to show that the alternative hypothesis presented was True
  T-Distribution
  **** *Example of sampling the height of 10 18 year olds
       *Heights can be modeled as a Gaussian distribution with parameters for pop. mean and pop. sd...the sample will follow a Gaussian distribution of the same mean but a smaller
        standard deviation (with now 10 samples), which ends up being sd/sqrt of 10. 
       *But if you don't know the value of sd? If mean and sd are known, we know that sample mean minus pop. mean, divided by sample sd divided by sqrt 10 would follow a standard
        normal distribution of N(0, 1^2)...this is sample standardization, and "sample mean minus pop. mean, divided by sample sd divided by sqrt 10" is called the Z-STATISTIC; but
        if sample sd is unknown, then this formula isn't useful...so what you do is replace sample sd with "estimate sd" (this is basically sd, except sum of squared differences is 
        divided by n-1 instead of n, and then sqrt'ed, which gives us the T-STATISTIC).
       *This T-distribution is not exactly a normal Gaussian...it is bell-shaped but has heavier tails, which account for variability.
       *Parameter of T-distribution is DEGREES OF FREEDOM, which controls heaviness of tails; the larger the sample size, the smaller the tails and the closer the t-distribution is to
        a Gaussian...specifically when it hits 30 samples (why that is a number thrown out a lot). Degrees of freedom is sample saize minus 1...it's independent of population mean and 
        variance.
       *T-stat is used when a population has a Gaussian distribution, but you don't know the variance.
  T-tests
  **** *Example of sampling the height of 10 18 year olds and mean of 68, and 3 sets of hypotheses (see right-tailed, left-tailed, two-tailed above)
       *If null height is 66, sd is 3 and sample size is 10, then if null/H_0 is true, the sample mean distributes normally with mean of 66 and sd of 3 over sqrt of 10...but the 
        difference is we don't know sigma, which could affect the distribution...so now complete the 3 tests using the T-STATISTIC instead.
       *In example, observed t-statistic is 1.77...now what is the probability that the t statistic is greater than 1.77 given the null/H_0 is true...the shaded area to right is 0.052,
        and since this is greater than 0.05, do not reject the null/H_0; with z-statistic we got that you should reject...the difference is the uncertainty of not knowing the sd; etc
