WEEK 3: Sampling and Point Estimation

* LESSON 1 - Population and Sample
  Population and Sample
  **** *Samples often have to be used because we can't get a full consensus population survey (billions of people can't be surveyed)...sample comes from a population
       *Ask a subset and estimate
       *There is random sampling, which is the goto
       *Dependent random sampling isn't good...must start over if necessary
       *Samples need to be independently distributed; whatever rules used to pick one has to be used to pick another
       *Every dataset we use is a sample
  Sample Mean
  **** *Good to do random sampling for mean, because it avoids bias and allows each instance an equal opportunity to be chosen
       *Bigger the sample size, the better
  Sample Proportion
  **** *Population proportion = number of items with a given characteristic divided by the number of items in a population, denoted by P
       *Sample proportion = number of items in a subset of a population with a given characteristic divided by the number of items in that subset, denoted by P-hat
  Sample Variance
  **** *All that is discussed is notation...big N for population calculation, little N for sample population...variance is still average squared deviation, or value of variable
        instance minus mean of variable squared, divided by number of population (or sample)
       *Bias is over or underestimation of variance
       *Exaample = Game where you could pick 1, 2, 3 out of a hat randomly...population mean is 2, population variance is 2/3...but you pick 1, 2, 3 out of a hat, put it back, and pick
        again, you only get a variance of 1/3 (which is wrong), which is why below is real
       *Dividing by n-1 in variance fixes bias when all you have is a sample. As n gets big, this doesn't matter...if it does, sample is too small. Without n-1, still have a pretty
        good estimation of variance, but will have a bias...in full populations, no n-1 is needed.
  Law of Large Numbers
  **** *More samples we get, the closer we get to population mean. If n is the number of samples, and x_i is the ith sample from the population, each x_i are independent and identically
        distributed...as x goes to infinity, expected value of x approaches the population mean
  Central Limit Theorem - Discrete Random Variable
  **** *You can take any distribution and take samples from it and plot the averages of it, and you'll get a population distribution
       *Example is coinflip - random variable X is number of heads when coin is flipped n times, and if n = 1, X = 1 for heads and X = 0 for tails. Probaility is 0.5 for X = 1 and
        0.5 for X = 2...as we increase the number of coins we flip, it actually starts to naturally look bell-shaped as a distribution related to expected value
       *Counting the number of heads in n coinflips is the same as adding n Bernoulli variables, where each variable takes value 1 when you see heads and 0 when tails
       *Mean is n*p, variance is n*p*(1-p)...in coinflip for heads, mean is 1*0.5 for heads (0.5), and variance is 1*0.5*0.5 (second 0.5 is probability of tails, or not heads) is 0.25
       *Mean and variance go up uniformly with added flips
  Central Limit Theorem - Continuous Random Variable
  **** *Example of the call; random variable X is the wait time for a call to be answered, with X = U(0,15), meaning it follows a uniform distribution, meaning every interval of the same
        length ahs the same chance of occurring, leading to a constant probability density function
       *In example, looiking at 1 call, versus average of 2 calls, vs average of 3 calls, random variable Y_n is the average for all the waits on the n calls. It also becomes bell shaped 
        over time and has less dispersion
       *Saince all variables are I.I.D, all outcomes have identical probability, and mean is sum of X_i divided by number of X_i, or simply mean of x. Variance is still normal variance,
        but divided again by n (WHY?).
       *As n grows, so does variance of average.
       *If more skewed initially, need more samples to see the normality of more samples take place.
       *I.I.D dictates that sample means are the same as population means, and sample variance is same as population variance, but divided by number of variables being averaged. Mean stays
        same but variance gets smaller; this makes sense because the more samples we take, the closer we'll be to the population mean, meaning less spread and less variance
       *Formal definition ofCentral Limit Theorem = as X goes to infinity, the standardized average will follow a standard normal distribution. Typicallyaround n of 30 or higher.

* LESSON 2 - Point Estimation
  Point Estimation
  **** *Definitions already heard
  Maximum Likelihood Estimation - Motivation
  **** *Essentially, if an outcome occurs, what is the likeliest cause? (spilled popcorn; caused by people watching a movie, playing board gmes, or taking a nap?)
       *Notation = P(outcome|cause)
       *The math for multiple runs (like a series of coin flips) is probability of cause multiplied by not cause (example: if heads is rolled 8 of 10 times, if a coin has a probability of 
        rolling heads 0.7, the probability is 0.7^8 * (1-0.7)^2. This actually ends up a higher value than if it were a non-bias coin of 0.5 probability heads in the 8 of 10 scenario, so
        it becomes a better estimate)
  MLE - Bernoulli Example
  **** *Same coin example as above
       *In example of finding probability to maximize p^8*(1-p)^2, finding the natural logarithm of the function simplifies and finds the maximum likelihood...this becomes
        8 log(p) + 2 log(1-p). This is the LOG-LIKELIHOOD. Notation would be L(p; 8H). The derivative of  8 log(p) + 2 log(1-p) is (8/p) + ((2/(1-p))*(-1))...equal this to 0 and the
        optimal value for p-hat is 8/10.
       *General equation is, for n coins and k heads as example, L(p; x) = P_p(X=x) = log(P^x_i * (1-p)^(n-x_i))
  MLE - Gaussian Example
  **** *Example = Say have have points -1 and 1 on a graph; given it was sampled from an unknown normal distribution, which distribution is more likely to have generated the sample...
        a distribution with a mean of 10 and a standard deviation of 1, or a mean of 2 and a standard deviation of 1? The obvious answer is 2, but even graphically, heights on curves
        correspond with likelihood...the curve with mean 2 corresponds with higher likelihood at the points -1 and 1 than on the curve with mean 10, because it is closer to the mean.
       *Multiply likelihoods of each observation on the different curves...the highest product is the curve most likely to have generated the observations
       *NOTE: The best distribution is the distribution where the mean of the distribution is the same as the mean of the sample...same with variance.
  MLE for Gaussian Population
  **** *Equation for a normal gaussian distribution is f_x(x) = (1/(sqrt of 2*pi)(sd))*e^(-1/2*((x-mean)^2/(sd)^2)).
       *1) Likelihood is a function of mean and sd, so notation is L(mean, sd; x) = (1/(2*pi)^(n/2)*(sd^n))*e^(-1/2*((x - mean)^2/(sd)^2)) with n for n
           number of samples.
       *2) To maximize/optimize likelihood, take derivative of above function and equate it to zero. This is easiest done with the LOGARITHM FUnction, because this logarithm
           function is always increasing, and logarithm has the property of turning a product into a sum. The notation for LOG-LIKELIHOOD is l(mean,sd) = log(L(mean, sd; x)), which
           gets you (-n/2) log (2 pi) - n log (sd) - (1/2)((sum of n (x-mean)^2)/sd^2).
       *3) With that, find the partial derivatives for mean and sd. For mean, the first two terms of l(mean,sd) don't include mean, so focusing on the 3rd term, the partial is
           (-1/2)((2*(x-mean))/sd^2)*(-1), or (x - mean)/sd^2 over n samples. For sd, it's (-n/sd) + ((x-mean)^2)/(sd^3).
       *4) Equate the partial derivatives to 0. This is sqrt of sum of ((x-mean)^2)/n for sd, and sum of x's over n for mean.
  Interactive Tool
  **** *Showed how log-likelihood is approached
  MLE: Linear Regression
  **** *Models with highest probability have maximum likelihood
       *Points that are regressable are on a graph...the line of best fit model would be the winning distribution; it ends up being the same as finding least squares error
        through linear regression
  Regularization
  **** *Example is a graph with 3 models being the possible creators of it, one badly underfit, one well fit, and one very overfit. using loss (squared error), the underfit one is no
        competitor, and the overfit one slightly edges the well fit one, although we intuitively know the well fit one is the better generalized model. This is where REGULARIZATION
        comes in.
       *REGULARIZATION IS A PENALTY; THE MORE COMPLICATED (AND, TYPICALLY AS A RESULT, OVERFITTING) THE MODEL, THE HIGHER THE PENALTY. IT'S CALLED THE L2-PENALTY; YOU GET IT BY
        SQUARING EVERY COEFFICIENT OF THE MODEL EQUATION AND ADDING THAT TO THE PREVIOUS LOSS, ENSURING THAT PENALTY FOR COMPLEXITY IS APPROPRIATELY BALANCED WITH ORIGINAL LOSS. THIS
        IS USED WHEN TRYING TO FIND THE SIMPLEST MODEL THAT FITS THE DATA WELL.
       *Mathematically, the model is y = a_nx^n + a_(n-1)x^(n-1)+...+a_1x + a_0; log-loss (original loss) is LL (notation); L2 regularization error is a_n^2 + a_(n-1)^2 + ... + a_1^2; 
        regularization parameter is rp; and the final regularization error is LL + rp(a_n^2 + a_(n-1)^2 + ... + a_1^2).
       *Note: What is regularization parameter?
  Back to Bayesics
  **** *Back to the popcorn example...we chose movies as the highest probabilitiy, but that's not the whole story; we compared movies to board games and sleeping before, but imagine now
        comparing movies to a popcorn throwing contest?...popcorn throwing would have a high probability of leaving popcorn on the ground, but movies should still be the winner of the 
        highest probaility, because popcorn throwing contest is itself an unlikely event.
       *The math is multiplying the likelihoods by the individual probability of each possible cause of the outcome; notation would be P(popcorn|movies) * P(movies). This looks like
        P(A|B) * P(B), which equals P(A intersection B), or the probability of A and B happening at the same time. That's what er care about...not cocnditional probability, but probability
        that both things happen at the same time.
