WEEK 1: Introduction to Probability and Probability Distributions

* LESSON 1 - INTRODUCTION TO PROBABILITY
  Course Introduction
  **** *Probability is important because a lot of algorithms in machine learning have results that can be phrased as a probability
       *Ex: spam detection outputs probability that an email is spam
       *Bayes Theorem = Calculate probability given certain events
       *Maximum Likelihood Estimation = Probability that modelgives you your data (squared error is used because you are using a Gaussian
        distribution which has a square...when you take the logrithm it has a square)
       *Regularization is important because when you start thinking about the probability of a model, there's another Gaussian there for the
        coefficients, which the squares of the regularization terms of the terms come out too (L2 regularization comes from a Gaussian
        distribution assumption)
       *Hypothesis testing is covered as well...also, confidence, confidence interval, p-value
  Check Your Knowledge
  **** *Understood the basic probability point...not sure about complement of probability...not sure about sum of probabilities...not sure
        about sum of proabilities, joint events...not sure about Bayes' Theorem...not sure about random variables...not sure about
        probability distributions...not sure about probability density function
  What is Probability?
  **** *Rules of probabiility and Bayes Theorem in Lesson 1; in Lesson 2 we'll learn about probability distributions
       *Probability is the measure of how likely an event is to occur
       *Ex: In a school with ten kids, 3 play soccer and 7 don't. Probability that the kid we pick at random plays soccer is 3/10, or 30%.
        Would be denoted as P(soccer) = soccer/total...the number of kids who play soccer is the EVENT, and the denominator total is the
        SAMPLE SPACE
       *Ex: On coin flips, P(heads) = heads/heads or tails; P(HH) = HH/HH*HT*TH*TT (flipping heads twice)
  What is Probability? - Dice Example
  **** *P(6) = 6/(1,2,3,4,5,6) = 1/6
       *P(66) = 6,6/((1,2,3,4,5,6)(1,2,3,4,5,6)) = 1/36
  Complement of Probability
  **** *Complement of probability is the probaility that an event does not occur; if an event has a 75% probzbility of occurring, then
        the complement of that event has a 25% probability of occurring
       *Ex: In a school with ten kids, 3 play soccer and 7 don't. Probability that the kid we pick at random doesn't play soccer is 7/10, 
        or 70%. Would be denoted as P(not soccer) = not soccer/total...the number of kids who play soccer is the EVENT, and the 
        denominator total is the SAMPLE SPACE...P(not soccer) = 1 - P(soccer).
       *Ex: Probability of not landing on heads three times is P(not HHH) = 1 - P(HHH) = 1 - 1/8 = 7/8.
       *Ex: Rolling a die, what is the probability of rolling anything other than 6? P(not 6) = 1 - P(6) = 1 - 1/6 = 5/6.
  Sum of Probabilities (Disjoint Events)
  **** *Ex: Rolling a die, the probability of getting a 2 or a 3 is P(2 or 3) = 2,3/(1,2,3,4,5,6), or 2/6 (1/3)...the probability of
        rolling an even number or 5 is P(even number U 5) = P(even number) + P(5) = ((2,4,6)+(5))/(1,2,3,4,5,6) = 4/6 = 2/3
       *Ex: Kids can only play one sport....if P(soccer) is 0.3 and P(basketball) is 0.4, what is the probability that a kid plays
        soccer or basketball? It's 0.7 (P(soccer or basketball) = (0.3 + 0.4)/1.0); they can't be intertwined because you are only
        allowed to do one sport. (Notation = P(soccer U basketball) = P(soccer + P(basketball), where U stands for union/or).
       *Given the example of rolling a die twice and getting 7 or 10 (18/36...all different combinations)
  Sum of Probabilities (Joint Events)
  **** *Ex: Probbility of rain is 80% and probability of windiness is 70%...what's the probability of rain or wind? Adding them gets
        you 150%, and shows the fact they can happen at the same time (you can have rain and wind at the same time)...we're using
        SUM rule
       *Sum rule is used to calcuate the possibility of combined events
       *Ex: Kids can play as many sports as they want.....if P(soccer) is 0.6 and P(basketball) is 0.5, what is the probability that a 
        kid plays soccer or basketball? If P(soccer) is 0.6 and P(basketball) is 0.5 and P(soccer U basketball) is 0.3, how many kids play
        soccer or basketball (5 + 6 - 3) - 8...the kids who play soccer and basketball are accounted for in the 5 + 6 already, so we don't
        want to double count.
       *Disjoint events don't overlap, and are mutually exclusive...joint events do, and are non-mutually exclusive.
       *Ex: Probability of rolling two dice that sum to 7 or have a difference of 1...14/36 (4,3 and 3,4 overlap)
  Independence
  **** *P(A and B) = P(A) * P(B)
  Birthday Problem
  **** *In a party of 30, which is more likely, that no two ppl have the same birthday, or that at least two ppl have the same birthday?
       *Probability that they all have different birthdays is 365/365 * 364/365 * 363/365...until 336/335 (which gives us around 30%); the
        complement of this is the probability that at least two ppl share the same birhtday (1 - 30% = 70%)
  Conditional Probability - Part 1
  **** *Conditional probability = probability that an event happened given that another event has happened
       *Ex: Landing on heads twice in coin flip; P(HH) = P(H and H) = P(H) * P(H) = 1/2 * 1/2 = 1/4 (25%)
       *Ex: Landing on heads twice GIVEN the FIRST FLIP IS HEADS; P(H) = 1/2 (50%)...notation is P(HH | first is HH) = HH/((HH)(HT))
       *Conditional probability changes sample space
       *Ex: Proability that a first roll of a die is 6 and the next gives you sum of 10 = P(1st is 6 and sum of 2 is 10) = P(6)*P(4)
        = 1/6 * 1/6...P(sum is 10, given 1st roll is six)
  Conditional Probability - Part 2
  **** *Ex: School of 100 kids (50 soccer likers, 50 non-soccer likers) are told they have two options: go in a room with the World Cup
        on TV, or go in a room not watching it...we would expect all 50 soccer likers to go in the room with the World Cup.
       *In previous examples, the rooms and the liking of soccer or not were independent; when you add the variable of the World Cup
        being played in one of the rooms, it becomes two dependent variables (liking soccer and students' chosen room).
       *Ex: School with 100 kids (50 soccer likers, 50 non-soccer likers)...on any given day, 80% of the soccer likers wear running
        shoes...how many soccer students wear running shoes? P(S and RS) = P(S) * P(RS | S) = (50) * (.8) = 40. Also, 50% who don't like soccer
        wear running shoes; how many nonsoccer students wear running shoes? P(NS and RS) = P(NS) * P(RS | NS) = (50) * (.5) = 25.
        In the scenario, there's 4 scenarios...soccer students who wear running shoes (40), soccer students who don't wear running shoes
        (10), nonsoccer students who wear running shoes (25), nonsoccer students who don't wear running shoes (25).
  Bayes Theorem - Intuition
  **** *Used for spam recognition, speech detection, etc
       *Ex: 1,000,000 million people in a population...1/10,000 are affected by an illness. Test that is given is 99% effective (out of 
        100 sick people, 99 are correcctly diagnosed as sick...out of 100 healthy people, 99 are correctly diagnosed as healthy). If
        someone went to the doctor and tested sick, are they really sick?
       *Answer to above: in the 1,000,000 person population, only 100 are sick (1/10,000), and 99 are actually sick and diagnosed as such. The 
        probability that someone in the population that tested sick is sick is P(sick | diagnosed as sick)...99 are sick, and this is out
        of the pool of 99 people that are correctly diagnosed as sick, and the 9,999 that are healthy and incorrectly diagnosed as sick.
       *The math is best depicted in the arrow map infographic...in the 1,000,000 population, 999,000 are healthy and 100 are sick (the
        1/10,000 ratio); in the 999,900 that are healthy, because of the 99% effectiveness, 99% of this pool (989,901) will be correctly
        diagnosed as healthy and 1% of the pool (9,999) will be incorrectly diagnosed as sick...in the 100 that are sick, 99% (99) will be
        correctly diagnosed as sick, and 1% (1) will be incorrectly diagnosed as healthy; the probability that someone is sick GIVEN they
        were diagnosed as sick is those correctly diagnosed as sick (99) divided by the correctly diagnosed as sick (99) plus those
        incorrectly diagnosed as sick (healthy incorrectly diagnosed...9,999).
  Bayes Theorem - Mathematical Formula
  **** *Conditional probability = P(A | B) = P (A and B) / P(B)
  Bayes Theorem - Spam example
  **** *Finding if emails are spam
       *Example of 100 emails with 20 of them being spam...building a classifier
       *P(spam | lottery) = spam emails with 'lottery' / all emails with 'lottery'
       *Bayes Theorem removes emails that don't matter (in the case of above, non-spam emails with 'lottery')
       *We are shown that P(spam) = 20% and P(not spam) = 80%, P(lottery | spam) = lottery given spam = 14/20 = 0.7, 
        P(lottery | not spam) = lottery given not spam = 10/80 = 0.125
       *It's how many of something given a certain defined sample space
  Bayes Theorem - Prior and Posterior
  **** *Prior = P(A); event = E; posterior = P(A | E)
       *In spam example, prior is probability of email beng spam; event is email containing 'lottery'; posterior is probability of spam
        given the email contains 'lottery'...the way to think about the EVENT is what comes after the 'given'
  Bayes Theorem - Naive Bayes Model
  **** *Example of two events in email classification (email contains 'lottery' and 'winning')
       *Naive assumption is assuming that the appearances of the words/features in a document are independent, because the more features
        there are, the more likely all of them together are less likely to be found (have 0 instances of what you're looking for)...it
        ends up being, in the example of lottery and winning, instead of lottery and winning being the test, it's the product of 
        lottery in the spam and winning in the spam (separate)
  Probability in Machine Learning
  **** *Machine learning is all about probabilities (for spam classifiers, the probability an email is a spam based on message or 
        recipient, or other features; in image recognition, does an image have a certain thing in it or not)
       *In image recognition, example is probability a cat is in an image is P(cat | image) = P(cat | pixel_1, ..., pixel_n)
       *Other options is health of a patient, the tone of a sentence, etc
       *It's all about conditional probabilities

* LESSON 2 - PROBABILITY DISTRIBUTIONS
  Random Variables
  **** *Random variables can take many values
       *Example: Random variable X equalling the result of a coin flip (X = 1 if heads, X = 0 if tails) and both P(X = 1) and P(X = 0)
        have a 50% probability
       *Random variables allow you to model the whole experiment at once
       *Other examples of random variables = X equalling the number of times we get 1 when we roll a die, or the number of sick patients
       *Two types of random variables are discrete (e.g., number of times flipping heads or rolling 1 on die) and continuous (e.g.,
        amount of time until a bus arrives, height of a jump, amount of rain during a month)..discrete have countable number of values,
        continuous takes values on an interval
       *Deterministic takes a value and it's always that value forever...random variables can take many values, and are associated with
        an uncertain outcome
  Probability Distributions (Discrete)
  **** *Putting all of the possible values of an experiment on a horizontal axis and showing the probability that each happen is a 
        discrete probability distribution
       *Shows three coin flips as a probability distribution of flipping heads in 3 coin tosses, then 4 coin tosses, then 5 coin tosses
       *Probability mass function is the name of the probability distribution
  Binomial Distribution
  **** *Ex: Flip a coin ten times and consider how many heads can be obtained...it can be 0, 1, 2, 3,..., 10...each one has a 
        corresponding probability; the histogram of probabilities is the BINOMIAL DISTRIBUTION
       *Binomial distributions are examples of discrete distributions
       *Ex: Probability that if a coin is flipped five times, two of them will land on heads? (.5 ^ 5 = 1/32; but also a factorial
        comes into play (there are actually ten combinations of ways this can happen)...in the mentioned case the equation would be
        5! / 2!(5-2)!, where 5! is the number of ways 5 coins can be ordered, 2! is the number of heads, and (5-2)! is the number of tails
       *The above can also be written as (5 2)^T (the 5 and 2 are actualy in a column) and this is called the BINOMIAL COEFFICIENT.
        In general, the notation is (n k)^T, where it counts all of the combinations for landing k heads in n coin tosses...also can
        have the notation of (n (n-k))^T, where k heads is the same as obtaining n-k tails
       *Ex: Probability of number of heads in 5 coin tosses? (5 x)^T*(p^x(1-p)^(5-x))...p^x is the probablity of choosing x amount of heads,
        (1-p)^(5-x) is the probability of choosing 5-x tails, and (5 x)^T represents all of the possible orders; this is the PROBABILITY
        MASS FUNCTION for x (number of heads in n coin tosses)...p_x(x) = (n x)^T*(p^x(1-p)^(n-x)), and n and p are the PARAMETERS of
        the equation (X ~ Binomial(n,p))...n is the amount of iterations of the test, and p is the natural probability.
  (Optional) Binomial Coefficient
  **** *k! factorial is important in understanding the amount of soultions and combinations of a problem
       *(n k)^T*(p^x(1-p)^(n-x))
  Bernoulli Distribution
  **** *Bernoulli distribution is essentially a binomial distribution, but always with a theoretically correct probability (ex: p in 
        (n x)^T*(p^x(1-p)^(n-x)) for rolling a number on a die would be 1/6).
  Probability Distributions (Continuous)
  **** *Continuous is based off of intervals that cannot be listed
       *Interesting: with continuous distributions, the probability that we will wait any specific time for a call is 0...so the question
        has to be restructured in terms of windows (probability that we will wait between 1 and 2 minutes, etc)...the intervals can be
        split...the sum of heights of this distribution will equal 1
  Probability Density Function
  **** *The binomial distribution, but for intervals (with discrete, you cannot have significant probabilities for pinpoint values)
       *It's a shaded area probability...but cannot just look at height, because as you create more and more intervals, shaded areas
        squeeze and squeeze until close to zero
       *Notation = f_x(x)
       *It tells you the rate at which you accumulate probability for each point
       *Probability notation is P(a < x < b) being the area under f_x(x), with a and b being the interval (ex: between 1 and 2 minutes
        call is a=1 and b=2)
       *Assumptions = 1) f_x(x) is defined for all numbers (0 before and after the curve, for example), and 2) it needs to be greater than
        or equal to 0 at all values (there is no negative chance/probability that something has to occur), and 3) the area under the curve
        needs to be 1 (all probabilities defined under the curve)
       *Reiterate: discrete random variables take on a finite or countable number of variables (ex: trial of 10 coin flips, with
        experiment result being probability of certain amount of times landed on heads...this is probability mass function 
        p_x(x) = P(X = x)); continuous random variables take on values at intervals (ex: phone call length, with experiment result being
        probability it lasts within certain intervals of time...this is probability density function f_x(x) = P(X = x) = 0, because at any
        singular point, the probability will be 0)
  Cumulative Distribution Function
  **** *Shows actual probability that a call is between 0 and a certain number of minutes for example (for continuous); also a cumulative
        distribution function for discrete distributions
       *For continuous, it ends up being the sum of all the preceding intervals and the current interval; for discrete, it's similar (the
        amount under the curve up until a point)
  Interactive Tool
  **** *Interesting; of note was the continuous - uniform function (from -2 to 3 was constant 0.2 on y value); slants up to 1 during that
        interval
  Uniform Distribution
  **** *It is essentially a density function, but with a constant (for a phone call experiment with 1 minute intervals over 15 minutes,
        the constant is 0.06 on y value over that period)
       *Continuous random variable can be modeled with it if all possible values lie within an interval andthere is the same theoretical
        frequency of occurrence
       *Straight line, followed by diagonal, followed by straight line
       *f_x(x) = {(0 for x < a), ((x-a)/(b-a) for a less than or greater than x nd x less than b), (1 for b > x)}
  Normal Distribution
  **** *If data looks like a bell curve (with e^(-1/2(x^2)) as the function), to properly adjust it if the mean is two would mean to 
        adjust the equation to e^(-1/2((x-2)^2); to properly adjust the curve if it's too skinny and needs to be widened by a multiple
        of 3 is e^(-1/2(((x-2)/3)^2); to properly adjust the height of a curve if it is too tall is divide by the area of the too tall 
        curve (1/3 * e^(-1/2(((x-2)/3)^2), for example (if area of wrong curve is 3 and area of correct curve is 1)
       *Parameters of a normal distribution are mean and standard deviation (sd)...the above dictates the equation of the Gaussian, which
        is 1/(sd*sq_root_of_2pi) * e^(-1/2(((x-mean)/sd)^2)...1/(sd*sq_root_of_2pi) is the SCALING CONSTANT...notation is
        X ~ N(mean, sd^2)...sd^2 is the VARIANCE (sd and sd^2 convey the same info)
       *Mean is the center of the bell and standard deviation is the spread of the bell
       *Range is all real numbers; symmetrical distribution
       *Standardization = subtracting by mean (to get it to 0), and dividing that result by the standard deviation (makes the standard
        deviation the right width of 1)...helps to compare variables of different magnitudes
       *CDF of a normal distribution looks like a sigmoid function (kind of s-like)
       *Models a lot of things in nature
  (Optional) Chi-Squared Distribution
  **** *Ex: communication between power lines can be obstucted by things in between them, noise, etc (a message sent of 10010 can be a
        message received of 10010 + Z (noise that affected the message, and has a random nature)
       *Question of above example is what is the power of the noise of the W channel between the power lines (W = Z^2); when we assume
        Z follows a standard normal distribution, F_w(W) = P(W LTOET w) = P(Z^2 LTOET w) = P(|Z| LTOET sqrt of w) =
        P(neg sqrt of w LTOET Z LTOET pos sqrt of w)...LTOET is less than or equal to.
       *CDF is integral of PDF...the probability that W LTOET value of w on the CDF is equal to finding the area under the curve of a
        normal distribution when W is its negative and positive value (w of probability of 2 on CDF is area between -2 and 2 on normal
        distribution)...this is the chi squared distribution with 1 degree of freedom
       *W = Z1^2 + Z2^2 is a chi squared distribution with 2 degrees of freedom...as number of degrees of freedom increases, it becomes a
        more symmetrical distribution
  Sampling from a Distribution
  **** *Create sythetic data from distribution (when you cannot get more data)
       *Step 1: Generate a random number between 0 and 1, step 2: find out which interval the number belongs to, step 3: assign outcome
        based on the interval
       *When you choose uniformly from the CDF, you get the variety of numbers that make up the normal/discrete distribution 
