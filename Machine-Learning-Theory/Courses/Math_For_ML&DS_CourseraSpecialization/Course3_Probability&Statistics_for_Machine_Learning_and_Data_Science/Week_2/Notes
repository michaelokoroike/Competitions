WEEK 2: Describing Probability Distributions and Probability Distributions with Multiple Variables

* LESSON 1 - Describing Distributions
  Expected Value
  **** *We are shown weighted average (expected value)
       *Notation = E[X]
       *What is expected in the long-term?
       *Notation for expected value for discrete variables = sum of probability x for all p_x's; notation for expected value for
        continuous variables = sum of probability x for all f_x's of an integral (the un-derivative of a function over an interval...
        for example x^2 in an integral means the function to test is 1/3*x^3)
       *Difference between mean and median is discussed later; we learn about torque
  Other Measures of Central Tendency: Median and Mode
  **** *Median = Important because of outliers (Michael Jordan was an outlier for geography majors, made the average starting salary
        appear higher than in actuality); middle point in even distribution, or average of two middle ones
       *Mode = Outcome with the highest probability; in continuous distribution, outcome with the highest height
       *Mean = Average (wieghted usually)
  Expected Value of a Function
  **** *Weighted average is shown in the E[x^2] example, etc
       *In general, E[aX + b] = aE[X] + b...E[aX] = aE{X], and E[b] = b
  Sum of Expectations
  **** *Ex: If you flip a coin (heads is $1, tails is nothing), then roll a die and win the amount you roll (ex: roll 1 and get $1, etc)
        what are the expected winnings?...it equals ($1+$0)/2 + ($1+$2+$3+$4+$5+$6)/6, or $4.
       *Ex: If there is a game where 3 people with three different names has their names in a bag, there are 6 different ways those three
        names can be distributed (one way with all three correct, and three other ways with 1 correct); as it would be a 6-run test with
        6 correct calculations, the expected number of correct assignments is 6/6 (which equals 1)
       *Easiest way to do above is E[Matches], and equal it to the sum of expectations for each person; because each person has 1/3 chance
        to pick their name in the 3 erson example above, the sum of expectations would be 1/3 + 1/3 + 1/3, which equals 1. It ends up
        being 1/n, for n amount of times.
  Variance
  **** *Distributions can have the same expectd value but have narrow vs wide spreads; for example, if there's heads you win 1dollar and
        tails you lose 1dollar your expected value is 0...in another example if there's heads you win 100dollars and tails you lose 
        100dollars which has an expected value of 0. The 100dollar wone has a larger spread...expected value is not a good measure here but
        variance is
       *Spread is how far away points are from expected outcome
       *One way for quantifying it is deviation (x - E[x]...actual outcome minus expected outcome); another is expected deviation
        (E[x - E[x]]), which appears when negative and positive deviations cancel each other out; and absolute deviation |E[x - E[x]]|;
        but the most common is SQUARED DEVIATION ((x - E[x])^2), and off that EXPECTED SQUARED DEVIATION (E[(x - E[x])^2]), the expected
        of which is the average of squared deviations and is VARIANCE
       *Steps to finding variance: 1) Finding x's mean, 2) find the deviation from the mean for every value of x, 3) square deviations,
        4) average those squared deviations
       *Heads win 2dollars vs tails lose 2dollars...vs heads win 3dollars vs tails lose 1dollar; they have different expected values (2 and
        2 is 0 EV; 3 and 1 is 1 EV), they have the same spread of outcomes (prob*(point-expected outcome)^2 for each point), which has
        the notation E[X^2]-E[X]^2, simplified from E[(x - E[x])^2] being a constant
       *Variance can be found by squaring the change in deviations when a function is transformed (ex: dice game where orginially it is 
        a 3.5 EV and a .5 deviation, but then it is transformed based on the game and it becomes 2 EV and a 1 deviation...in the transform,
        all the deviations have doubled so the variance of the transform is the square of that deviation change, when in comparison to
        the original)
       *in Var(ax + b), changes in b don't chage the pread, but changes in a do
