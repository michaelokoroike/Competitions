WEEK 2: Describing Probability Distributions and Probability Distributions with Multiple Variables

* LESSON 1 - Describing Distributions
  Expected Value
  **** *We are shown weighted average (expected value)
       *Notation = E[X]
       *What is expected in the long-term?
       *Notation for expected value for discrete variables = sum of probability x for all p_x's; notation for expected value for
        continuous variables = sum of probability x for all f_x's of an integral (the un-derivative of a function over an interval...
        for example x^2 in an integral means the function to test is 1/3*x^3)
       *Difference between mean and median is discussed later; we learn about torque
  Other Measures of Central Tendency: Median and Mode
  **** *Median = Important because of outliers (Michael Jordan was an outlier for geography majors, made the average starting salary
        appear higher than in actuality); middle point in even distribution, or average of two middle ones
       *Mode = Outcome with the highest probability; in continuous distribution, outcome with the highest height
       *Mean = Average (wieghted usually)
  Expected Value of a Function
  **** *Weighted average is shown in the E[x^2] example, etc
       *In general, E[aX + b] = aE[X] + b...E[aX] = aE{X], and E[b] = b
  Sum of Expectations
  **** *Ex: If you flip a coin (heads is $1, tails is nothing), then roll a die and win the amount you roll (ex: roll 1 and get $1, etc)
        what are the expected winnings?...it equals ($1+$0)/2 + ($1+$2+$3+$4+$5+$6)/6, or $4.
       *Ex: If there is a game where 3 people with three different names has their names in a bag, there are 6 different ways those three
        names can be distributed (one way with all three correct, and three other ways with 1 correct); as it would be a 6-run test with
        6 correct calculations, the expected number of correct assignments is 6/6 (which equals 1)
       *Easiest way to do above is E[Matches], and equal it to the sum of expectations for each person; because each person has 1/3 chance
        to pick their name in the 3 erson example above, the sum of expectations would be 1/3 + 1/3 + 1/3, which equals 1. It ends up
        being 1/n, for n amount of times.
  Variance
  **** *Distributions can have the same expectd value but have narrow vs wide spreads; for example, if there's heads you win 1dollar and
        tails you lose 1dollar your expected value is 0...in another example if there's heads you win 100dollars and tails you lose 
        100dollars which has an expected value of 0. The 100dollar wone has a larger spread...expected value is not a good measure here but
        variance is
       *Spread is how far away points are from expected outcome
       *One way for quantifying it is deviation (x - E[x]...actual outcome minus expected outcome); another is expected deviation
        (E[x - E[x]]), which appears when negative and positive deviations cancel each other out; and absolute deviation |E[x - E[x]]|;
        but the most common is SQUARED DEVIATION ((x - E[x])^2), and off that EXPECTED SQUARED DEVIATION (E[(x - E[x])^2]), the expected
        of which is the average of squared deviations and is VARIANCE
       *Steps to finding variance: 1) Finding x's mean, 2) find the deviation from the mean for every value of x, 3) square deviations,
        4) average those squared deviations
       *Heads win 2dollars vs tails lose 2dollars...vs heads win 3dollars vs tails lose 1dollar; they have different expected values (2 and
        2 is 0 EV; 3 and 1 is 1 EV), they have the same spread of outcomes (prob*(point-expected outcome)^2 for each point), which has
        the notation E[X^2]-E[X]^2, simplified from E[(x - E[x])^2] being a constant
       *Variance can be found by squaring the change in deviations when a function is transformed (ex: dice game where orginially it is 
        a 3.5 EV and a .5 deviation, but then it is transformed based on the game and it becomes 2 EV and a 1 deviation...in the transform,
        all the deviations have doubled so the variance of the transform is the square of that deviation change, when in comparison to
        the original)
       *in Var(ax + b), changes in b don't chage the pread, but changes in a do
  Standard Deviation
  **** *Variance squares things (leaves them in units like feet squared); standard deviation is the sqrt of variance (measures spread of
        a distribution using same units of the distribution)
       *Example is a distribution of meters...variance will provide a meters squared figure; just take the sqrt for standard deviation
       *68% of data is within 1 standard deviation of a maean of a normal distribution...95% is within 2 sd...99.7% is within 3 sd
  Sum of Gaussians
  **** *Ex: Total response time of a computer system (R) being sum of processing time (T) and network latency (L)...equation is
        R = T + L. If we could model T in milliseconds with a Gaussian distribution of T ~ N(10, 2^2) and L in milliseconds with
        L ~ N(5, 1^2) which means a mean of 5 and standard deviation of 1, and that T and L are independent of each other, the
        Gaussian of R would be N(10+5,4+1)
  Standardizing a Distribution
  **** *Desiring for a mean of 0 and a standard deviation of 1
       *With a distribution of X of mean 2 and sd 2, for example X - 2 centers the distribution, (X - 2) / 2 scales the distribution,
        and it is fully standardized
  Skewness and Kurtosis: Moments of a Distribution
  **** *E[x] is the first moment of a distribution, and E[x^2] is the second distribution (second moment is related to variance, but uncentered);
        third moment is E[x^3] and so on until the kth moment (probability at this point would be p1x1^k+...+pnxn^k)
  Skewness and Kurtosis - Skewness
  **** *Lottery vs Car Insurer...lottery = pay $1, can win $100 means that you can win $99 with 1% probability and lose $1 with 99% probability;
        car insurance = costs $1, crash repairs are covered to $100 means that you gain $1 (as insurer) with 99% probability and lose $99 with
        1% probability
       *In above example variance (spread of outcomes) looks the same (-1 to 99, 1 to -99); and expected values are the same (99*.01 - 1*.99 = 0;
        1*.99 - 99*.01 = 0). You could find actual variance by simply squaring the expected value and then multiplying it by its probability and adding
        the different components (ex: for lottery, (-1)^2*0.99 + (99)^2*0.01 = 99, for car insurance, (1)^2*0.99 + (-99)^2*0.01 = 99)
       *By having the same expected value and variance, they have the same first and second moment respectively...but these are different games so the
        way to tell them apart is to go on to the third moment, which is CUBING the expected value and then multiplying it by its probability (ex: 
        for lottery, (-1)^3*0.99 + (99)^3*0.01 = 9702, for car insurance, (1)^2*0.99 + (-99)^2*0.01 = -9702)...this tells us, in the example of 9702,
        there are some values really far out to the right (positive skewness), and for -9702, some values really far to the left (negative skewness)
       *Skewness is the standardized and cubed distribution; the third moment (notation is E[(X - mean/sd)^3]...note when this is 0, it's not a skewed
        distribution
  Skewness and Kurtosis - Kurtosis
  **** *Game 1 vs Game 2...game 1 is 1/2 chance you win a dollar, 1/2 chance you lose a dollar; game 2 is 100/202 chance you win 10 cents, 100/202 chance
        you lose 10 cents, 1/202 chance you win 10 dollars, 1/202 chance you lose 10 dollars
       *It appears both distributions have the same expected value (they do)...same variance (they do)...same skewness (they do...distributions symmetric
        around the midpoint always do)
       *The way to describe the difference is Game 2 has values that are really far away from the midpoint 0...this can be in the fourth moment or KURTOSIS
       *Notation of kurtosis = E[(X - mean/sd)^4]; in the case of distributions when tails are skinny (not many far out points), kurtosis is small...when
        the tails are thicker, kurtosis is large
  Quantiles and Box-Plots
  **** *Median is the point that splits data in half...the median percentile isthe 50% percentile or square percentile
       *Quantized essays provide solution, as the kth quantile is value that leaves k% of data to the lef, and (100-k)% to the right
       *25% = first quartile; 50% = second quartile; 75% = third quartile
  Visualizing Data: Box-Plots
  **** *Quartile in even list is the average of the two middle values
       *Interquartile range tells you where the percentage of your data lies (ex: difference between Q3 and Q1 tells how much of your data is in 50% of the data)
       *Shown a box plot without outliers, with outliers
  Visualizing Data: Kernel Density Estimation
  **** *Approximate the probability density function of a dataset with KERNEL DENSITY ESTIMATION
       *Kernel Density Estimation process = 1) draw the observations along the x axis, and see where there's a lot of density, then 2) draw a gaussian centered at each
        observation, and 3) multiplying everything by 1/n and sum the curves
  Visualizing Data: Violin Plots
  **** *Brings together the data of the kernel density estimation and the box plot
  Visualizing Data: QQ Plots
  **** *Some models assume normally distributed data (linear/logistic regression, Gaussian Naive Bayes, etc)
       *How do we get an exact answer of if a distribution is normal? Quantile Quantile plots, or QQ plots.
       *Get it by standardizing data (subtracting by mean and dividing by standard deviation), then compare quantiles of data, then
        compare to Gaussian quantiles
