WEEK 2: Describing Probability Distributions and Probability Distributions with Multiple Variables

* LESSON 1 - Describing Distributions
  Expected Value
  **** *We are shown weighted average (expected value)
       *Notation = E[X]
       *What is expected in the long-term?
       *Notation for expected value for discrete variables = sum of probability x for all p_x's; notation for expected value for
        continuous variables = sum of probability x for all f_x's of an integral (the un-derivative of a function over an interval...
        for example x^2 in an integral means the function to test is 1/3*x^3)
       *Difference between mean and median is discussed later; we learn about torque
  Other Measures of Central Tendency: Median and Mode
  **** *Median = Important because of outliers (Michael Jordan was an outlier for geography majors, made the average starting salary
        appear higher than in actuality); middle point in even distribution, or average of two middle ones
       *Mode = Outcome with the highest probability; in continuous distribution, outcome with the highest height
       *Mean = Average (wieghted usually)
  Expected Value of a Function
  **** *Weighted average is shown in the E[x^2] example, etc
       *In general, E[aX + b] = aE[X] + b...E[aX] = aE{X], and E[b] = b
  Sum of Expectations
  **** *Ex: If you flip a coin (heads is $1, tails is nothing), then roll a die and win the amount you roll (ex: roll 1 and get $1, etc)
        what are the expected winnings?...it equals ($1+$0)/2 + ($1+$2+$3+$4+$5+$6)/6, or $4.
       *Ex: If there is a game where 3 people with three different names has their names in a bag, there are 6 different ways those three
        names can be distributed (one way with all three correct, and three other ways with 1 correct); as it would be a 6-run test with
        6 correct calculations, the expected number of correct assignments is 6/6 (which equals 1)
       *Easiest way to do above is E[Matches], and equal it to the sum of expectations for each person; because each person has 1/3 chance
        to pick their name in the 3 erson example above, the sum of expectations would be 1/3 + 1/3 + 1/3, which equals 1. It ends up
        being 1/n, for n amount of times.
  Variance
  **** *Distributions can have the same expectd value but have narrow vs wide spreads; for example, if there's heads you win 1dollar and
        tails you lose 1dollar your expected value is 0...in another example if there's heads you win 100dollars and tails you lose 
        100dollars which has an expected value of 0. The 100dollar wone has a larger spread...expected value is not a good measure here but
        variance is
       *Spread is how far away points are from expected outcome
       *One way for quantifying it is deviation (x - E[x]...actual outcome minus expected outcome); another is expected deviation
        (E[x - E[x]]), which appears when negative and positive deviations cancel each other out; and absolute deviation |E[x - E[x]]|;
        but the most common is SQUARED DEVIATION ((x - E[x])^2), and off that EXPECTED SQUARED DEVIATION (E[(x - E[x])^2]), the expected
        of which is the average of squared deviations and is VARIANCE
       *Steps to finding variance: 1) Finding x's mean, 2) find the deviation from the mean for every value of x, 3) square deviations,
        4) average those squared deviations
       *Heads win 2dollars vs tails lose 2dollars...vs heads win 3dollars vs tails lose 1dollar; they have different expected values (2 and
        2 is 0 EV; 3 and 1 is 1 EV), they have the same spread of outcomes (prob*(point-expected outcome)^2 for each point), which has
        the notation E[X^2]-E[X]^2, simplified from E[(x - E[x])^2] being a constant
       *Variance can be found by squaring the change in deviations when a function is transformed (ex: dice game where orginially it is 
        a 3.5 EV and a .5 deviation, but then it is transformed based on the game and it becomes 2 EV and a 1 deviation...in the transform,
        all the deviations have doubled so the variance of the transform is the square of that deviation change, when in comparison to
        the original)
       *in Var(ax + b), changes in b don't chage the pread, but changes in a do
  Standard Deviation
  **** *Variance squares things (leaves them in units like feet squared); standard deviation is the sqrt of variance (measures spread of
        a distribution using same units of the distribution)
       *Example is a distribution of meters...variance will provide a meters squared figure; just take the sqrt for standard deviation
       *68% of data is within 1 standard deviation of a maean of a normal distribution...95% is within 2 sd...99.7% is within 3 sd
  Sum of Gaussians
  **** *Ex: Total response time of a computer system (R) being sum of processing time (T) and network latency (L)...equation is
        R = T + L. If we could model T in milliseconds with a Gaussian distribution of T ~ N(10, 2^2) and L in milliseconds with
        L ~ N(5, 1^2) which means a mean of 5 and standard deviation of 1, and that T and L are independent of each other, the
        Gaussian of R would be N(10+5,4+1)
  Standardizing a Distribution
  **** *Desiring for a mean of 0 and a standard deviation of 1
       *With a distribution of X of mean 2 and sd 2, for example X - 2 centers the distribution, (X - 2) / 2 scales the distribution,
        and it is fully standardized
  Skewness and Kurtosis: Moments of a Distribution
  **** *E[x] is the first moment of a distribution, and E[x^2] is the second distribution (second moment is related to variance, but uncentered);
        third moment is E[x^3] and so on until the kth moment (probability at this point would be p1x1^k+...+pnxn^k)
  Skewness and Kurtosis - Skewness
  **** *Lottery vs Car Insurer...lottery = pay $1, can win $100 means that you can win $99 with 1% probability and lose $1 with 99% probability;
        car insurance = costs $1, crash repairs are covered to $100 means that you gain $1 (as insurer) with 99% probability and lose $99 with
        1% probability
       *In above example variance (spread of outcomes) looks the same (-1 to 99, 1 to -99); and expected values are the same (99*.01 - 1*.99 = 0;
        1*.99 - 99*.01 = 0). You could find actual variance by simply squaring the expected value and then multiplying it by its probability and adding
        the different components (ex: for lottery, (-1)^2*0.99 + (99)^2*0.01 = 99, for car insurance, (1)^2*0.99 + (-99)^2*0.01 = 99)
       *By having the same expected value and variance, they have the same first and second moment respectively...but these are different games so the
        way to tell them apart is to go on to the third moment, which is CUBING the expected value and then multiplying it by its probability (ex: 
        for lottery, (-1)^3*0.99 + (99)^3*0.01 = 9702, for car insurance, (1)^2*0.99 + (-99)^2*0.01 = -9702)...this tells us, in the example of 9702,
        there are some values really far out to the right (positive skewness), and for -9702, some values really far to the left (negative skewness)
       *Skewness is the standardized and cubed distribution; the third moment (notation is E[(X - mean/sd)^3]...note when this is 0, it's not a skewed
        distribution
  Skewness and Kurtosis - Kurtosis
  **** *Game 1 vs Game 2...game 1 is 1/2 chance you win a dollar, 1/2 chance you lose a dollar; game 2 is 100/202 chance you win 10 cents, 100/202 chance
        you lose 10 cents, 1/202 chance you win 10 dollars, 1/202 chance you lose 10 dollars
       *It appears both distributions have the same expected value (they do)...same variance (they do)...same skewness (they do...distributions symmetric
        around the midpoint always do)
       *The way to describe the difference is Game 2 has values that are really far away from the midpoint 0...this can be in the fourth moment or KURTOSIS
       *Notation of kurtosis = E[(X - mean/sd)^4]; in the case of distributions when tails are skinny (not many far out points), kurtosis is small...when
        the tails are thicker, kurtosis is large
  Quantiles and Box-Plots
  **** *Median is the point that splits data in half...the median percentile isthe 50% percentile or square percentile
       *Quantized essays provide solution, as the kth quantile is value that leaves k% of data to the lef, and (100-k)% to the right
       *25% = first quartile; 50% = second quartile; 75% = third quartile
  Visualizing Data: Box-Plots
  **** *Quartile in even list is the average of the two middle values
       *Interquartile range tells you where the percentage of your data lies (ex: difference between Q3 and Q1 tells how much of your data is in 50% of the data)
       *Shown a box plot without outliers, with outliers
  Visualizing Data: Kernel Density Estimation
  **** *Approximate the probability density function of a dataset with KERNEL DENSITY ESTIMATION
       *Kernel Density Estimation process = 1) draw the observations along the x axis, and see where there's a lot of density, then 2) draw a gaussian centered at each
        observation, and 3) multiplying everything by 1/n and sum the curves
  Visualizing Data: Violin Plots
  **** *Brings together the data of the kernel density estimation and the box plot
  Visualizing Data: QQ Plots
  **** *Some models assume normally distributed data (linear/logistic regression, Gaussian Naive Bayes, etc)
       *How do we get an exact answer of if a distribution is normal? Quantile Quantile plots, or QQ plots.
       *Get it by standardizing data (subtracting by mean and dividing by standard deviation), then compare quantiles of data, then
        compare to Gaussian quantiles


* LESSON 2 - Probability Distributions with Multiple Variables
  Joint Distribution (Discrete) - Part 1
  **** *Ex: In a 10 child pool, what's the probability a child is 9 years old and 49 inches tall? (0.4 likelihood of child being 9, because 4 of the kids in the child pool are 9; 
        3 of these kids are 49 inches tall...divide 3 by 10 and you get a joint distribution
       *Notation = Pxy(x, y)
  Joint Distribution (Discrete) - Part 2
  **** *Probability mass function of rolling a die; all six options have a 1/6 propbability of being rolled
       *If there were two distributions of die roll probabilities, they would be independent distributions...for example, probability of rolling 1 on one die and 6 on another is
        1/6 multiplied by 1/6, which equals 1/36. Probability mass function of this joint distribution would be a table of 1/36 for each combination.
       *Notation if the dice were labelled x and y would be P_xy(x, y) = P(X = x, Y = y) = P(x) * P(y)
       *Now non-independent example; x is a first die rolled and y is the sum of the two dice after a second die is rolled. Distribution of x is the same 1/6 probaility for each
        number to be rolled...distribution of y is shaped like a bell-shaped normal distribution with some values having more probbability than others (6/36 probability for 7, for
        example)...an example in this case is P(X=1, Y=1) = 0 because can't roll 0 on a die...P(X=1, Y=2) = 1/36 because only 1 can be rolled on the second die, so 1/6 * 1/6 = 1/36.
  Joint Distribution (Continuous)
  **** *Ex: X is the waiting time before a call is picked up (0 to 10 minutes), and Y is the customer satisfaction rating (rating from 0 to 10)...sideways histogram, heatmap, etc
  Marginal and Conditional Distribution
  **** *Using the example of ages and heights...conditional distribution (only heights of people over the age of 30) and marginal distribution (only caring about heights; aggregate over
        ages)
       *Ex: Marginal Distribution is getting heights and ages and choosing only the variable height and seeing the probability of ech height in the dataset. Notation is P_y(y_j) = sum of
        P_xy(x_i, y_j)...sum over column or row of joint distribution table is marginal probability
  Conditional Distribution
  **** *Ex: In the ages/heights problem, caring only about one of the ages like age 9; notation is P_Y|X=9(y) = P(Y=y|X=9)...in the case that the conditional distributions do not have 
        values that add up to one, you NORMALIZE the row  (ex in the video was age 9 had 4/10 of the total original distribution...in the conditional distribution, that becomes 4/4)
       *Notation = P(A,B) = P(A) * P(B|A)...probability of A and probability of B given A
       *Example of the normalizing = P(Y=49|X=9) = (3/10)/(4/10), because it's P(X=9,Y=49)/P(X=9) or probability of age equalling 9 and height equalling 49, given the distribution being
        analyzed is only age 9
  Covariance of a Dataset
  **** *Example of distribution of age & height; each has their own expected value and variance; but still needs to be captured is the relationship between the two...there is a 
        relationship between being older and being taller...this is captured in COVARIANCE and there is also correlation
       *The best way to depict it is scatterplots between the different variables
       *First find the mean of the variables; the find the variance for each variable (in the video example it's the same for age because same exact distribution for all three datasets;
        it becomes different for height, grades, and amount of naps taken per day), remembering variance is average of expected squared deviation (see above)
       *non-mathematically, covariance is morethan anything a test of relationship (if there's a positive relationship between two variables, there's a covariance above 0 with them;
        if it's negative, the covariance below 0; if no obvious relationship, it's 0)
       *First step = center graph (get the mean of the x and y coordinates to find the center and subtract points by it, then divide by standard deviation to make each step a standard 
        and comparable step). Second step = notice trend (sign of x and y?)...find the sum of centered x and y pairs for the dataset, average is based on and you have COVARIANCE
  Covariance of a Probability Distribution
  **** *Using the win dollar lose dollar games as example; find the expected value for each player; if expectancies don't tell the games apart, maybe variance (SECOND MOMENT; EXPECTED
        VALUE SQUARED)...if neither sometimes the way to look at it as not only a measure of each player individually, but together, which is where COVARIANCE comes in
       *When probabilities are unequal, multiply by the probbility for each outcome (minus mean)...instead of adding them all and dividing by n at the end, divide by n for each
       *Covariance of a joint continuous distribution is E(XY) - E(X)E(Y) (product of expected value (sum of outcomes/number of outcomes) for each individual variable, subtracted from
        their covariance (sum of product of centered outcomes together/number of outcomes)
       *Maybe come back...
  Covariance Matrix
  **** *A matrix where the downwards diagonal (left to right) has the variances of each variable, while the other places have the covariance of variables
  Correlation Coefficient
  **** *How we can tell how strong of a correlation two variables have with each other
       *Between -1 and 1
       *Correlation coefficient is Cov(x,y)/((sd_x)*(sd_y))...it is a standardized covariance (remembering sd or standard deviation is the sq root of variance)
       *NOTE: VARIANCE IS (VALUE OF ALL VARIABLE X INSTANCES - MEAN OF ALL THE VARIABLE X's INSTANCES) SQUARED, SUMMED OVER ALL INSTANCES OF A VARIABLE X IN A DATASET, DIVIDED BY 
        NUMBER OF INSTANCES
       *NOTE: COVARIANCE IS --- GOING OFF ABOVE --- THE PRODUCT OF (VALUE - MEAN) FOR A HYPOTHETICAL VARIABLE X AND A HYPOTHETICAL VARIABLE Y, SUMMED OVER ALL INSTANCES OF
        VARIABLE X-VARIABLE Y PAIRS OF A DATASET, DIVIDED BY NUMBER OF INSTANCES OF THE PAIRS
       *NOTE: COVARIANCE IS ALSO EXPECTED VALUE OF X AND Y PAIRS, SUBTRACTED BY THE PRODUCT OF THE VARIANCE OF X AND THE VARIANCE OF Y
  Multivariate Gaussian Distribution
  **** *We recall the equation for a normal Gaussian distribution is f_x(x) = (1/(sqrt of 2*pi)(sd))*e^(-1/2*((x-mean)^2/(sd)^2)), where mean is center of bell and sd is spread of bell;
        with multiple variables, if the variables are independent (for example weight and height) then f_hw(h,w) = f_h(h)*f_w(w)...the equation we'd get for the Gaussian is
        (1/(2*pi)(sd_height*sd_weight))*e^(-1/2*((height-mean_height)^2/(sd_height)^2) + (weight-mean_weight)^2/(sd_weight)^2)). IN THE CASE OF DEPENDENT, THIS WILLCAUSE AN ELONGATED
        DISTRIBUTION THAT LOOKS ALMOST SLOPED IN A DIRECTION WITH THE SLICE PLOT (DUE TO COVARIANCE, WHICH MEANS THERE IS A RELATIONSHIP). ALSO, THE EQUATION
        ((height-mean_height)^2/(sd_height)^2) + ((weight-mean_weight)^2/(sd_weight)^2) IS THE SQUARED NORM OF
        || ((height-mean_height)^2/(sd_height)^2) | ((weight-mean_weight)^2/(sd_weight)^2) ||, WHICH CAN BE WRITTEN AS
        ([height | weight] - [mean_height | mean_weight])^T * [sd_height**2, 0 | 0, sd_weight**2]^-1 * ([height | weight] - [mean_height | mean_weight]).
        [sd_height**2, 0 | 0, sd_weight**2] IS THE COVARIANCE MATRIX; [mean_height | mean_weight] is the VECTOR MEAN; (sd_height*sd_weight) IS SQUARE ROOT OF COVARIANCE MATRIX DETERMINANT.
        Finally this is (1/(2*pi)(SQRT COV MAT DET))*e^(-1/2*(([height | weight] - VECTOR MEAN)^T * COV MAT^-1([height | weight] - VECTOR MEAN))).
       *General definition = f_x(x) = (1/(2*pi)^(n/2)*(SQRT COV MAT DET))*e^(-1/2*((x - mean)^T * COV MAT^-1(x - mean))).
       *NOTE: THE BIGGER THE DETERMINANT THE BIGGER THE SPREAD.
       *NOTE: FOR UNIVARIATE, WE WORK WITH SCALAR VALUES AND VARIANCES. FOR MULTIVARIATE, WE WORK WITH VECTORS AND THE COVARIANCE MATRIX.
