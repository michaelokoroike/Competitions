WEEK 3: Vectors and Linear Transformations

* VECTOR ALGEBRA
  Machine Learning Motivation
  **** *Main topics are vectors and matrices, and their properties such as size/direction of a vector
       *Think of vectors and matrices as a lot like numbers, can be added, multiplied, divided, subtracted (together)
       *Just like numbers you can find the multiplicative inverse of a matrix (inverse of 2 is 1/2, for example), under 
        certain conditions
       *Matrices and vectors are essential in data sets
       *Will learn about linear transformations (can hlp you visualize matrices graphically)
       *Examples in real world/uses include AI generated images, image-to-text/text-to-image generations...all of this is
        derived from manipulating matrices, vectors and their operations
  Check Your Knowledge
  **** *Didn't know distance between vectors, matrix that represents a transformation, compute the inverse of a matrix off 
        the bat
  Vectors and their Properties
  **** *Seen as arrows in a plane/higher dimensional place
       *Impoartant aspects are magnitude (size/direction)
       *Vector simply put is a couple of numbers
       *Number of coordinates in the vector is the dimension of the space in which it lives
       *Taxicab distance (two sides of triangle) vs helicopter distance (hypotenuse)...taxicab distance is "L1 norm" (|a| + |b|),
        helicopter distance is "L2 norm" (square root of (a**2 + b**2); default algorithm
       *Direction of a vector can be derived from coordinates; showed me need to target a little bit
  Sum and Difference of Vectors
  **** *Example of sum = u: (4, 1), v: (1, 3), u + v = (4+1, 1+3) = (5,4). The sum of (5, 4) creates a diagonal in a parallelogram
        between the vectors u and v.
  **** *Example of difference = u: (4, 1), v: (1, 3), u - v = (4-1, 1-3) = (3,-2). The difference of (3, -2) when translated matches
        the vector obtained by joining the points u and v.
       *General definition is sum/difference component by component
  Distance between Vectors
  **** *One measurement is L1 distance (|a| + |b|), and another is L2 distance (square root of (|a|**2 + |b|**2))
       *Example is u: (6, 2), v: (1, 5), where u - v = (6-1, 2-5) = (5,-3). L1 distance is |5| + |-3|, which is 8. L2 distance is
        square root of |5|**2 + |-3|**2, which is 5.8.
  Multiplying a Vector by a Scalar
  **** *Result of vector * scalar is an elementwise product (ex: (1,2) vector * 3 scalar = (1*3, 2*3) vector (3,6))
       *It's a stretcing of a vector...if the scalar is negative, it's a stretching and reflecting across the origin
  The Dot Product
  **** *Example is buying different foods at different prices and finding how much you spent in total (2 apples and 4 bananas, with
        an apple costing $5 and a banana costing $3...2*$5 + 4*$3 = $22)
       *Ususally first vector is a row and second is a column
       *L2-Norm is always the square root of the dot product between a vector and itself
       *Vector transpose; converts columns into rows, and rows into columns. Matrix transpose; the columns become the rows. 
        X * y-transpose is typically how two vectors are multiplied.
       *<x, y> is another notation of the dot product
  Geometric Dot Product
  **** *The angle between two vectors is a significant measure...there's actually a relationship between the angle between vectors and
        the dot product
       *Orthogonal (perpendicular) vectors ave a dot product of 0 (ex: u: (-1, 3), v: (6, 2)...-1*6 + 3*2 = 0); necessity for
        orthogonality.
       *Dot product between a vector and itself (<u,u>) is L2-Norm squared (|a|**2 + |b|**2); dot product between two orthogonal 
        (perpendicular) vectors (<u,v>) is always 0; 
       *Dot product of a vector and itself (u, u) is the norm squared over the product of the norm and itself; dot product of a vector 
        and a longer vector (u, v) in the same direction is similar (simply the product of the two norms u * v); if the vectors (u, v) 
        have an angle between them, basically same as the sentence before this one, but you have to perpendicularly project one vector 
        onto the other (u' * v)...this is where COSINE OF AN ANGLE comes in (u' * v is equal to u * v * cos of angle between vectors)
       *Geometrically a dot product is positive because the projection onto another vector has positive length; it is negative because
        the projection of a vector moves in the opposite direction of the major vector...therefore THE SIGN OF A DOT PRODUCT OF A
        VECTOR CORRESPONDS TO BEING ON A SIDE OF THE ORTHOGONAL VECTOR TO THE VECTOR IN QUESTION
  Multiplying a Matrix by a Vector
  **** *Basically an amalgamation of vector by vector multiplication...in multiple Ax = b in a system of equations for example,
        x is a constant, while A and b can be switched out. It's multiple dot products stacked together

* LINEAR TRANSFORMATIONS
  Matrices as Linear Transformations
  **** *Matrices have previously been painted as arrays of numbers that represent systems of linear equations; but another useful
        representations of matrices is as LINEAR TRANSFORMATIONS (a way to send each point in a plane into nother point in the plane
        in a very structured way)
       *In example in video (square became parallelogram), untransformed and transformed coordinates are the BASIS. The main property
        of a BASIS is that it covers an entire plane (both do)...because this is the case, LINEAR TRANSFORMATIONS are simply change in
        coordinates, since it encompasses the same dimensional space.
       *In example in video, you see that the linear transformation value is the value if moving by the same amount of original 
        coordinates through the new coordinate system (ex: moving 2 to the left and 3 upwards in the original system, if the same 2 and 3
        is done in the new system, you end up at 3 to the left and 4 up
  Linear Transformations as Matrices
  **** *Instead of matrix to transformation, transformation to matrix
       *The only necessary info in the video is the point translated from (1,0) and the point translated from (0,1). In a 2x2 matrix,
        the one translating from (1, 0) will be the first column, and the one translating from (0,1) will be second.
  Interactive Tool: Linear Transformations
  **** *When shearing/updating along the x axis, update the second column; along the y axis, update the first. Rotating along y = x
        yields no change. Reflection about the origin is the same as rotating along y = -x. Projection onto an axis, is flattening onto
        the axis (for x, it's zeroing out the second column...for y, the first). 45 degree turn is 0.70...anticlockwise is negative in
        the second (x-changing) column, clockwise is is negative in the first column. Rotation by 180 degrees is negative in both columns.
       *2a) Rotation about the y axis makes the face look left with the same up-down orientation. 2b) Rotation 180 degrees keeps the face
            looking left but turns the face upside down (difference between this and 2a is a negative x translation in 2b, which flips
            the face upside down across the x). 2c) the face disappears with a projection.
       *3a) Depends; a standard [1 0|0 1] keeps the image ths same; a bigger identity matrix makes it a bigger image, a smaller one will
            make it smaller, a negative one will flip upside and left along both axes. 3b) in the second column, if the 1 in the identity
            matrix is made to be 2, it stretches vertically, but in the first column it stretches horizontally. 3c) If you change one
            of the zeroes to a 1, it shears (in the second column horizontally, and in the first column vertically).
       *4b) The sheared has to be sheared, etc...interestingly, you can't transform a line to a plane...but a plane can go to a line.
