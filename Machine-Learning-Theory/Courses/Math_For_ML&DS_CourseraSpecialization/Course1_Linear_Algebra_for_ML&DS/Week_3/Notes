WEEK 3: Vectors and Linear Transformations

* VECTOR ALGEBRA
  Machine Learning Motivation
  **** *Main topics are vectors and matrices, and their properties such as size/direction of a vector
       *Think of vectors and matrices as a lot like numbers, can be added, multiplied, divided, subtracted (together)
       *Just like numbers you can find the multiplicative inverse of a matrix (inverse of 2 is 1/2, for example), under 
        certain conditions
       *Matrices and vectors are essential in data sets
       *Will learn about linear transformations (can hlp you visualize matrices graphically)
       *Examples in real world/uses include AI generated images, image-to-text/text-to-image generations...all of this is
        derived from manipulating matrices, vectors and their operations
  Check Your Knowledge
  **** *Didn't know distance between vectors, matrix that represents a transformation, compute the inverse of a matrix off 
        the bat
  Vectors and their Properties
  **** *Seen as arrows in a plane/higher dimensional place
       *Impoartant aspects are magnitude (size/direction)
       *Vector simply put is a couple of numbers
       *Number of coordinates in the vector is the dimension of the space in which it lives
       *Taxicab distance (two sides of triangle) vs helicopter distance (hypotenuse)...taxicab distance is "L1 norm" (|a| + |b|),
        helicopter distance is "L2 norm" (square root of (a**2 + b**2); default algorithm
       *Direction of a vector can be derived from coordinates; showed me need to target a little bit
  Sum and Difference of Vectors
  **** *Example of sum = u: (4, 1), v: (1, 3), u + v = (4+1, 1+3) = (5,4). The sum of (5, 4) creates a diagonal in a parallelogram
        between the vectors u and v.
  **** *Example of difference = u: (4, 1), v: (1, 3), u - v = (4-1, 1-3) = (3,-2). The difference of (3, -2) when translated matches
        the vector obtained by joining the points u and v.
       *General definition is sum/difference component by component
  Distance between Vectors
  **** *One measurement is L1 distance (|a| + |b|), and another is L2 distance (square root of (|a|**2 + |b|**2))
       *Example is u: (6, 2), v: (1, 5), where u - v = (6-1, 2-5) = (5,-3). L1 distance is |5| + |-3|, which is 8. L2 distance is
        square root of |5|**2 + |-3|**2, which is 5.8.
  Multiplying a Vector by a Scalar
  **** *Result of vector * scalar is an elementwise product (ex: (1,2) vector * 3 scalar = (1*3, 2*3) vector (3,6))
       *It's a stretcing of a vector...if the scalar is negative, it's a stretching and reflecting across the origin
  The Dot Product
  **** *Example is buying different foods at different prices and finding how much you spent in total (2 apples and 4 bananas, with
        an apple costing $5 and a banana costing $3...2*$5 + 4*$3 = $22)
       *Ususally first vector is a row and second is a column
       *L2-Norm is always the square root of the dot product between a vector and itself
       *Vector transpose; converts columns into rows, and rows into columns. Matrix transpose; the columns become the rows. 
        X * y-transpose is typically how two vectors are multiplied.
       *<x, y> is another notation of the dot product
  Geometric Dot Product
  **** *The angle between two vectors is a significant measure...there's actually a relationship between the angle between vectors and
        the dot product
       *Orthogonal (perpendicular) vectors ave a dot product of 0 (ex: u: (-1, 3), v: (6, 2)...-1*6 + 3*2 = 0); necessity for
        orthogonality.
       *Dot product between a vector and itself (<u,u>) is L2-Norm squared (|a|**2 + |b|**2); dot product between two orthogonal 
        (perpendicular) vectors (<u,v>) is always 0; 
       *Dot product of a vector and itself (u, u) is the norm squared over the product of the norm and itself; dot product of a vector 
        and a longer vector (u, v) in the same direction is similar (simply the product of the two norms u * v); if the vectors (u, v) 
        have an angle between them, basically same as the sentence before this one, but you have to perpendicularly project one vector 
        onto the other (u' * v)...this is where COSINE OF AN ANGLE comes in (u' * v is equal to u * v * cos of angle between vectors)
       *Geometrically a dot product is positive because the projection onto another vector has positive length; it is negative because
        the projection of a vector moves in the opposite direction of the major vector...therefore THE SIGN OF A DOT PRODUCT OF A
        VECTOR CORRESPONDS TO BEING ON A SIDE OF THE ORTHOGONAL VECTOR TO THE VECTOR IN QUESTION
  Multiplying a Matrix by a Vector
  **** *Basically an amalgamation of vector by vector multiplication...in multiple Ax = b in a system of equations for example,
        x is a constant, while A and b can be switched out. It's multiple dot products stacked together

* LINEAR TRANSFORMATIONS
  Matrices as Linear Transformations
  **** *Matrices have previously been painted as arrays of numbers that represent systems of linear equations; but another useful
        representations of matrices is as LINEAR TRANSFORMATIONS (a way to send each point in a plane into nother point in the plane
        in a very structured way)
       *In example in video (square became parallelogram), untransformed and transformed coordinates are the BASIS. The main property
        of a BASIS is that it covers an entire plane (both do)...because this is the case, LINEAR TRANSFORMATIONS are simply change in
        coordinates, since it encompasses the same dimensional space.
       *In example in video, you see that the linear transformation value is the value if moving by the same amount of original 
        coordinates through the new coordinate system (ex: moving 2 to the left and 3 upwards in the original system, if the same 2 and 3
        is done in the new system, you end up at 3 to the left and 4 up
  Linear Transformations as Matrices
  **** *Instead of matrix to transformation, transformation to matrix
       *The only necessary info in the video is the point translated from (1,0) and the point translated from (0,1). In a 2x2 matrix,
        the one translating from (1, 0) will be the first column, and the one translating from (0,1) will be second.
  Interactive Tool: Linear Transformations
  **** *When shearing/updating along the x axis, update the second column; along the y axis, update the first. Rotating along y = x
        yields no change. Reflection about the origin is the same as rotating along y = -x. Projection onto an axis, is flattening onto
        the axis (for x, it's zeroing out the second column...for y, the first). 45 degree turn is 0.70...anticlockwise is negative in
        the second (x-changing) column, clockwise is is negative in the first column. Rotation by 180 degrees is negative in both columns.
       *2a) Rotation about the y axis makes the face look left with the same up-down orientation. 2b) Rotation 180 degrees keeps the face
            looking left but turns the face upside down (difference between this and 2a is a negative x translation in 2b, which flips
            the face upside down across the x). 2c) the face disappears with a projection.
       *3a) Depends; a standard [1 0|0 1] keeps the image ths same; a bigger identity matrix makes it a bigger image, a smaller one will
            make it smaller, a negative one will flip upside and left along both axes. 3b) in the second column, if the 1 in the identity
            matrix is made to be 2, it stretches vertically, but in the first column it stretches horizontally. 3c) If you change one
            of the zeroes to a 1, it shears (in the second column horizontally, and in the first column vertically).
       *4b) The sheared has to be sheared, etc...interestingly, you can't transform a line to a plane...but a plane can go to a line.
  Matrix Multiplication
  **** *Matrix multiplications = transforming two linear transformations into a third one...knowing the B from A to C, and using that
       *Number of columns in first matrix must match numbr of rows in second matrix (example of 1. 2x3 and 2. 3x4... 3 columns in 1,
        matches 3 rows in 2). Result takes number of rows in first matrix and number of columns in second (example of 1. 2x3 and 2. 3x4;
        result is a 2x4 matrix)
  Identity Matrix
  **** *Identity matrix is the matrix equivalent of multiplying by 1; it leaves the plane intact
  Matrix Inverse
  **** *Example is inverse of a number (ex: inverser of 2 is 1/2, which multiply together to equal 1); inverse of a matrix has the 
        product of an identity matrix
       *Inverse matrix, in a linear transformation, undoes the job of the original matrix, returning the plane to what it was at the
        beginning
       *Way to think about it...original plane starts out as identity matrix times coordinates...the linear transformation is the 
        transformation matrix times the identity matrix times those original coordinates. You get it back by technically dividing
        out that transformation matrix (ex of inverse of 2 being 1/2...inverse is essentially dividng by itself). The inverse matrix
        is the transformation matrix to the -1 (example in video: notation of inverse of [3 1|1 2] is [3 1|1 2]^-1).
       *You get the inverse by solving a system of linear equations (ex: there's an [a b|c d] that is [3 1|1 2]^-1...so 
        [3 1|1 2] * [a b|c d] = [1 0|0 1]; in this case, we actually have four dot products (thinking back to matrix multiplication,
        since it's 2x2 and 2x2, dot product of n row of first matrix and n column of second matrix is the value of the n row/n column
        of the result matrix)...therefore [3 1]*[a c] = 1 ([a c] is the first column of the second matrix), [3 1]*[b d] = 0,
        [1 2]*[a c] = 0, [1 2]*[b d] = 1...therefore 3a + 1c = 1, 3b + 1d = 0, 1a + 2c = 0, and 1b + 2d = 1...
       *[1 1|2 2] has no solution because there is a contradiction in its system of equations
  Which Matrices have an Inverse?
  **** *Not all numbers have inverses (0 doesn't, etc)
       *Matrices with a determinant of 0 have no inverse
  Neural Networks and Matrices
  **** *Neural networks are largely made up of matrices and matrix products
       *Example of a spam dataset is given (which emails are spam, and how many instances of the word lotteery and win are in the
        examples)
       *You have to find a spam filter based on thresholds and actual values (based on what are/aren't spam and how many instances of
        'lottery' and 'win' exist in the emails, what are the points to allot to each word, and the threshold to determine if it is
        spam or not
       *It ends up that the threshold line is a dot product of the points allotted to each word * word, and equalling that threshold
        value (ex: 1 point to win and 1 point to lottery and a threshold of 1.5 = [1|1]*[number of 'win's|number of 'lottery's] as a
        dot product)...that line is a linear classifier (negative zone has less than 1.5, positive zone has more than 1.5)
       *Note: Linear classifiers are the simplest neural networks (they're networks with one layer)
       *Note: A one layer neural network can be seen as a matrix product followed by a threshold check (example: a spam email
        with 2 'lottery's and 1 'win' and points of 1 for each word is essentially [2 1]*[1 1], which is 3; since that is greater 
        than 1.5, it is in the positive zone and is spam)
       *[1|1]*[number of 'win's|number of 'lottery's] > 1.5 is the same as [1|1]*[number of 'win's|number of 'lottery's] - 1.5 > 0...
        the -1.5 is the bias of the equation...to include it in the equation, you essentially add it into the dot product equation
        (example...if bias is -1.5, you add a column to the dataset so that the matrix becomes [1|1|1] and 
        [number of 'win's|number of 'lottery's|-1.5]
       *Shown a perceptron (a tiny version of a neural network) show weights*variables, with the bias also included in the input), 
        which is then updated by the activation function
