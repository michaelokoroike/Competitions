WEEK 1: UNSUPERVISED LEARNING

* WELCOME TO THE COURSE!
  Welcome!
  **** *We will learn about clustering (grouping data into clusters) and anomaly detection, forms of unsupervised learning.    
       *We will learn about recommender systems in the second week, one of the most commercially important machine learning technologies.
       *We will learn about reinforcement learning in the third week.

* CLUSTERING
  What is Clustering?
  **** *Clustering looks at data points and automatically finds data points that are similar in some way, shape or form.
       *In unsupervised learning, you are not given y...can't tell an algorithm what is the correct algorithm; can only find interesting 
        aspects of the data. CLUSTERING algorithms look for structures in the data.
       *Applications of clustering are grouping news, market segmentation, DNA analysis, astronomical analysis, etc.
  K-means Intuition
  **** *What K-means does is assign points to cluster centroids and move cluster centroids    
       *First takes a random guess as to where are the centers of the clusters (CLUSTER CENTROIDS)
       *Second after the initial guess, it (K-means) will go through all of the data points and check which centroid it is closer/closest to
       *After this, it will look at all the points closest to a centroid and take the average of them, and therefore recompute the centroid
        as that average
       *It repeats until CONVERGENCE, where there is no more changes to centroid
  K-means Algorithm
  **** *First step is to randomly initialize k cluster centroids (u1, u2, ..., uk)
       *Like already said, first k meands assigns points to cluster centroids; distance between point and centroid is the L2 norm...in this 
        step, knowing the initialized k cluster centroids, you find the minimum distance between the point and the centroids...better to 
        use the squared distance (only magnifies the relationships)
       *Second step is to move the cluster centroids by computing the average of points assigned to each cluster
       *When a cluster has no points assigned to it, either eliminate the cluster or reinitialize it
  Optimization Objective
  **** *Recap: Typically, with algortihms it goes choose a cost function, then take an algorithm and optimize that cost function
       *K means is optimizing a specific cost function; remember that it is the average of the squared distance between a training example
        and its assigned centroid
       *Terms: c_i = index of cluster (1...k) to which example x_i (a training example) has been currently assigned; u_k = cluster centroid
        k; u_(c_i) = cluster centroid of cluster to which example x_i has been currently assigned.
       *Cost function: J(c_1...c_m, u_1...u_k) = 1/m * sum of ||x_i - u_(c_i)||^2. This is a cost function run for each cluster c_i through
        c_m, and each initialized centroid u_1 through u_k; and new centroids are calculated by these averages. It is trying to minimize 
        the squared distance. J is called the DISTORTION FUNCTION.
       *An option is moving the cluster centroid, instead of by average squared difference between centroid and its assigned examples, is to
        possibly take the average of the training examples and move the centroid there...seemed to minimize the cost function more when there
        was a lot of skew between distance between training points and centroids...just to play around; after taking the average, you would
        still need to do average squared difference, but with a better chosen centroid.
  Initializing K-means
  **** *Video will show ways you go about the random initialization of centroids
       *First off, centroids k should always be less than training examples m
       *The most common way to initialize cluster centroids is to randomly select k training examples, and initialize the cluster with
        those training example values. The problem here can be you can end up with a LOCAL OPTIMA, where there is no longer any
        reductions in distance per run, but a bad initialization choice created the centroids getting stuck at a local minimum.
       *The way to combat the getting stuck at local optima is running K-means multiple times. Run multiple and choose which one gives
        the lowest value for cost function J. 50-1000 times are the optimal amount of random initializations.
  Choosing the Number of Clusters
  **** *ELBOW METHOD = choose the value where the cost function decreases rapidly before, and slowly after (Andrew doesn't use elbow
        because it is subject and the graph is often smooth)
       *Choose based on how well the clusters perform for a desired later purpose (example is t-shirt sizing; a business can group
        clusters based on s/m/l or xs/s/m/l/xl, depending on the business and what makes sense for them...either would be completely
        legitimate)...but also examine tradeoff (how well the xs/s/m/l/xl fits, vs the extra costs of the more complicated sizes)

* ANOMALY DETECTION
  Finding Unusual Events
  **** *ANOMALY DETECTION looks at an unlabeled dataset of normal events and thereby learns to detect red flags or outlier occurrences
       *Example used is of aircraft engines...using features like heat generated by engine, vibration frequency of the engine, etc to
        be able to spot defective ones. After the algorithm has seen typical behavior, we can see if new examples seem typical or if there
        is something to be analyzed further.
       *Best analyzed by plotting and comparing visually.
       *Also use DENSITY ESTIMATION. Meaning, what is the probability that the training example given could be seen in the dataset? If
        less than some small number, you could call it an anomaly.
       *Used frequently in fraud detection (how often log in, web pages visited, transactions, posts, typing speed), manufacturing, 
        monitoring computers, etc.
  Gaussian (Normal) Distribution
  **** *Gaussian have mean u ("mu") and variance sigma^2 (which is just standard deviation squared)
       *Probability of x is p(x) = (1/((sq root of (2*pi))*sigma)) * e^((-(x-mu)^2)/(2*sigma^2))
       *When sigma is smaller, the distribution is skinnier and taller; when larger, the distribution is fatter and shorter. If the mu is
        changed the curve just shifts to the left or right
       *Mu is computed by the average of all of the training examples, while sigma squared is the average of the squared difference between
        examples and mu. Called the MAXIMUM LIKELIHOOD ESTIMATES for mu and sigma. Gives a PROBABILITY DISTRIBUTION, used to predict 
        anomalies.
       *Probabilities always have to sum to 1, so when sigma gets smaller, curve also gets taller
  Anomaly Detection Program
  **** *Now example with multiple features
       *Example back to aircraft. each example is a two-dimensional vector (has two features)
       *Probability equation is p(x[vec]) = p(x1)*p(x2)*p(x3)*...*p(xn); these multiplications illustrate statistical independence from
        the different features. Also include the mean mu and the variance sigma squared for each feature probability (needed in the 
        calculation of probability (see p(x) in section above)
       *The steps to creating an anomaly detection algorithm is 1) choosing n features x_i that may be indicative of anomalous examples,
        then 2) fitting mu and sigma squared for each example, and then 3) when given a new example, compute P(x), and 4) see if 
        P(x) is less than epsilon (if it is it is an anomaly)
  Developing and Evaluating an Anomaly Detection Program
  **** *Good to have a small numer of anomalous examples
       *Do great engines in a training set, more great engines and a few bad ones on cross validation, and the same kind of profile for a
        testing set as there was for a cross validation set
