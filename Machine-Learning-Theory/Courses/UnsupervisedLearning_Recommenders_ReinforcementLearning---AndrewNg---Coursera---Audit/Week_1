WEEK 1: UNSUPERVISED LEARNING

* WELCOME TO THE COURSE!
  Welcome!
  **** *We will learn about clustering (grouping data into clusters) and anomaly detection, forms of unsupervised learning.    
       *We will learn about recommender systems in the second week, one of the most commercially important machine learning technologies.
       *We will learn about reinforcement learning in the third week.

* CLUSTERING
  What is Clustering?
  **** *Clustering looks at data points and automatically finds data points that are similar in some way, shape or form.
       *In unsupervised learning, you are not given y...can't tell an algorithm what is the correct algorithm; can only find interesting 
        aspects of the data. CLUSTERING algorithms look for structures in the data.
       *Applications of clustering are grouping news, market segmentation, DNA analysis, astronomical analysis, etc.
  K-means Intuition
  **** *What K-means does is assign points to cluster centroids and move cluster centroids    
       *First takes a random guess as to where are the centers of the clusters (CLUSTER CENTROIDS)
       *Second after the initial guess, it (K-means) will go through all of the data points and check which centroid it is closer/closest to
       *After this, it will look at all the points closest to a centroid and take the average of them, and therefore recompute the centroid
        as that average
       *It repeats until CONVERGENCE, where there is no more changes to centroid
  K-means Algorithm
  **** *First step is to randomly initialize k cluster centroids (u1, u2, ..., uk)
       *Like already said, first k meands assigns points to cluster centroids; distance between point and centroid is the L2 norm...in this 
        step, knowing the initialized k cluster centroids, you find the minimum distance between the point and the centroids...better to 
        use the squared distance (only magnifies the relationships)
       *Second step is to move the cluster centroids by computing the average of points assigned to each cluster
       *When a cluster has no points assigned to it, either eliminate the cluster or reinitialize it
  Optimization Objective
  **** *Recap: Typically, with algortihms it goes choose a cost function, then take an algorithm and optimize that cost function
       *K means is optimizing a specific cost function; remember that it is the average of the squared distance between a training example
        and its assigned centroid
       *Terms: c_i = index of cluster (1...k) to which example x_i (a training example) has been currently assigned; u_k = cluster centroid
        k; u_(c_i) = cluster centroid of cluster to which example x_i has been currently assigned.
       *Cost function: J(c_1...c_m, u_1...u_k) = 1/m * sum of ||x_i - u_(c_i)||^2. This is a cost function run for each cluster c_i through
        c_m, and each initialized centroid u_1 through u_k; and new centroids are calculated by these averages. It is trying to minimize 
        the squared distance. J is called the DISTORTION FUNCTION.
       *An option is moving the cluster centroid, instead of by average squared difference between centroid and its assigned examples, is to
        possibly take the average of the training examples and move the centroid there...seemed to minimize the cost function more when there
        was a lot of skew between distance between training points and centroids...just to play around; after taking the average, you would
        still need to do average squared difference, but with a better chosen centroid.
  Initializing K-means
  **** *Video will show ways you go about the random initialization of centroids
       *First off, centroids k should always be less than training examples m
       *The most common way to initialize cluster centroids is to randomly select k training examples, and initialize the cluster with
        those training example values. The problem here can be you can end up with a LOCAL OPTIMA, where there is no longer any
        reductions in distance per run, but a bad initialization choice created the centroids getting stuck at a local minimum.
       *The way to combat the getting stuck at local optima is running K-means multiple times. Run multiple and choose which one gives
        the lowest value for cost function J. 50-1000 times are the optimal amount of random initializations.
