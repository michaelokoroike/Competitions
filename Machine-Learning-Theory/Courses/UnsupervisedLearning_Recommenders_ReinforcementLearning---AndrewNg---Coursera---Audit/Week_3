WEEK 3: REINFORCEMENT LEARNING

* REINFORCEMENT LEARNING INTRODUCTION
  What is Reinforcement Learning?
  **** *Moving an object from state s to action a
       *Supervised learning is an option but not a great approach; it's hard to find ideal acions for helicopters every moment it is
        in flight for example
       *REWARD FUNCTION = Tells when an object is doing well and when poorly. Maybe +1 is positive, -1000 is negative.
       *Tell it what to do rather than how to do it...this makes it powerful.
       *Examples of helicopters and dogs.
       *Applications = Controlling robots, factory optimization, stock trading, playing games
  Mars Rover Example
  **** *Rover can be in six positions; each position is a STATE. 1, 2, 3, 4, 5, 6.
       *State 1 has an interesting surface, and State 6 has an interesting surface but not as interesting as State 1. Set reward of State 1
        as 100 and State 6 as 40. Left is State 1, right is State 6.
       *Receive a reward based on every step taken
       *The ends are often called TERMINAL STATES
       *Summary = Starts at state S, takes action A, enjoys rewards R(S), ends up at S' (new state)
  The Return in Reinforcement Learning
  **** *Return is sum of rewards for steps taken, weighted by a DISCOUNT FACTOR gamma
       *Example: Return = S + (gamma)*(S') + (gamma^2)*(S") + ...
       *Has the effect of making the reinforcment learning algorithm impatient
       *Based on highest return decide what action to take from state
       *Push negative rewards out into future (same way put off paying people because money is worth less in future due to interest rate)
  Making Decisions: Policies in Reinforcement Learning
  **** *Always go for a nearer reward, larger reward, smaller reward, left/right
       *Create a function that is called POLICY (pi is notational figure) that tells you what action to take in every state (s) to maximize return
  Review of Key Concepts
  **** *States, actions, rewards, discount factor gamma, return, policy pi
       *Can be used in otherthings like helicopter, chess
       *The formalism of reinforcement learning application is called MARKOV DECISION PROCESS (MDP); future only depends on current state

* STATE-ACTION VALUE FUNCTION
  State-Action Value Function Definition
  **** *It is the value that reinforcement learning algorithms try to compute
       *Notation = Q(s,a)
       *It is the return if you start in state s and take the action a once, and then behave optimally afterwards.
     *Q(s,a) is often called the Q function/optimal Q function
       *Basically having all of the relative full returns in each state based on direction, allowing the algorithm 
        to know where to go
       *The best possible return is max Q(s,a)
  State-Action Value Function Example
  **** *Shows how changing the reward function and the discount factor gamma and seeing Q(s,a) and optimal return/optimal policy changes
  Bellman Equation
  **** *Helps us compute Q(s,a)
       *Notation = R(s) is reward of current state; s is current state; a is current action; s' is state you get after taking action a; a'
        is action you take in state s'; gamma is discount factor
       *Q(s,a) = R(s) + gamma*max_a' of Q(s',a')
