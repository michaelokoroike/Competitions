WEEK 3: REINFORCEMENT LEARNING

* REINFORCEMENT LEARNING INTRODUCTION
  What is Reinforcement Learning?
  **** *Moving an object from state s to action a
       *Supervised learning is an option but not a great approach; it's hard to find ideal acions for helicopters every moment it is
        in flight for example
       *REWARD FUNCTION = Tells when an object is doing well and when poorly. Maybe +1 is positive, -1000 is negative.
       *Tell it what to do rather than how to do it...this makes it powerful.
       *Examples of helicopters and dogs.
       *Applications = Controlling robots, factory optimization, stock trading, playing games
  Mars Rover Example
  **** *Rover can be in six positions; each position is a STATE. 1, 2, 3, 4, 5, 6.
       *State 1 has an interesting surface, and State 6 has an interesting surface but not as interesting as State 1. Set reward of State 1
        as 100 and State 6 as 40. Left is State 1, right is State 6.
       *Receive a reward based on every step taken
       *The ends are often called TERMINAL STATES
       *Summary = Starts at state S, takes action A, enjoys rewards R(S), ends up at S' (new state)
  The Return in Reinforcement Learning
  **** *Return is sum of rewards for steps taken, weighted by a DISCOUNT FACTOR gamma
       *Example: Return = S + (gamma)*(S') + (gamma^2)*(S") + ...
       *Has the effect of making the reinforcment learning algorithm impatient
       *Based on highest return decide what action to take from state
       *Push negative rewards out into future (same way put off paying people because money is worth less in future due to interest rate)
  Making Decisions: Policies in Reinforcement Learning
  **** *Always go for a nearer reward, larger reward, smaller reward, left/right
       *Create a function that is called POLICY (pi is notational figure) that tells you what action to take in every state (s) to maximize return
  Review of Key Concepts
  **** *States, actions, rewards, discount factor gamma, return, policy pi
       *Can be used in otherthings like helicopter, chess
       *The formalism of reinforcement learning application is called MARKOV DECISION PROCESS (MDP); future only depends on current state

* STATE-ACTION VALUE FUNCTION
  State-Action Value Function Definition
  **** *It is the value that reinforcement learning algorithms try to compute
       *Notation = Q(s,a)
       *It is the return if you start in state s and take the action a once, and then behave optimally afterwards.
     *Q(s,a) is often called the Q function/optimal Q function
       *Basically having all of the relative full returns in each state based on direction, allowing the algorithm 
        to know where to go
       *The best possible return is max Q(s,a)
  State-Action Value Function Example
  **** *Shows how changing the reward function and the discount factor gamma and seeing Q(s,a) and optimal return/optimal policy changes
  Bellman Equation
  **** *Helps us compute Q(s,a)
       *Notation = R(s) is reward of current state; s is current state; a is current action; s' is state you get after taking action a; a'
        is action you take in state s'; gamma is discount factor
       *Q(s,a) = R(s) + gamma*max_a' of Q(s',a')
       *R(s) is an IMMEDIATE REWARD; gamma*max_a' of Q(s',a') is the return of behaving OPTIMALLY or in the fashion dictated
  Random (Stochastic) Environment (Optional)
  **** *Sequence of different awards
       *Max of average value of discounted rewards...EXPECTED RETURN
       *Modifies Bellman Equation to Q(s,a) = R(s) + gamma*E[max_a' of Q(s',a')}; E is expected average return
       *"MISSTEP PROBABILITY

* CONTINUOUS STATE SPACES
  Example of Continuous State Space Applications
  **** *What we learned above is discrete state action values
       *Continous state spaces are vectors that usually contain positioning and rate of change of object; compared to discrete which is a state and
        possible other states
  Lunar Leader
  **** *Goal is to learn a policy pi, given state s is [vector of positions and velocities] and action a = pi*s which maximizes the return.
  Learning the State-Value Function
  **** *Train a neural network to approzximate Q(s,a)
       *Use Bellman's equation to create a training set with lots of examples; then use supervised learning to map from x to y
       *Create a dataset by trying different things; leads us to observes instances of being in a state, taking some action, getting a reward for being
        in that state, and getting to some new state. Notation of this would be a tuple (s, a, R(s), s'). 
       *Each of the tuples are enough to create a single training example; (s, a) make up x...(R(s), s') make up y.
       *Steps for the learing algorithm: 1) randomly initialize the neural network as a guess of Q(s,a)
        2) Repeat {*Take actions in the lunar lander. Get (s, a, R(s), s').
                   *Store 10,000 most recent (s, a, R(s), s') tuples. This is the REPLAY BUFFER.
                   *Train a NEURAL NETWORK; create training set of 10,000 examples using x = (s,a) and y = R(s) + gamma*max_a' of Q(s',a') (or (R(s), s')).
                    Train Q_new so that Q_new(s,a) = y.
                   *Set Q = Q_new.}
       *The algorithm above is called a DEEP Q_NETWORK (DQN).
  Algorithm Refinement: Improved Neural Network Architecture
  **** *Normal DQN is inefficient because you have to carry out inferences for multiple Qs at each state; the more efficient way to do it is to do all inferences
        simultaneously with a single neural network.
  Algorithm Refinement: Epsilon-Greedy Policy
  **** *Addresses question of how to take actions while still learning
       *Two typical options; 1) Pick an action a that maximizes Q(s,a). 2) With probability 0.95, pick the action that maximizes Q(s,a)...and with probability 0.05,
        pick an action a randomly.
       *The better option is 2) because with 1), if it is decided something is never a good idea by random chance, it will never learn that something may have its 
        instances of being a good idea. With 2) you give a small probability of trying out actions (called an EXPLORATION STEP). Probability of 0.95 asect is called
        a GREEDY/EXPLOITATION STEP because we're trying to maximize our return with what we know. This whole 1) and 2) algorithm is called EPSILON-GREEDY POLICY.
       *Start epsilon (random guessing) high. Gradually decrease. Over time, less likely to need to randomly guess.
  Algorithm Refinement: Mini-Batch Updates and Soft Updates (Optional)
  **** *Mini-batch = When you have too many training examples and your algorithm (like gradient descent) by definition averages over them all to compute...instead
        just taking a subset. It can be adding up examples or using different examples each time. Ex: Use 100 of 1,000 examples.
       *Soft Update = When you are updating Q with Q_new in the DQN process above, a soft updates prevents the neural network from being too negatively influenced
        by one off noisy Q's. Ex: When updatng parameters w and b (using w as the example), use formula w = 0.01*w_new + 0.99*w...giving the new less weight/gradually
        changing.
  The State of Reinforcement Learning
  **** *Limitations = Easier to get to work in a simulation than a real robot; few applications compared to supervised/unspuervised learning;.
       *But...exciting research!

* SUMMARY AND THANK YOU
  Summary and Thank You
  **** *Supervised Machine Learning: Regression and Classification (Linear Regression, Logistic Regression, Gradient Descent)
       *Advanced Learning Algorithms (Neural Networks, Decision Trees, Advice for ML)
       *Unsupervised Learning, Recommenders, Reinforcement Learning (Clustering, Anomaly Detection, Collaborative Filtering, Content-Based Filtering, Reinforcement
        Learning)
