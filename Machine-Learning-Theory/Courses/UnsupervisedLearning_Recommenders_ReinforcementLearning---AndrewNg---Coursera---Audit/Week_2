WEEK 2: RECOMMENDER SYSTEMS

* COLLABORATIVE FILTERING
  Making Recommendations
  **** *Example of predicting movie ratings; users making ratings from 1 to 5 stars
       *For recommender systems, there typically will be a number of users and a number of items
       *Notation: n_u is number of users, n_m is number of items, r(i,j) = 1 if a user has rated a movie in the example, y(i,j) = rating
        given by user j to to movie i (only if r(i,j) is 1)
  Using Per-Item Features
  **** *Continuing the movie example; adding romance and action as features to the dataframe/table
       *Notation continued: w_j, b_j is parameters for user j; x_i is feature vector for movie i; for user j and movie i, predict rating
        with w_j*x_i + b_j (linear regression); m_j is number of movies rated by user j; 
       *Cost function is ((1/(2*m_j)) * sum of (w(j) * x(i)+b(j) - y(i,j)) squared for each example) + 
        ((lambda/(2*m_j)) * sum of w_k_j squared for parameters w (b not included for simplicity))...trying to minimize squared error just
        like linear regression; difference between rating predicted and actual rating. i:r(i,j)=1 means looking only at the movies the
        user actually rated. Cost function notation here is min J(w_j, b_j) for user j.
       *Broaden out to all users.
  Collaborative Filtering Algorithm
  **** *Ways to go about finding features when features not given; take a reasonable guess (if you have all the parameters and four 
        ratings), to make the prediction equal to the actual.
       *Cost function is ((1/(2)) * sum of (w(j) * x(i)+b(j) - y(i,j)) squared for each example) + 
        ((lambda/(2)) * sum of x_k_i squared for features x)...trying to minimize squared error just
        like linear regression; difference between rating predicted and actual rating. i:r(i,j)=1 means looking only at the movies the
        user actually rated. Cost function notation here is min J(x_i) for movie i. This is to learn features like romance, etc without
        features being given.
       *
       *Broaden out to all movies; sum it.
       *This is given you haven w and b for all of the users, and wanting to learn x_i features of a movie using gradient descent.
       *Put the two cost functions together; Cost function is ((1/(2)) * sum of (w(j) * x(i)+b(j) - y(i,j)) squared for each example) + 
        ((lambda/(2)) * sum of x_k_i squared for features x) + ((lambda/(2*m_j)) * sum of w_k_j squared for parameters w...trying to minimize 
        squared error just like linear regression; difference between rating predicted and actual rating. i:r(i,j)=1 means looking only at 
        the movies the user actually rated. Cost function notation here is min J(w_j, b_j, x_i) for parameters w and b for user j and features x 
        for movie i.
       *Gradient descent would be 
        repeat until convergence {
         w_i_j = w_i_j - alpha*partial derivative with respect to w_i_j*J(w_j, b_j, x_i) over all training examples;
         b_j = b_j - alpha*partial derivative with respect to b_j*J(w_j, b_j, x_i) over all training examples;
         x_k_i = x_k_i - alpha*partial derivative with respect to x_k_i*J(w_j, b_j, x_i) over all training examples }.
       *Collaborative filtering isusing multiple users rating in a sort of collaboration, which can be used to help users
        in the future.
  Binary Labels: Favs, Likes, and Clicks
  **** *Examples of binary applications = did a user purchase an item after being shown it, did a user like an item, did they spend
        more than 30 seconds with an item, etc? Yes or no answers
       *See video = becomes a logistic regression model

* RECOMMENDER SYSTEMS IMPLEMENTATION DETAIL
  Mean Normalization
  **** *You will have wide ranges of values 1-5 for movies for example; mean normalization will help th algorithm move faster and make
        better predictions
       *Adding a new user who hasn't rated doesn't negatively affect the algorithm because it's only based on who has rated things...
        but also not good because the algorithm will start to think all new users will give only zero star ratings
       *Take the averages of ratings for each movie (?'s are zeros) and put into a vector those means...than subtract the original 
        ratings used to find the means by the mean...then there is a normalized rating table
       *Answer is w(j)*x(i)+b(j) + mean of the movie...?'s end up being the mean of all the ratings of the movie as opposed to 0, which
        is more reasonable
       *Normalize only rows (movie ratings)...normalizing user ratings isn't important if the movie has been rated...if it hasn't been seen
        then it doesn't make sense to ask for ratings and then columns make sense
