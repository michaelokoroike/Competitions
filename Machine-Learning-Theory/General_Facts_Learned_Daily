11/21/2022

- Why do ROC curves remain unchanged between balanced and imbalanced datasets? (The ROC Curve can still look good because it takes True Negatives into consideration, the 
  majority class in an imbalanced dataset...because thedataset has a high majority, the ROC Curve can seem to be accurate on the True Negatives, all while struggling on 
  the True Positives.)
- Why is normalization bad when input variables have outliers? (It will skew the effect of the variable; normalization creates a range for each variable between 0 and 
  1...in the case of credit fraud if a majority of the fraud loans are done at amounts between 5000 and 10000, but there is a good loan at 1000000, this good loan 
  actually screws up the correlation between price and fraud (as now thouse highly likely values for fraud are small (5000 divided by max of 1000000) and likely are 
  weighted low compared to inputs without outliers) and negates an otherwise strong relationship.)
- (Normalize when input variables have a Gassian distribution)
- "No skill": (A model that is as good as a random prediction; when imbalanced, can be the minority/(minority+majority))
- What does a great classifier model do? (Increases true positive rate, decreases false positive rate)
- How does threshold value in classifiers influence precision and recall? (Classifies what probabilities can be classified as a positive or negative class --- example 
  is if something had a 60% probability, at a 50% threshold it would be deemed a 1 (positive), while with a 65% threshold it would be deemed a 0 (negative)).
- What is precision and recall? (Precision (also called positive predictive value or PPV) = how many positive identifications were correct (TP/(TP+FP))? Recall (also 
  called sensitivity) = What proportion of actual positives were identified correctly (TP/(TP+FN))?)
- Why are precision and recall always in a tug of war? (Depending on how threshold is updated, a datapoint that can be deemed a positive, can become a negative...more 
positives equal higher precision (less FPs) and lower recall (more FNs), and vice versa).
- Why is a default threshold bad for classification problems with severe class imbalance? (Higher probabilities based on imbalance; also, lower probabilities, like in 
the case of having a 30% chance of cancer, should be deemed important, which is why the default is not always ideal).
- Type 1 Error: False positives out of total true negatives (see https://en.wikipedia.org/wiki/Precision_and_recall)
- Type 2 Error: False negatives out of total true positives (see https://en.wikipedia.org/wiki/Precision_and_recall); recall = 1 - Type 2 Error
- Null hypothesis: Given an item is irrelevant, absnce of type 1 and Type 2 errors leads to perfect precision and perfect recall
- Precision/Recall example: When a search engine returns 30 pages, only 20 of which are relevant, while failing to return 40 additional relevant pages, its precision is 
  20/30 = 2/3, which tells us how valid the results are, while its recall is 20/60 = 1/3, which tells us how complete the results are.
- (Precision/Recall are estimations of probabilities - Precision is the estimated probability that a document randomly selected from the pool of retrieved documents is 
   relevant; Recall is the estimated probability that a document randomly selected from the pool of relevant documents is retrieved.)
- (High precision is good in imbalance because it avoids irrelevant results (good in high imbalance). High recall provides complete results, including irrelevants (good 
   during balance).)
- F score: Harmonic mean of precision and recall
- SVC is bad with large datasets because it scales quadratically (need to understand this one)
- Make_pipeline: Runs functions in order of being listed, as steps

11/22/2022

- (For imbalanced classification problems, the majority class is typically referred to as the negative outcome (e.g. such as “no change” or “negative test result“), 
  and the minority class is typically referred to as the positive outcome (e.g. “change” or “positive test result“))
- F1 Score = Harmonic mean of precision and recall (good for cases like cancer; you want to optimize for recall (higher chance of finding all cases), while not being so 
  imprecise that you tell people that don't have cancer that they have it (high false positives)...simultaneously you would want to optimize for precision (correctly 
  guessing cancer patients), while not having such bad recall that you miss a large number of the cancer patients (false negatives)).
- (Low F1 score means poor precision and poor recall)
- TPR = True Positive Rate ((TP/(TP+FN)) --- how many positives guessed correctly out of all positives (is recall)
- FPR = False Positive Rate ((FP/(TN+FP)) --- how many positives guessed incorrectly out of all negatives
- AUC = Area Under the Curve (Measures the entire area under a curve from (0,0) to (1,1))
- Confusion matrix = provides more insight into not only the performance of a predictive model, but also which classes are being predicted correctly, which 
  incorrectly, and what type of errors are being made
- AUC ROC = Area Under the ROC Curve (Illustrates the tradeoff between sensitivity (1-TPR) and specificity (1-FPR); the furthey away the curve from the FPR=TPR 
  diagonal, the lower the FPR at a corresponding TPR, meaning more correct predictions per incorrect predictions and a more accurate model)
- (AUC ROC generally measures how many correct positive classifications are being gained with an increment in the rate of false positives.)  
- (AUC ROC Curve is essentially how, at each threshold how many true positives are predicted, versus false positives)
- (We can think of the AUC ROC plot as the fraction of correct predictions for the positive class (y-axis) versus the fraction of errors for the negative class 
  (x-axis)).
- AUC PR = Area Under the Precision-Recall Curve (A precision-recall curve (or PR Curve) is a plot of the precision (y-axis) and the recall (x-axis) for different 
  probability thresholds).
- (Difference between AUC ROC and AUC PR = AUC PR doesn't take True Negatives into consideration, which is typically the majority class in the imbalanced dataset).
- Regularization = Fits function on a given dataset and avoids overfitting
- Finding linear separability for SVM = https://stats.stackexchange.com/questions/182329/how-to-know-whether-the-data-is-linearly-separable

11/23/22

- Stratified fold = Stratified kfold cross validation is an extension of regular kfold cross validation but specifically for classification problems where rather than 
  the splits being completely random, the ratio between the target classes is the same in each fold as it is in the full dataset.
- Cross validation = a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the 
  complementary subset of the data.
- Calibration (ClassifierCV) = the post-processing of a model to improve its probability estimate. It helps us compare two models that have the same accuracy or other standard 
  evaluation metrics.
- (We say that a model is well calibrated when a prediction of a class with confidence p is correct 100p % of the time. To illustrate this calibration effect, let’s 
   consider that you have a model that predicts cancer with a score of 70% for each patient out of 100. If your model is well calibrated, we would have 70 patients 
   with cancer, if it is ill-calibrated, we will have more (or less).)
 - (Calibration uses things like Platt scaling, isotonic regression, etc...11/26/22)
 - Isotonic/Platt scaling = Transforms model output into probability
 - (Platt Scaling uses a sigmoid shape to calibrate the model, which implies a sigmoid-shaped distortion in our probability distribution. Isotonic Regression is a 
    more powerful calibration method that can correct any monotonic distortion. It projects a non parametric function into a set of increasing 
    functions (monotonic).)
 - Monotonic = Varying in a way that it never decreases or increases.
 - Piecewise = In mathematics, a piecewise-defined function is a function defined by multiple sub-functions, where each sub-function applies to a different interval 
   in the domain. Piecewise definition is actually a way of expressing the function, rather than a characteristic of the function itself.
 - Spline = Spline functions are formed by joining piecewise polynomials together at fixed points called knots.
 - Disjoint data = In computer science, a disjoint-set data structure, also called a union–find data structure or merge–find set, is a data structure that stores a 
   collection of disjoint sets. 
 - Hinge loss = "Maximum-margin classification" for classifiers, most especially SVM
 - Hard margin/soft margin = Hard margin (SVM) all data can be linearly separated...when not possible due to data points, soft margin
- Look at source of functions on documentation

11/24/22

- Cross val score = "cross_val_score" splits the data into say 5 folds. Then for each fold it fits the data on 4 folds and scores the 5th fold. Then it gives you
  the 5 scores from which you can calculate a mean and variance for the score. You crossval to tune parameters and get an estimate of the score. This includes fitting, 
  in fact it includes 5 fits!  
- Stratified cross val score = Ratio of target classes same on each fold as on entire dataset
- Bias = Difference between actual and predicted values
- Variance = Overfitting
- Pickling data = the process of converting a Python object into a byte stream to store it in a file/database, maintain program state across sessions, or transport 
  data over the network
  
11/26/22

- Variance = Sum of squares/number of data points
