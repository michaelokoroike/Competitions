11/21/2022

- Why do ROC curves remain unchanged between balanced and imbalanced datasets? (The ROC Curve can still look good because it takes True Negatives into consideration, the majority class in an imbalanced dataset...because thedataset has a high majority, the ROC Curve can seem to be accurate on the True Negatives, all while struggling on the True Positives.)
- Why is normalization bad when input variables have outliers? (It will skew the effect of the variable; normalization creates a range for each variable between 0 and 1...in the case of credit fraud if a majority of the fraud loans are done at amounts between 5000 and 10000, but there is a good loan at 1000000, this good loan actually screws up the correlation between price and fraud (as now thouse highly likely values for fraud are small (5000 divided by max of 1000000) and likely are weighted low compared to inputs without outliers) and negates an otherwise strong relationship.)
- "No skill": (A model that is as good as a random prediction; when imbalanced, can be the minority/(minority+majority))
- What does a great classifier model do? (Increases true positive rate, decreases false positive rate)
- How does threshold value in classifiers influence precision and recall? (Classifies what probabilities can be classified as a positive or negative class --- example is if something had a 60% probability, at a 50% threshold it would be deemed a 1 (positive), while with a 65% threshold it would be deemed a 0 (negative)).
- What is precision and recall? (Precision (also called positive predictive value or PPV) = how many positive identifications were correct (TP/(TP+FP))? Recall (also called sensitivity) = What proportion of actual positives were identified correctly (TP/(TP+FN))?)
- Why are precision and recall always in a tug of war? (Depending on how threshold is updated, a datapoint that can be deemed a positive, can become a negative...more positives equal higher precision (less FPs) and lower recall (more FNs), and vice versa).
- Why is a default threshold bad for classification problems with severe class imbalance? (Higher probabilities based on imbalance; also, lower probabilities, like in the case of having a 30% chance of cancer, should be deemed important, which is why the default is not always ideal).
- Type 1 Error: False positives out of total true negatives (see https://en.wikipedia.org/wiki/Precision_and_recall)
- Type 2 Error: False negatives out of total true positives (see https://en.wikipedia.org/wiki/Precision_and_recall); recall = 1 - Type 2 Error
- Null hypothesis: Given an item is irrelevant, absnce of type 1 and Type 2 errors leads to perfect precision and perfect recall
- Precision/Recall example: When a search engine returns 30 pages, only 20 of which are relevant, while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3, which tells us how valid the results are, while its recall is 20/60 = 1/3, which tells us how complete the results are.
- (Precision/Recall are estimations of probabilities - Precision is the estimated probability that a document randomly selected from the pool of retrieved documents is relevant; Recall is the estimated probability that a document randomly selected from the pool of relevant documents is retrieved.)
- (High precision is good in imbalance because it avoids irrelevant results (good in high imbalance). High recall provides complete results, including irrelevants (good during balance).)
- F score: Harmonic mean of precision and recall
- SVC is bad with large datasets because it scales quadratically (need to understand this one)
- Make_pipeline: Runs functions in order of being listed, as steps
